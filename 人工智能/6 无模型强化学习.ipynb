{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 无模型强化学习\n",
    "\n",
    "\n",
    "在上一节强化学习中, 我们预先知道每个状态到达下一步状态的概率分布, 可以用贝尔曼迭代求解.\n",
    "\n",
    "但是, 实际上这个概率分布我们常常不知道!\n",
    "\n",
    "此时, 我们只能模拟, 从反复的尝试中估计这个概率分布."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 蒙特卡洛 (Monte Carlo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序差分 (Temporal Difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一个问题是如何\"尝试\". 倘若每一步完全按最大估计值选择行动 (action), 得到新的尝试与样本, 容易陷入局部极小值而错过全局最大值.\n",
    "\n",
    "应具有一定随机性选择新的行动, 即探索 (explore).\n",
    "\n",
    "### SARSA\n",
    "\n",
    "对于每个状态, 有 $1-\\epsilon$ 概率按照最大的估值行动, $\\epsilon$ 概率尝试其他行动.\n",
    "\n",
    "$$Q(s^{(t)},a)=(1-\\alpha)Q(s^{(t)},a)+\\alpha Q(s^{(t+1)},a')$$\n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "对于每个状态, 有 $1-\\epsilon$ 概率按照最大的估值行动, $\\epsilon$ 概率尝试其他行动.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 值近似方法\n",
    "\n",
    "实际上状态空间太大, 无法记录每个状态对应的效用值.\n",
    "\n",
    "可以用一个函数 (如神经网络) $f(S)$ 预测状态 $S$ 的效用值."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
