{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Optimality Conditions\n",
    "\n",
    "If a function $f:\\ \\mathbb R^n\\rightarrow \\mathbb R$ is continuously differentiable, one can find its local minimum by derivative.\n",
    "\n",
    "**Theorem** Suppose $f:\\ \\mathbb R^n \\rightarrow \\mathbb R$ is continuously differentiable over an open set. If some point on the open set has $\\nabla f(x_0)\\neq 0$, then $x_0$ is not a local minimum (or local maximum). \n",
    "\n",
    "Proof: Suppose $d = \\nabla f(x_0)\\neq 0$ and a sufficiently small $r>0$ such that $f$ is continuously differentiable over the ball $\\{ x:\\ \\Vert x - x_0\\Vert \\leqslant r\\}$. By the continuity of the derivative and $f(x_0)^Td = d^Td>0$ we may also assume $f(x)^Td>0$ in the ball. Recall that the Taylor's theorem states that there exists some $\\alpha \\in (0,1)$ such that \n",
    "$$f\\left(x_0 - r\\frac{d}{\\Vert d \\Vert}\\right) = f(x_0) -  r \\nabla f\\left(x_0-\\alpha r\\frac{d}{\\Vert d\\Vert}\\right)^T\\frac{d}{\\Vert d\\Vert} < f(x_0).$$\n",
    "\n",
    "\n",
    "**Theorem** Suppose $f:\\ \\mathbb R^n \\rightarrow \\mathbb R$ is continuously differentiable over an open set. If on the open set it has some local minimum (or local maximum) $x_*$, then $\\nabla f(x_*) = 0$.\n",
    "\n",
    "Proof: This is the converse of the previous theorem. \n",
    "\n",
    "**Theorem** Suppose $f:\\ \\mathbb R^n \\rightarrow \\mathbb R$ is continuously differentiable and convex over an open set. Then on the open set $x_*$ is a global minimum iff $\\nabla f(x_*) = 0$.\n",
    "\n",
    "Proof: The $\\Rightarrow $ side is trivial by the argument above. The $\\Leftarrow$ side yields from $f(y)\\geqslant f(x_*) + \\nabla f(x_*)^T(y-x_*) = f(x_*)$.\n",
    "\n",
    "\n",
    "**Theorem**  Suppose $f:\\ \\mathbb R^n \\rightarrow \\mathbb R$ is twice continuously differentiable over an open set. Then if on the open set $x_*$ is local minimum will $\\nabla f(x_*) = 0 $ and $\\nabla^2 f(x_*)\\succeq 0$. Conversely if  $\\nabla f(x_*) = 0$ and $\\nabla^2 f(x_*)\\succ 0$, then $x_*$ is a local minimum.\n",
    "\n",
    "Proof: $\\Rightarrow $ side: If not, we assume $\\nabla^2 f(x_*)<0$ and thus there exists some $d$ such that $d^T\\nabla ^2 f(x_*)d<0$. The continuity of the second order derivative guarantees that $d^T\\nabla f(x_*)d < 0$ around a neighborhood of $x_*$. Then the Taylor's theorem claims that for sufficiently small $r>0$ we have \n",
    "$$f\\left(x_* + r\\frac{d}{\\Vert d \\Vert}\\right)= f(x_*) + \\frac{d^Tr}{\\Vert d\\Vert }\\nabla^2 f\\left(x_* - \\alpha r\\frac{d}{\\Vert d \\Vert}\\right) \\frac{rd}{\\Vert d\\Vert }<f(x_*).$$\n",
    "\n",
    "$\\Leftarrow $ side is also a direct application of Taylor's theorem and the continuity of $\\nabla^2 f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fritz John (FJ) Necessary Conditions\n",
    "\n",
    "Fritz John necessary conditions applies Lagrange multipliers on the constrained optimization problems. If $f,g,h$ is \n",
    "continuously differentiable on the region, suppose the optimization problem \n",
    "$$\\min_x \\{f(x):\\quad g_i(x)\\leqslant 0,\\quad h_j(x) = 0\\}$$\n",
    "has a local minimum $x_*$.  Then there exist $u,\\lambda_i,\\mu_j\\in \\mathbb R$ that are not simultaneously zero such that \n",
    "$$\\begin{aligned}\n",
    "g_i(x_*)& \\leqslant 0 \\\\\n",
    "h_i(x_*)& = 0\\\\\n",
    "u\\nabla f(x_*) + \\sum \\lambda_i \\nabla g_i(x_*) + \\sum \\mu_j \\nabla h_j(x)  & = 0\\\\\n",
    "u &\\geqslant 0\\\\\n",
    "\\lambda_i &\\geqslant 0\\\\\n",
    "\\lambda_i g_i(x_*) &= 0.\n",
    "\\end{aligned}$$\n",
    "\n",
    "The leading two conditions are the original ones. The third is the gradient of after introducing the Lagrange multpliers. The fourth and fifth guarantees the inequality while the last implies that either $\\lambda_i = 0$ or \n",
    "$g_i(x_*) = 0$, known as complementary slackness.\n",
    "\n",
    "The converse does not hold, i.e. a Fritz John solution might not be a local minimum.\n",
    "\n",
    "## Karush-Kuhn-Tucker (KKT) conditions\n",
    "\n",
    "If $f,g,h$ is \n",
    "continuously differentiable on the region, suppose the optimization problem \n",
    "$$\\min_x \\{f(x):\\quad g_i(x)\\leqslant 0,\\quad h_j(x) = 0\\}$$\n",
    "has a local minimum $x_*$.  Then, if denote by $I=\\{i\\in\\{1,2,\\dotsc\\}:\\ g_i(x_*) = 0\\}$ the active set and \n",
    "the set of vectors $\\{\\nabla g_i(x_*)\\}_{i\\in I}\\cup \\{\\nabla h_j(x_*)\\}$ are linearly independent, there exist $\\lambda_i,\\mu_j\\in \\mathbb R$  such that \n",
    "$$\\begin{aligned}\n",
    "g_i(x_*)& \\leqslant 0 \\\\\n",
    "h_i(x_*)& = 0\\\\\n",
    "\\nabla f(x_*) + \\sum \\lambda_i \\nabla g_i(x_*) + \\sum \\mu_j \\nabla h_j(x)  & = 0\\\\\n",
    "\\lambda_i &\\geqslant 0\\\\\n",
    "\\lambda_i g_i(x_*) &= 0.\n",
    "\\end{aligned}$$\n",
    "And we call $x_*$ a Karush-Kuhn-Tucker (KKT) point. \n",
    "\n",
    "The converse does not hold, i.e. a KKT point might not be a local minimum.\n",
    "\n",
    "Note that when the set of vectors are linearly independent will the problem solved by some KKT point, it implies there might not be any solution of KKT system otherwise. Compared to FJ necessary condition, KKT ensures $u>0$ in FJ.\n",
    "\n",
    "Example: Solve\n",
    "$$\\min \\{x_1:\\quad (x_1 - 1)^2+(x_2 - 1)^2\\leqslant 1,\\quad (x_1 - 1)^2+(x_2 +1)^2\\leqslant 1\\}$$\n",
    "While there is only one point $[1,0]^T$ in the feasible set, the minimum is $1$. But the KKT system is \n",
    "$$\\begin{aligned}\n",
    "(x_1 - 1)^2+(x_2 - 1)^2&\\leqslant 1\\\\\n",
    "\\quad (x_1 - 1)^2+(x_2 +1)^2&\\leqslant 1\\\\\n",
    "1 + 2\\lambda_1 (x_1 - 1) + 2\\lambda_2 (x_2 - 1) &= 0\\\\ \n",
    "2\\lambda_1 (x_2 - 1) + 2\\lambda_2 (x_2 + 1) &= 0\\\\\n",
    "& \\dotsm\n",
    "\\end{aligned}$$\n",
    "The first two have required $[x_1,x_2] = [1,0]$ but as a consequence, the third and the fourth cannot hold both. So there is no KKT solution here, simply because $\\nabla f_1([1,0]^T = [0,-1]^T$ and $\\nabla f_2([1,0]^T) = [0,1]^T$ are linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Minimum\n",
    "\n",
    "#### Weierstrass Theorem\n",
    "\n",
    "$f$ is continuous on a compact set $C\\in\\mathbb R^n$, then it has a global minimum over $C$.\n",
    "\n",
    "#### Coerciveness\n",
    "\n",
    "Let $f:\\mathbb R^n\\rightarrow \\mathbb R$ be a continuous function and it is called coercive if \n",
    "$$\\lim_{\\Vert x\\Vert\\rightarrow \\infty}f(x) = \\infty$$\n",
    "\n",
    "If $f$ is coercive, and $S\\subset \\mathbb R^n$ is an arbitrary nonempty closed set, then \n",
    "$f$ has a global minimum over $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slater Condition\n",
    "\n",
    "**Definition** If $f,g,h$ is \n",
    "continuously differentiable on the region, consider the optimization problem \n",
    "$$\\min_x \\{f(x):\\quad g_i(x)\\leqslant 0,\\quad h_j(x) = 0\\}.$$\n",
    "If $x_*$ is a local minimum of the  and $I = \\{i:\\ g_i(x_*) = 0\\}$ is the active set. Then if we have at least one $x'$ in the feasible set such that $g_i(x')<0$ for $i\\in I$, we say that the Slater condition is satisfied.\n",
    "\n",
    "More generally, the Slater condition allow $g_i(x') = 0$ if $g_i$ is affine.\n",
    "\n",
    "### Necessary KKT\n",
    "\n",
    "**Theorem** If in the problem $g_i$ are all **convex** and $h_j$ are all **affine**. Then if $x_*$ is a local minimum and the Slater condition gets satisfied, $x_*$ is a KKT point, i.e. $g_I(x)$ is linearly independent. \n",
    "\n",
    "<font color = red> **Note** The target function $f$ is NOT necessarily convex.</font>\n",
    "\n",
    "Proof: As $h_j$ are affine, without loss of generality we may assume that $\\nabla h_j$ are linearly independent. \n",
    "\n",
    "From FJ necessary condition, we have $u\\nabla f(x_*) +\\sum \\lambda_i\\nabla g(x_*) + \\sum \\mu_j  \\nabla h(x_*) = 0$ and $u,\\lambda \\geqslant 0$. Then it suffices to  show that $u>0$ is strict. If not, on the one hand $u = 0$ implies that \n",
    "$$\\sum_{i\\in I} \\lambda_i \\nabla g_i(x_*) + \\sum \\mu_j \\nabla h_j(x_*) = 0\\quad (\\star)$$\n",
    "where  $I = \\{i:\\ g_i(x_*) = 0\\}$ is the active set because $\\lambda_i = 0$ as long as $i\\notin I$ by the complementary slackness. \n",
    "\n",
    "On the other, note that $u,\\lambda_i,\\mu_j$ are not simutaneously zero, the linear independence of $h_j$ constraints that $I$ is nonempty and there exists at least some $i\\in I$ such that $\\lambda_i \\neq 0$. the Slater condition gives\n",
    "\n",
    "$$0>g_i(x') \\geqslant  g_i(x_*) + \\nabla g_i(x_*)^T(x' - x_*)= \\nabla g_i(x_*)^T(x' - x_*)\\quad \\forall i\\in I.$$ \n",
    "\n",
    "By linearity of $h_j$ we learn that $\\nabla h_j^T$ is independent of $x$ and since $0 = h_j(x_*) = h_j(x')$, \n",
    "$$0=\\nabla h_j(x_*)^T (x' - x_*)\\quad \\forall j.$$ \n",
    "\n",
    "Consequently, sum these up to obtain\n",
    "$$\\sum_{i\\in I} \\lambda_i \\nabla g_i(x_*)^T(x' - x_*) + \\sum \\mu_j \\nabla h_j(x_*)^T(x' - x_*) < 0,$$\n",
    "which is  a contradiction to $(\\star)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex KKT\n",
    "\n",
    "For a convex optimization problem, if $f,g,h$ is \n",
    "continuously differentiable on the open and convex set $X$, consider the convex optimization problem \n",
    "$$\\min_x \\{f(x):\\quad g_i(x)\\leqslant 0,\\quad h_j(x) = 0,\\quad x\\in X\\}$$\n",
    "where $f,g$ are convex and $h$ are affine. Then if $x_*$ is a KKT point, i.e. there exists $\\lambda_i,\\mu_j$ such that \n",
    "$$\\begin{aligned}\n",
    "g_i(x_*)& \\leqslant 0 \\\\\n",
    "h_i(x_*)& = 0\\\\\n",
    "x_*&\\in X\\\\\n",
    "\\nabla f(x_*) + \\sum \\lambda_i \\nabla g_i(x_*) + \\sum \\mu_j \\nabla h_j(x)  & = 0\\\\\n",
    "\\lambda_i &\\geqslant 0\\\\\n",
    "\\lambda_i g_i(x_*) &= 0.\n",
    "\\end{aligned}$$\n",
    "Then $x_*$ must be the global minimum.\n",
    "\n",
    "Proof: For a KKT point $x_*$ we prove that it is the  global minimum. We have already known that it is sufficient \n",
    "to show that $\\nabla f(x_*) = 0$ by the convexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "#### Log Determinant\n",
    "\n",
    "Given $A\\in \\mathcal S_{++}^n$ and $b>0$. Solve in closed form\n",
    "$$\\min\\{-\\log {\\rm det} Z:\\ A\\bullet Z\\leqslant b,\\ Z\\in \\mathcal S_{++}^n\\}$$\n",
    "where $A\\bullet Z = {\\rm tr}(A^*Z) = {\\rm vec}(A)^*{\\rm vec}(Z)$ is the standard inner product and $^*$ stands for the conjugate transpose.\n",
    "\n",
    "Solution: We have shown in previous courses that the target function is convex, note that \n",
    "$$-\\nabla \\log {\\rm det} Z = -\\frac{\\partial}{\\partial {\\rm det}Z}\\log {\\rm det} Z  \\cdot \n",
    "\\frac{\\partial}{\\partial Z}{\\rm det}Z =-\\frac{{\\rm adj}Z}{ {\\rm det}Z} = -Z^{-1} $$\n",
    "\n",
    "And the KKT condition requires $-Z^{-1}+\\lambda A = 0$, leading to $Z = \\lambda^{-1}A^{-1}$ as long as $\\lambda \\neq 0$. Since $$b = A\\bullet Z = \\lambda^{-1}A\\bullet A^{-1} = \\lambda^{-1}{\\rm tr}(I) = n\\lambda^{-1},$$\n",
    " we obtain $\\lambda = \\frac nb$ and $Z = \\frac bnA^{-1}$.\n",
    "\n",
    " \n",
    "#### Water Filling\n",
    "\n",
    "Given $\\alpha>0$ and $e = [1,1,\\dotsc,1]^T$. Solve\n",
    "$$\\min\\{-\\sum_{i=1}^n \\log(x_i+\\alpha_i):\\quad x\\geqslant 0,\\quad e^Tx = 1\\}.$$\n",
    "\n",
    "Solution: This is a convex optimization and we construct from the gradient that\n",
    "$$-\\frac{1}{x_i+\\alpha_i} - \\lambda_i +\\mu  = 0.$$\n",
    "The complementary slackness requires that $\\lambda_i = 0$ or $x_i = 0$. If $\\lambda_i = 0$, then $0\\leqslant x_i = \\frac{1}{\\mu} - \\alpha_i$. If $x_i = 0$, then $0\\leqslant \\lambda_i = \\mu - \\frac{1}{\\alpha_i}$. \n",
    "\n",
    "Conversely, given $\\mu$ and $i$, if $\\frac{1}{\\mu} \\geqslant \\alpha_i$, then $x_i = \\frac{1}{\\mu} - \\alpha_i$. If $\\frac{1}{\\mu} < \\alpha_i$, then $x_i = 0$. Hence we summarize\n",
    "\n",
    "$$x_i = \\max\\{\\frac{1}{\\mu}-\\alpha_i,0\\}.$$\n",
    "\n",
    "Back substitution yields that $\\sum_{i=1}^n\\max\\{\\frac{1}{\\mu}-\\alpha_i,0\\}$, of which a geometric \n",
    "characterization is illustrated below."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1403262caf5c96c562d23c2ec1d0e9409dfef636549d4d9dd4887bed656bec38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
