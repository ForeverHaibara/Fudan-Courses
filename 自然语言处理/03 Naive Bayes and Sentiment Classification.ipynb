{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Naive Bayes and Sentiment Classification\n",
    "\n",
    "## 朴素贝叶斯\n",
    "\n",
    "### 文本分类\n",
    "\n",
    "文本分类 (classification) 可以用于 判断垃圾邮件、判断一个文本属于哪个领域（食物 / 电影） 等。\n",
    "\n",
    "最简单的可以用机器学习中的朴素贝叶斯 (naive bayes, NB) 方法进行文本分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self,\n",
    "            data: List[str],\n",
    "            labels: List[Union[str, int]],\n",
    "            vocab: Optional[dict] = None,\n",
    "            smoothk: float = 1.\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Naive Bayes classifier for sentiment analysis.\n",
    "\n",
    "        Parameters\n",
    "        --------\n",
    "        data: List[str]\n",
    "            The dataset.\n",
    "        labels: List[Union[str, int]]\n",
    "            The class name of each datum.\n",
    "        vocab: Optional[dict]\n",
    "            Prior vocabulary list. If None, use all the vocabulary in the training data.\n",
    "        smoothk: float\n",
    "            Bayes smoothing constant. Defaults to 1 (Laplacian smoothing).\n",
    "        \"\"\"\n",
    "        # map string-like labels to integer indices\n",
    "        label_names = sorted(list(set(labels)))\n",
    "        self.label_name2ind = dict((name, ind) for name, ind in zip(label_names, range(len(label_names))))\n",
    "        self.label_ind2name = dict((ind, name) for name, ind in zip(label_names, range(len(label_names))))\n",
    "\n",
    "        self.num_class = len(label_names)\n",
    "\n",
    "        if vocab is None:\n",
    "            # construct the vocabulary list from the training set if not given\n",
    "            vocab = set()\n",
    "            for line in data:\n",
    "                vocab.update(line.split())\n",
    "        \n",
    "        self.log_prob = dict((key, [0 for i in range(self.num_class)]) for key in vocab)\n",
    "        self.log_prior = [0] * self.num_class\n",
    "        self.total_words = [0] * self.num_class\n",
    "        log_prob = self.log_prob\n",
    "        \n",
    "        # count frequency\n",
    "        name2ind = self.label_name2ind\n",
    "        for line, label in tqdm(zip(data, labels), total = len(data)):\n",
    "            label = name2ind[label]\n",
    "            split = line.split()\n",
    "            words_count = 0\n",
    "            for word in split:\n",
    "                box = log_prob.get(word)\n",
    "                if box is not None:\n",
    "                    box[label] += 1\n",
    "                    words_count += 1\n",
    "            \n",
    "            self.log_prior[label] += 1\n",
    "            self.total_words[label] += words_count\n",
    "        \n",
    "        log = np.log\n",
    "        self.log_prior = np.array(self.log_prior)\n",
    "        self.log_prior = log(self.log_prior) - log(self.log_prior.sum())\n",
    "    \n",
    "        # compute log probability\n",
    "        log_normalizer = log(np.array(self.total_words) + (len(vocab) + 1) * smoothk)\n",
    "        for key in log_prob.keys():\n",
    "            log_prob[key] = log(np.array(log_prob[key]) + smoothk) - log_normalizer\n",
    "\n",
    "\n",
    "    def __getitem__(self, \n",
    "            x: Union[str, Tuple[str, Union[str, int]]]\n",
    "        ) -> Union[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Get the smoothed log probability log(P(word|c)) for all c or some c if specified.\n",
    "        \"\"\"\n",
    "        if isinstance(x, str):\n",
    "            return self.log_prob[x]\n",
    "        if len(x) == 2:\n",
    "            s = x[1]\n",
    "            if isinstance(s, str):\n",
    "                s = self.label_name2ind[s]\n",
    "        return self.log_prob[x[0]][s]\n",
    "    \n",
    "    def predict(self,\n",
    "            data: Union[List[str], str],\n",
    "            need_lower: bool = True,\n",
    "            verbose: bool = False\n",
    "        ) -> Union[List[str], str]:\n",
    "        \"\"\"\n",
    "        Make prediction (classification) on data.\n",
    "\n",
    "        Parameters\n",
    "        --------\n",
    "        data: Union[List[str], str]\n",
    "            The data to make prediction on.\n",
    "        need_lower: bool\n",
    "            Whether convert the text to lower case.\n",
    "        verbose: bool\n",
    "            If True, display a tqdm bar.\n",
    "        \"\"\"\n",
    "        only_one = isinstance(data, str)\n",
    "        if only_one:\n",
    "            data = [data]\n",
    "\n",
    "        result = []\n",
    "        verbose = tqdm if verbose else (lambda x: x) \n",
    "        for line in verbose(data):\n",
    "            if need_lower:\n",
    "                line = line.lower()\n",
    "\n",
    "            pred = self.log_prior.copy()\n",
    "            for word in line.split():\n",
    "                box = self.log_prob.get(word)\n",
    "                if box is not None:\n",
    "                    pred += box\n",
    "                # unknown words contribute to the prob of each class equally\n",
    "                # so we can just ignore them\n",
    "\n",
    "            result.append(self.label_ind2name[np.argmax(pred)])\n",
    "        \n",
    "        if only_one:\n",
    "            result = result[0]\n",
    "        return result\n",
    "\n",
    "    def test(self, \n",
    "            data: List[str],\n",
    "            true_labels: List[str],\n",
    "            plot_in_notebook: bool = True,\n",
    "            **kwargs\n",
    "        ) -> dict:\n",
    "        \"\"\"\n",
    "        Make test on data given ground truth labels. Return the confusion matrix.\n",
    "        \n",
    "        Parameters\n",
    "        -------\n",
    "        data: List[str]\n",
    "            The data to make prediction on.\n",
    "        true_labels: List[str]\n",
    "            Ground truth labels for the data.\n",
    "        plot_in_notebook: bool\n",
    "            Whether plot the testing result directly in notebook.\n",
    "            Only takes effect in notebook environment.\n",
    "        kwargs:\n",
    "            Other keyword argument passed directly to function 'predict'.\n",
    "        \"\"\"\n",
    "        pred_y = self.predict(data, **kwargs)\n",
    "        p = self.num_class\n",
    "\n",
    "        cross_tab = np.zeros((p+1, p+1), dtype = 'int32')\n",
    "        name2ind = self.label_name2ind\n",
    "        for pred, truth in zip(pred_y, true_labels):\n",
    "            cross_tab[name2ind[truth]][name2ind[pred]] += 1\n",
    "        cross_tab[-1,:] = cross_tab.sum(axis = 0)\n",
    "        cross_tab[:,-1] = cross_tab.sum(axis = 1)\n",
    "\n",
    "        metric_funcs = {\n",
    "            'Accuracy': lambda tp, fp, fn, tn: (tp + tn) / (tp + fp + fn + tn),\n",
    "            'Precision': lambda tp, fp, fn, tn: tp / (tp + fp),\n",
    "            'Recall': lambda tp, fp, fn, tn: tp / (tp + fn),\n",
    "            'F1': lambda tp, fp, fn, tn: 2 * tp / (2 * tp + fp + fn)\n",
    "        }\n",
    "\n",
    "        metrics = {'macro': {}, 'micro': {}}\n",
    "        for key, func in metric_funcs.items():\n",
    "            # compute macro / micro value for each metric\n",
    "            s = 0\n",
    "            for i in range(p):\n",
    "                # extract the partial contingency table\n",
    "                tp = cross_tab[i,i]\n",
    "                fp, fn = cross_tab[-1,i] - tp, cross_tab[i,-1] - tp\n",
    "                tn = cross_tab[-1,-1] - tp - fp - fn\n",
    "                s += func(tp, fp, fn, tn)\n",
    "            metrics['macro'][key] = s / p\n",
    "\n",
    "            tp_all, fp_all, fn_all, tn_all = 0, 0, 0, 0\n",
    "            for i in range(p):\n",
    "                tp = cross_tab[i,i]\n",
    "                fp, fn = cross_tab[-1,i] - tp, cross_tab[i,-1] - tp\n",
    "                tn = cross_tab[-1,-1] - tp - fp - fn\n",
    "                tp_all += tp\n",
    "                fp_all += fp\n",
    "                fn_all += fn\n",
    "                tn_all += tn\n",
    "            metrics['micro'][key] = func(tp_all, fp_all, fn_all, tn_all)\n",
    "\n",
    "        label_names = list(name2ind.keys()) + ['Total']\n",
    "        confusion_matrix = pd.DataFrame(cross_tab, columns = label_names, index = label_names)\n",
    "        metrics = pd.DataFrame(metrics).T\n",
    "        acc = (cross_tab[:-1,:-1].flat[::p+1]).sum() / cross_tab[-1,-1]\n",
    "\n",
    "        if plot_in_notebook:\n",
    "            try: \n",
    "                from IPython.display import display\n",
    "                get_ipython\n",
    "                print('Accuracy = %.2f%%\\n'%(100. * acc) + '=' * 60)\n",
    "                print('True \\\\ Pred')\n",
    "                display(confusion_matrix)\n",
    "                display(metrics)\n",
    "            except:\n",
    "                plot_in_notebook = False\n",
    "\n",
    "        return {\n",
    "            'acc': acc,\n",
    "            'prediction': pred_y,\n",
    "            'confusion_matrix': confusion_matrix,\n",
    "            'metrics': metrics\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
