{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 N grams\n",
    "\n",
    "英语中, \"I want\" 后面时常跟着 \"to\"。 \"I want to\" 后面一般跟着动词。假设现有 $(n-1)$ 个词 $w_1,\\dotsc,w_{n-1}$, 预测下一个词为 $w_n$. 则不同的词 $w_n$ 出现的条件概率不同\n",
    "\n",
    "$$\\mathbb P(w_n| w_1,\\dotsc,w_{n-1})$$\n",
    "\n",
    "例如上述例子说明 $\\mathbb P({\\rm to|\\ I \\ want})$ 比较大, 但是如 $\\mathbb P({\\rm apple|\\ I\\ want\\ to})$ 很小。\n",
    "\n",
    "如果一个模型可以输入 $w_1,\\dotsc,w_{n-1}$, 且对于任何一个词 $w_n$ 输出它是下一个词的概率 $\\mathbb P(w_n| w_1,\\dotsc,w_{n-1})$, 则被称为语言模型 (language model).\n",
    "\n",
    "<br>\n",
    "\n",
    "如果语言文本的数据集, 可以统计序列 $(w_1,\\dotsc,w_{n-1})$ 的出现次数与 $(w_1,\\dotsc,w_{n-1},w_n)$ 的出现次数, 根据比值求得近似的概率:\n",
    "\n",
    "$$\\mathbb P(w_n| w_1,\\dotsc,w_{n-1}) \\approx \\frac{N(w_1,\\dotsc,w_{n-1},w_{n})}{N(w_1,\\dotsc,w_{n-1})}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N grams\n",
    "\n",
    "数据集词语组合太多, 数据集不可能所有组合都包含足够多次. 用 $N(w_1,\\dotsc,w_n) / N(w_1,\\dotsc,w_{n-1})$ 不合适。\n",
    "\n",
    "可以截断只有最近的几个词近似. 例如二元模型 (bigram):\n",
    "\n",
    "$$\\mathbb P(w_n| w_1,\\dotsc,w_{n-1})\\approx \\mathbb P(w_n| w_{n-1}) \\approx  \\frac{N(w_{n-1},w_n)}{N(w_{n-1})}$$\n",
    "\n",
    "例如 \"I want to\" 的下一个词 (很可能是动词) 主要取决于 \"to\". 这样一来样本数量 $N(w_{n-1},w_n), N(w_{n-1})$ 远高于 $N(w_1,\\dotsc,w_n), N(w_1,\\dotsc,w_{n-1})$, 估计效果更好。\n",
    "\n",
    "同理有一元模型 (unigram) $\\mathbb P(w_n| w_1,\\dotsc,w_{n-1})\\approx \\mathbb P(w_n)$, 三元模型 (trigram) $\\mathbb P(w_n| w_1,\\dotsc,w_{n-1})\\approx \\mathbb P(w_n| w_{n-1},w_{n-2}) \\approx \\frac{N(w_{n-2},w_{n-1},w_n)}{N(w_{n-2},w_{n-1})}$ 等。\n",
    "\n",
    "形如二元模型中的 $(w_{n-1},w_n)$ 被称为词袋 (gram)。\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "### 陌生词\n",
    "\n",
    "如果在测试集上遇到陌生的词汇, 输出概率为零就会产生严重误差。可以**先确定一个常用词汇表 (vocabulary)** $V$，**所有不在** $V$ **中的单词被看做一个“新词”**(unknown word, unk)。\n",
    "\n",
    "词汇表可以选择为训练集中出现频率高于一定次数的单词。\n",
    "\n",
    "以 $46$ MB 的英文维基数据集为例, $120000$ 个单词中仅有 $7000$ 多个出现次数 $>100$。\n",
    "\n",
    "<br>\n",
    "\n",
    "### 平滑\n",
    "\n",
    "可以用拉普拉斯平滑 (Laplacian/Bayes smoothing): 设 $|V|$ 为所有单词数量，以二元模型为例\n",
    "\n",
    "$$\\hat {\\mathbb P}(w_n| w_{n-1}) = \\frac{N(w_{n-1},w_n)+1}{N(w_{n-1})+|V|}$$\n",
    "\n",
    "分子加 $1$， 分母加 $|V|$，这样子仍然能保证 $|V|$ 个词袋概率之和为一:\n",
    "\n",
    "$$\\sum_{w_n\\in V}\\hat {\\mathbb P}(w_n| w_{n-1})= \\frac{\\sum_{w_n\\in V}\\left(N(w_{n-1},w_n)+1\\right)}{N(w_{n-1})+|V|}=1$$\n",
    "\n",
    "<br>\n",
    "\n",
    "引进陌生词后，分母还要包括陌生词的一个，公式应修正为：\n",
    "\n",
    "$$\\hat {\\mathbb P}(w_n| w_{n-1}) = \\frac{N(w_{n-1},w_n)+1}{N(w_{n-1})+|V|+1}$$\n",
    "\n",
    "训练时可以跳过含有陌生词的词袋，因为平滑后保证遇到陌生词也有非零的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "class SizedDefaultDict(defaultdict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        An override of defaultdict that has an extra argument to store info.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._boxsize = 0\n",
    "\n",
    "class Ngrams:\n",
    "    def __init__(self, \n",
    "            data: List[Union[List[str], str]] = [], \n",
    "            n: int = 2, \n",
    "            vocab: Optional[dict] = None,\n",
    "            bos_token: str = '<s>',\n",
    "            eos_token: str = '<e>',\n",
    "            unk_token: str = '<unk>',\n",
    "            smooothk: float = 1., \n",
    "            need_reg: bool = True,\n",
    "            ignore_unk: bool = True,\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Construct an ngram model on some data.\n",
    "\n",
    "        Parameters\n",
    "        -------\n",
    "        data: List[Union[List[str]], str]\n",
    "            A corpus of list of articles or list of tokenized sentences.\n",
    "            No need to provide bos/eos token manually.\n",
    "        n: int\n",
    "            Length N for N-gram model.\n",
    "        vocab: Optional[dict]\n",
    "            Prior vocabulary dictionary.\n",
    "            If None, use all the vocabulary on the training dataset.\n",
    "        bos_token: str\n",
    "            Beginning-of-sentence token.\n",
    "        eos_token: str\n",
    "            Ending-of-sentence token.\n",
    "        unk_token: str\n",
    "            Unknown word token.\n",
    "        smoothk: float\n",
    "            Bayes smoothing constant. Defaults to 1 (Laplacian smoothing).\n",
    "        need_reg: bool\n",
    "            Whether perform regularization on dataset \n",
    "            (remove punctuation and transform to lower case).\n",
    "        ignore_unk: bool\n",
    "            If True, ignore ngrams with unknown words in the training dataset.\n",
    "            If False, unknown words in training dataset are treated as unk_token.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.n = n\n",
    "        self.smoothk = smooothk\n",
    "\n",
    "        def reg(s):\n",
    "            def strreg(s):\n",
    "                return ''.join(filter(\n",
    "                    lambda x: 96 < ord(x) < 123 or x == ' ' or 47 < ord(x) < 58, s.lower()))\n",
    "\n",
    "            if isinstance(s, str):\n",
    "                return strreg(s)\n",
    "            return [strreg(_) for _ in s]\n",
    "\n",
    "        self.reg = reg\n",
    "\n",
    "        self.prob = reduce(lambda x, y: SizedDefaultDict(lambda: deepcopy(x)), [0] * (n+1))\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "        self.ignore_unk = ignore_unk\n",
    "\n",
    "        self.vocab = vocab\n",
    "        if self.vocab is None:\n",
    "            # if no vocabulary is provided, initialize it with the whole dataset\n",
    "            vocab = set([self.bos_token, self.eos_token, self.unk_token])\n",
    "        else:\n",
    "            # write bos/eos/unk tokens to the vocabulary list\n",
    "            for token in (self.bos_token, self.eos_token, self.unk_token):\n",
    "                self.vocab[token] = 0\n",
    "\n",
    "        for article in tqdm(data):\n",
    "            if isinstance(article, str):\n",
    "                article = sent_tokenize(article)\n",
    "            \n",
    "            for sents in article:\n",
    "                # train with each sentence\n",
    "                split_sent = sents.split()\n",
    "\n",
    "                # add all words to the vocabulary list if not given\n",
    "                if self.vocab is None:\n",
    "                    vocab.update(split_sent)\n",
    "\n",
    "                # pad each sentence with bos and eos tokens\n",
    "                words = [self.bos_token] * (n-1) + split_sent\n",
    "                words.append(self.eos_token)\n",
    "\n",
    "                if not ignore_unk:\n",
    "                    words = self._mask_unknown_word(words)\n",
    "\n",
    "                for i in range(max(0, len(words) - n + 1)):\n",
    "                    if (not ignore_unk) or self._is_known_ngram(words[i:i+n]):\n",
    "                        box = reduce(lambda x, y: x[y], words[i:i+n-1], self.prob)\n",
    "                        box[words[i+n-1]] += 1\n",
    "\n",
    "                        # it is desperately essential to compute the count of (w1,...,w{n-1}) \n",
    "                        # by accumulation in advance\n",
    "                        box._boxsize += 1\n",
    "\n",
    "        if self.vocab is None:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def _is_known_word(self,\n",
    "            word: Union[List[str], str]\n",
    "        ) -> Union[List[bool], bool]:\n",
    "        \"\"\"\n",
    "        Check whether a single word or a list of word is in the vocabulary list.\n",
    "        When `self.vocab` is None, returns True defaultedly.\n",
    "        \"\"\"\n",
    "        if isinstance(word, str):\n",
    "            if self.vocab is None:\n",
    "                return True\n",
    "            return self.vocab.get(word) is not None\n",
    "        return [self._is_known_word(w) for w in word]\n",
    "\n",
    "    def _is_known_ngram(self, \n",
    "            ngram: List[str]\n",
    "        ) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether each word in the ngram is in the vocabulary list.\n",
    "        When `self.vocab` is None, returns True defaultedly.\n",
    "        \"\"\"\n",
    "        if self.vocab is None:\n",
    "            return True\n",
    "        return not any(self.vocab.get(word) is None for word in ngram)\n",
    "\n",
    "    def _mask_unknown_word(self,\n",
    "            word: Union[List[str], str]\n",
    "        ) -> str:\n",
    "        \"\"\"\n",
    "        Convert a single word or a list of word to unk token if it is not \n",
    "        in the vocabulary list.\n",
    "        \"\"\"\n",
    "        if isinstance(word, str):\n",
    "            if self.vocab is not None and self.vocab.get(word) is None:\n",
    "                return self.unk_token\n",
    "            return word\n",
    "        return [self._mask_unknown_word(w) for w in word]\n",
    "\n",
    "    def get(self, *args, **kwargs) -> Union[float, SizedDefaultDict]:\n",
    "        \"\"\"\n",
    "        Get the probability of some ngram or the frequency dict of the ngram. \n",
    "        See details at __getitem__.\n",
    "        \"\"\"\n",
    "        return self.__getitem__(*args, **kwargs)\n",
    "    \n",
    "    def __getitem__(self, \n",
    "            pos: Union[List[str], str], \n",
    "            need_reg: bool = False, \n",
    "            need_lower: bool = True\n",
    "        ) -> Union[float, SizedDefaultDict]:\n",
    "        \"\"\"\n",
    "        Get the probability of kgram. If k == n, return the probability.\n",
    "        If k < n: return the frequency dict of the kgram.\n",
    "\n",
    "        Parameters\n",
    "        -------\n",
    "        pos: Union[List[str], str]\n",
    "            The kgram in the form of spaced str or listed strs.    \n",
    "        need_reg: bool\n",
    "            Whether perform regularization on the kgram.\n",
    "        need_lower: bool\n",
    "            Whether convert the kgram into lower case.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(pos, str):\n",
    "            if need_reg: pos = self.reg(pos)\n",
    "            pos = pos.split()\n",
    "        \n",
    "        if need_lower:\n",
    "            pos = [_.lower() for _ in pos]\n",
    "            \n",
    "        if len(pos) < self.n:\n",
    "            return reduce(lambda x, y: x[y], pos, self.prob)\n",
    "        \n",
    "        box = reduce(lambda x, y: x[y], pos[:-1], self.prob)\n",
    "\n",
    "        return (box[pos[-1]] + self.smoothk) /\\\n",
    "                    (box._boxsize + len(self.vocab) * self.smoothk)\n",
    "\n",
    "    def perplexity(self, \n",
    "            sentences: Union[List[str], str], \n",
    "            need_reg: bool = False, \n",
    "            need_lower: bool = True, \n",
    "            verbose: bool = False, \n",
    "            avg: bool = True\n",
    "        ) -> Union[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Compute the perplexity of a sentence or multiple sentences. The perplexity \n",
    "        is geometrically averaged on multiple sentences.\n",
    "\n",
    "        Parameters\n",
    "        -------\n",
    "        sentences: Union[List[str], str]\n",
    "            Sentences to compute perplexity on. No need to provide bos/eos token manually.\n",
    "        need_reg: bool\n",
    "            Whether perform regularization on each sentence.\n",
    "        need_lower: bool\n",
    "            Whether convert each sentence to lower case.\n",
    "        verbose: bool\n",
    "            If True, display a tqdm bar.        \n",
    "        avg: bool\n",
    "            If False, return all the perplexities on the multiple sentences instead of \n",
    "            just the average.\n",
    "        \"\"\"\n",
    "        only_one = isinstance(sentences, str)\n",
    "        if only_one:\n",
    "            sentences = [sentences]\n",
    "\n",
    "        result = []\n",
    "        verbose = (lambda x: x) if not verbose else tqdm\n",
    "        n = self.n\n",
    "        for words in verbose(sentences):\n",
    "            if need_reg: words = self.reg(words)\n",
    "            words = [self.bos_token] * (n-1) + words.split()\n",
    "            if False:# len(words) < n:\n",
    "                result.append(np.nan)\n",
    "            else:\n",
    "                words.append(self.eos_token)\n",
    "\n",
    "                if not self.ignore_unk:\n",
    "                    words = self._mask_unknown_word(words)\n",
    "\n",
    "                p = -np.log([self.__getitem__(words[i:i+n], need_reg = need_reg, need_lower = need_lower)\n",
    "                            for i in range(max(0, len(words) - n + 1))]).sum()\n",
    "\n",
    "                # compute the (geometric) mean\n",
    "                p /= (len(words))\n",
    "                result.append(np.exp(p))\n",
    "\n",
    "        if only_one: result = result[0]\n",
    "    \n",
    "        result = np.array(result)\n",
    "        if avg: result = np.exp(np.nanmean(np.log(result)))\n",
    "        return result\n",
    "\n",
    "    def gen(self, \n",
    "            start: Union[List[str], str], \n",
    "            length: int = 10\n",
    "        ) -> Union[List[str], str]:\n",
    "        \"\"\"\n",
    "        Generate sentences with given starts. Sentences are truncated \n",
    "        when encountering eos_token in generation.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        start: Union[List[str], str]\n",
    "            A single start or multiple starts (for multiple sentence generation).\n",
    "            No need to provide bos/eos token manually.\n",
    "        length: int\n",
    "            Maximum generation length.\n",
    "        \"\"\"\n",
    "        only_one = isinstance(start, str)\n",
    "        if only_one:\n",
    "            start = [start]\n",
    "\n",
    "        sentences = []\n",
    "        n = self.n\n",
    "        for words in start:\n",
    "            sentence = [self.bos_token] * (n-1) + self.reg(words).split()\n",
    "            buffer_ = sentence[1-n:].copy()\n",
    "            for _ in range(length):\n",
    "                box = reduce(lambda x, y: x[y], buffer_, self.prob) if self.n > 1 else self.prob\n",
    "                if box._boxsize == 0:\n",
    "                    break\n",
    "                \n",
    "                # we do not consider the unknown words / unseen ngrams here\n",
    "                prob = (np.array(list(box.values())) + 1) / (box._boxsize + len(box))\n",
    "                next_word = np.random.choice(list(box.keys()), p = prob)\n",
    "\n",
    "                # pop the first word from and add the new word to the buffer\n",
    "                buffer_ = buffer_[1:]\n",
    "                buffer_.append(next_word)\n",
    "                sentence.append(next_word)\n",
    "            sentences.append(' '.join(' '.join(sentence).replace(\n",
    "                        self.bos_token,'').replace(self.eos_token,'').strip().split()))\n",
    "            \n",
    "        if only_one:\n",
    "            sentences = sentences[0]\n",
    "        return sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "假设已知 $w_1,\\dotsc,w_{n-1}$, 语言模型预测下一个词是 $w_n$, 定义生成的句子的困惑度 (perplexity) 为\n",
    "\n",
    "$${\\rm PP}=\\sqrt[n]{\\frac{1}{\\mathbb P(w_1,\\dotsc,w_{n-1},w_n)}}$$\n",
    "\n",
    "一定程度上，困惑度越小越好, 说明生成结果的实际出现概率越大。\n",
    "\n",
    "以三元模型为例，一句话的概率为\n",
    "\n",
    "$$\\mathbb P(w_1,\\dotsc,w_{n-1},w_n) = \\mathbb P(w_n|w_{n-1},w_{n-2})\\cdot \\mathbb P(w_{n-1}|w_{n-2},w_{n-3})\n",
    "\\cdot \\dotsm \\cdot \\mathbb P(w_3|w_2,w_1) \\cdot \\mathbb P(w_2|w_1)\\cdot \\mathbb P(w_1)$$\n",
    "\n",
    "注意上式最右边的初始项。\n",
    "\n",
    "### 填充\n",
    "\n",
    "可以将每一句话的开头填充 (pad) 足够多用来表示开头的特殊词 “bos” (beginning of sentence) 或 “\\<s\\>”。以三元模型为例，如果每句话前面加上**两个** “bos”\n",
    "\n",
    "$$\\begin{aligned}\\mathbb P(w_1,\\dotsc,w_{n-1},w_n) &= \\mathbb P(w_n|w_{n-1},w_{n-2})\\cdot \\mathbb P(w_{n-1}|w_{n-2},w_{n-3})\n",
    "\\cdot \\dotsm \\cdot \\mathbb P(w_3|w_2,w_1) \\cdot \\mathbb P(w_2|w_1,{\\rm bos})\\cdot \\mathbb P(w_1|{\\rm bos}, {\\rm bos})\n",
    "\\\\ &= \\prod_{k=1}^n \\mathbb P(w_k|w_{k-1},w_{k-2})\\end{aligned}$$\n",
    "\n",
    "其中 $w_0=w_{-1}={\\rm bos}$。形式统一便于写代码处理。\n",
    "\n",
    "另外，每一句话结尾可以添加特殊词 “eos” (end of sentence) 或 “\\<\\\\s\\>”，用来表示句子结束的概率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
