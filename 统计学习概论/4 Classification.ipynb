{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 分类\n",
    "\n",
    "\n",
    "接下来看分类问题.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-近邻算法 K-Nearest Neighbor (KNN)\n",
    "\n",
    "对于每个数据点, 找到训练集中距离它最近的 $k$ 个点, 寻找其中最多的一类, 于是将这个数据点也分为该类.\n",
    "\n",
    "\n",
    "$k$ 是超参数, 可用交叉验证尝试.\n",
    "\n",
    "<br>\n",
    "\n",
    "如果是回归问题, 理论上也可以类似套用该方法: 每个数据点的值用其邻居的平均值估计."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 线性判别分析 Linear Discrimiant Analysis (LDA)\n",
    "\n",
    "假设 $X=x$ 属于 $k$ 类中的某一类: $c_1,\\dotsc,c_k$. 假设对于每个 $c_i$, 我们可以求出 $c_i$ 中 $X$ 的分布密度 $f_i(x) = f(X=x|c_i)$, 我们可以选取 $x$ 对应的分布密度最大的一类将其归类.\n",
    "\n",
    "\n",
    "甚至还能用贝叶斯公求出 $x$ 在每一类的概率: \n",
    "$$\\mathbb P(c_j|X=x) = \\frac{\\mathbb P(c_j) f(x|c_j)}{\\sum_i \\mathbb P(c_i)f(x|c_i)}$$\n",
    "其中 $\\mathbb P(c_i)$ 是类 $i$ 出现的先验概率 (可以用训练集中每一类出现的频率估计), $\\sum \\mathbb P(c_i) = 1$.\n",
    "\n",
    "<br>\n",
    "\n",
    "如果假设在每一类 $c_j$ 中, $X$ 的分布为正态的:\n",
    "$$f(x|c_j) = \\frac{1}{{(2\\pi )}^{p/2}|\\Sigma_j|^\\frac 12}\\exp\\left\\{-\\frac 12(x-\\mu_j)^T\\Sigma_j^{-1}(x-\\mu_j)\\right\\}$$\n",
    "其中 $\\mu_j,\\Sigma_j$ 为分布均值与方差. 可利用训练集的数据首先求得这两个参数.\n",
    "\n",
    "\n",
    "进一步, 如果假设每一类方差相等: $\\Sigma_1=\\dotsc = \\Sigma_k = \\Sigma$, 那么两个类的对数似然之比为:\n",
    "$$\\begin{aligned}\\log\\frac{\\mathbb P(c_{j_1}|X=x)}{\\mathbb P(c_{j_2}|X=x)}&\n",
    "=-\\frac 12(x-\\mu_{j_1})^T\\Sigma ^{-1}(x-\\mu_j)+\\frac 12(x-\\mu_{j_2})^T\\Sigma ^{-1}(x-\\mu_{j_2})\n",
    "+\\log \\frac{\\mathbb P(c_{j_1})}{\\mathbb P(c_{j_2})}\n",
    "\\\\ &\n",
    "=(\\mu_{j_1}-\\mu_{j_2})^T\\Sigma ^{-1}x-\\frac 12(\\mu_{j_1}+\\mu_{j_2})^T\\Sigma ^{-1}(\\mu_{j_1}-\\mu_{j_2})+\\log \\frac{\\mathbb P(c_{j_1})}{\\mathbb P(c_{j_2})}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "它是关于 $x$ 的线性函数. 即任取两个类, 对其中某个的倾向可以用线性分类, 故称线性判别分析."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯 Naive Bayes (NB)\n",
    "\n",
    "类似线性判别分析 (LDA), \n",
    "\n",
    "$$\\mathbb P(Y=c_j|X=x) = \\frac{\\mathbb P(Y=c_j) f(x|Y=c_j)}{\\sum_i \\mathbb P(Y=c_i)f(x|Y=c_i)}$$\n",
    "\n",
    "使用条件独立性假设: 向量 $X\\in\\mathbb R^p$ 的每个维度互相独立, 则\n",
    "$$f (X = x|Y = c) = \\prod_{i=1}^p f (X^{(i)}=x^{(i)}|Y=c)$$\n",
    "\n",
    "若 $X$ 在每个维度的取值是离散的, 可用训练集中数据的频率估计 $f (X^{(i)}=x^{(i)}|Y=c)$.\n",
    "\n",
    "对于任意 $x$, 取概率最大的一类:\n",
    "$$\\begin{aligned}y &= {\\rm argmax}_y\\ \\mathbb P(Y= y|X =x)= {\\rm argmax}_y \\ \\frac{\\mathbb P(Y=y) f(x|Y=y)}{f(x)}\\\\ &=  {\\rm argmax}_y \\ {\\mathbb P(Y=y) f(x|Y=y)}\\\\& = {\\rm argmax}_y \\ \\mathbb P(Y=y) \\prod_{i=1}^p f (X^{(i)}=x^{(i)}|Y=y)\\end{aligned}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**定理** 假设 $(X,Y)$ 来自全空间的联合分布 $(\\Omega,\\mathcal F,\\mathbb P)$, 则如上朴素贝叶斯 (后验概率最大化) 等价于期望风险最小化. 其中期望风险为:\n",
    "$$\\begin{aligned}R(\\varphi) &= \\mathbb E(\\mathbb I_{Y\\neq \\varphi(X)})=\\int \\mathbb E(\\mathbb I_{Y\\neq \\varphi(X)}\n",
    "|X=x)f(X = x)dx\\\\ &\n",
    "=\\int \\mathbb P(Y\\neq \\varphi(x)|X=x)f(X = x)dx\\\\ & \n",
    "=\\int \\left( 1 -\\mathbb  P(Y=\\varphi(x)|X=x)\\right) f(X = x)dx.\n",
    "\\end{aligned}$$\n",
    "\n",
    "因此最小化 $R(\\varphi)$ 就是对于每个 $x$, 最大化 $\\mathbb P(Y = \\varphi(x)|X = x)$, 即朴素贝叶斯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class NaiveBayes:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Naive Bayes Classifier. See function `fit` for details.\"\"\"\n",
    "        self.feature_prob = None\n",
    "        self.class_prob = None\n",
    "    \n",
    "    def fit(self, \n",
    "            X: np.ndarray, \n",
    "            y: np.ndarray, \n",
    "            class_num: Optional[np.ndarray] = None, \n",
    "            class_num_y: Optional[int] = None, \n",
    "            smooth: float = 1.\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Fit on data (X, y).\n",
    "\n",
    "        Parameters\n",
    "        -------\n",
    "        X: ndarray[int], shape (N, k)\n",
    "            Feature matrix on which to make prediction. k is the dimension of features.\n",
    "        y: ndarray[int], shape (N,)\n",
    "            Ground truth labels, integers within 0 ~ classnum - 1.\n",
    "        class_num: Optional[np.ndarray]\n",
    "            Each feature has c_i classes (0 <= i < k). \n",
    "            Provide the class number of each feature.\n",
    "            If None, it will be inferenced by the maximum index + 1 in the training dataset.\n",
    "        class_num_y: Optional[int]\n",
    "            The class number of `y`. If None, it will be inferenced from the maximum index + 1 of `y`.\n",
    "        smooth: float\n",
    "            Parameter for Bayes smoothing. Defaults to 1 (Laplacian smoothing).\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate the number of classes of each feature\n",
    "        if class_num is None:   class_num   = np.max(X, axis = 0) + 1\n",
    "        if class_num_y is None: class_num_y = np.max(y) + 1\n",
    "\n",
    "        N, K, p = X.shape[0], X.shape[1], class_num.max()\n",
    "        self.feature_prob = np.zeros((class_num_y, K, p))\n",
    "        self.class_prob   = np.zeros(class_num_y)\n",
    "        for c in range(class_num_y):\n",
    "            self.class_prob[c] = (y == c).sum()\n",
    "            for l in range(p):\n",
    "                # conditional count\n",
    "                self.feature_prob[c, :, l] = ((X == l) & np.tile((y == c).reshape((N, 1)), (1, K))).sum(axis = 0)\n",
    "        \n",
    "        self.feature_prob = (self.feature_prob + smooth) / \\\n",
    "            (np.tile(np.reshape(self.class_prob, (-1, 1)), (1, K)) + np.reshape(class_num * smooth, (1, K))).reshape((-1, K, 1))\n",
    "        self.class_prob = (self.class_prob + smooth) / (N + class_num_y * smooth)\n",
    "\n",
    "        # take logarithm to prevent underflow in multiplication\n",
    "        self.feature_prob = np.transpose(np.log(self.feature_prob), (1, 2, 0)) # -> (K, p, c)\n",
    "        self.class_prob = np.log(self.class_prob)\n",
    "\n",
    "    def predict(self, \n",
    "            X: np.ndarray, \n",
    "            return_prob: bool = False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Make prediction on `X`. \n",
    "        \n",
    "        Parameters\n",
    "        -------\n",
    "        X: ndarray[int], shape (N, k)\n",
    "            Feature matrix on which to make prediction.\n",
    "        return_prob: bool\n",
    "            If True, return the logarithm of probability.\n",
    "            If False, return the class with maximum predicted probability.\n",
    "        \"\"\"\n",
    "        # we use summation because we have taken logarithm on probability\n",
    "        prob = self.feature_prob[np.arange(self.feature_prob.shape[0]), X].sum(axis = 1) # (N, c)\n",
    "        prob += self.class_prob.reshape((1, -1))\n",
    "\n",
    "        if return_prob:\n",
    "            return prob\n",
    "        return np.argmax(prob, axis = 1)\n",
    "\n",
    "    def test(self, \n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray, \n",
    "            label_names: Optional[List[str]] = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Make test on `X` given labels `y`. Return a cross table.\n",
    "        \n",
    "        Parameters\n",
    "        -------\n",
    "        X: ndarray[int], shape (N, k)\n",
    "            Feature matrix on which to make prediction.\n",
    "        y: ndarray[int], shape (N,)\n",
    "            Ground truth labels, integers within 0 ~ classnum - 1.\n",
    "        label_names: Optional[List[str]]\n",
    "            Name of each label.\n",
    "        \"\"\"\n",
    "        pred_y = self.predict(X)\n",
    "        p = self.class_prob.size\n",
    "        \n",
    "        if label_names is None:\n",
    "            label_names = [str(i) for i in range(p)]\n",
    "\n",
    "        cross_tab = np.zeros((p+1, p+1), dtype = 'int32')\n",
    "        for i in range(p):\n",
    "            for j in range(p):\n",
    "                cross_tab[i,j] = ((pred_y == j) & (y == i)).sum()\n",
    "        cross_tab[-1,:] = cross_tab.sum(axis = 0)\n",
    "        cross_tab[:,-1] = cross_tab.sum(axis = 1)\n",
    "        \n",
    "        print('Accuracy = %.2f%%\\n'%(100. * (pred_y == y).mean()) + '=' * 60)\n",
    "        print('True \\ Pred')\n",
    "        label_names = label_names + ['Total']\n",
    "        cross_tab = pd.DataFrame(cross_tab, columns = label_names, index = label_names)\n",
    "        return cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.T =\n",
      "[[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n",
      " [0 1 1 0 0 0 1 1 2 2 2 1 1 2 2]]\n",
      "y   =\n",
      " [0 0 1 1 0 0 0 1 1 1 1 1 1 1 0]\n",
      "\n",
      "Pred X =\n",
      "[[1 0]]\n",
      "Prob   =\n",
      "[[0.06100218 0.03267974]]\n"
     ]
    }
   ],
   "source": [
    "# example borrowed from <统计学习方法> 李航 p. 52\n",
    "nb = NaiveBayes()\n",
    "X = np.array([[1,1,1,1,1,2,2,2,2,2,3,3,3,3,3], [1,2,2,1,1,1,2,2,3,3,3,2,2,3,3], [1,1,2,2,1,1,1,2,2,2,2,2,2,2,1]]) - 1\n",
    "X, y = X[:-1].T, X[-1]\n",
    "print('X.T =\\n%s\\ny   =\\n %s'%(X.T, y))\n",
    "nb.fit(X, y)\n",
    "\n",
    "pred_X = np.array([[2, 1]]) - 1\n",
    "pred_prob = np.exp(nb.predict(pred_X, return_prob = True))\n",
    "print('\\nPred X =\\n%s\\nProb   =\\n%s'%(pred_X, pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 73.33%\n",
      "============================================================\n",
      "True \\ Pred\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Feature 1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature 2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature 1  Feature 2  Total\n",
       "Feature 1          3          3      6\n",
       "Feature 2          1          8      9\n",
       "Total              4         11     15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.test(X, y, label_names = ['Feature 1', 'Feature 2'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
