{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 线性回归\n",
    "\n",
    "线性回归是线性模型. 形式为\n",
    "$$y_i = x_i^T\\beta + \\epsilon_i$$\n",
    "其中 $y_i,\\epsilon_i \\in\\mathbb R$, $x_i,\\beta\\in\\mathbb R^p$. $y_i$ 是因变量 (response), $x_i$ 是自变量 (covariate).\n",
    "\n",
    "假设随机性只在 $\\epsilon_i$ 上且 $\\epsilon_i \\sim N(0,\\sigma^2)$ 相互独立.\n",
    "令 $X=[x_1,\\dotsc,x_N]^T\\in\\mathbb R^{N\\times p}$, $y=[y_1,\\dotsc,y_N]^T\\in \\mathbb R^n$. 为了使得 $\\Vert X\\beta - y\\Vert$ 最小, 当 $X$ 满列秩时可以求得 $\\hat \\beta = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "当 $X$ 不列满秩, 可以证明所有最小解 $\\hat\\beta$ 均满足 $X^TX\\hat \\beta = X^Ty$.\n",
    "\n",
    "\n",
    "## 统计量\n",
    "\n",
    "因为 $\\hat \\beta = (X^TX)^{-1}X^T y = (X^TX)^{-1}X^T(X\\beta +\\epsilon) = \\beta + (X^TX)^{-1}X^T\\epsilon$, 故\n",
    "\n",
    "$$\\begin{aligned}\\mathbb E(\\hat \\beta) &= \\mathbb E((X^TX)^{-1}X^T(X\\beta +\\epsilon )) = \\beta \\\\\n",
    "{\\rm Cov}(\\hat \\beta) &= \\mathbb E\\left((\\hat\\beta - \\beta)(\\hat \\beta^T - \\beta^T)\\right)\n",
    "=\\mathbb E((X^TX)^{-1}X^T\\epsilon\\epsilon^TX(X^TX)^{-1} ) =\\sigma^2 (X^TX)^{-1}\n",
    "\\end{aligned}$$\n",
    "\n",
    "对于 $\\sigma^2$, 有无偏估计 $\\hat \\sigma^2 = \\dfrac{1}{N- p}\\Vert \\hat y - y\\Vert^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯-马尔科夫定理\n",
    "\n",
    "(Gauss-Markov Theorem) 如果存在, 最佳线性无偏估计量 (best linear unbiased estimator, BLUE) 就是如上线性回归的解 $\\hat\\beta$.\n",
    "\n",
    "证明: 假设 $\\hat\\beta$ 是 BLUE, 下证它是线性回归的解. 根据线性, 可设 $\\hat \\beta = Ay+r$. 如果 $y$ 的真实值满足 $y = X\\beta + \\epsilon$, $\\epsilon \\sim N(0,\\sigma^2I)$, 则无偏性要求 \n",
    "$$\\beta \\equiv \\mathbb E(\\hat \\beta) = \\mathbb E(A(X\\beta + \\epsilon)+r) = AX\\beta+r\\ \\Rightarrow\\ AX\\beta = \\beta,\\ r=0 .$$\n",
    "\n",
    "上式对于任意 $\\beta$ 成立, 故 $AX = I_p$. 若 ${\\rm rank}(X)<p$, 则 ${\\rm rank}(AX)\\leqslant {\\rm rank}(X)$ 矛盾. 因此 $X$ 需要满秩, 可设分解 $X = Q_1R$, 其中 $R\\in \\mathbb R^{p\\times p}$ 可逆, $Q_1\\in\\mathbb R^{N\\times p}$ 使得 $Q_1^TQ_1=I_p$. 进而,\n",
    "$$AQ_1R=I_p\\ \\Rightarrow\\ A=R^{-1}Q_1^T + BQ_2^T$$\n",
    "其中 $Q_2\\in \\mathbb R^{N\\times (N-p)}$ 是 $Q_1$ 的正交补,  $B$ 是任意的矩阵 (但与 $\\epsilon$ 独立). 可以计算 $\\hat \\beta$ 的方差:\n",
    "$${\\rm Var}(\\hat \\beta) = {\\rm Var}(A\\epsilon) = \\epsilon^2AA^T=\n",
    "\\epsilon^2\\left(R^{-1}R^{-T}+BB^T\\right)\\succeq \\epsilon^2 R^{-1}R^{-T}.$$\n",
    "取等当且仅当  $A = R^{-1}Q_1^T$, 此时对应 $\\hat\\beta$ 为 BLUE. 这个时候,\n",
    "$$(X^TX)^{-1}X^Ty=(R^TR)^{-1}R^TQ_1^Ty=R^{-1}Q_1^Ty = Ay =\\hat \\beta.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 假设检验\n",
    "\n",
    "假若 $X$ 列满秩, $\\epsilon$ 为 i.i.d. 的正态分布 $N(0,\\sigma^2)$, 则 \n",
    "$$\\hat\\beta \\sim N(\\beta, (X^TX)^{-1}\\sigma^2)\\quad\\quad (N-p)\\hat \\sigma^2 \\sim \\sigma^2\\chi_{N-p}^2$$\n",
    "且两者独立.\n",
    "\n",
    "若要检验 $H_0:\\ \\beta_1 = \\dotsc = \\beta_{p_0} = 0$ 与 $H_1:\\ \\neg p_0$, 可检验\n",
    "$$F = \\frac{({\\rm SSR}_0 - {\\rm SSR}_1)/p_0}{{\\rm SSR}_1/(N-p_1)}\\sim F(p_0,N-p_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拟合优度\n",
    "\n",
    "(Goodness of fit)\n",
    "\n",
    "定义总方差 (total sum of squares) ${\\rm SST} = \\Vert y -\\bar y\\Vert^2$, 可解释方差 (explained sum of squares) ${\\rm SSE} = \\Vert \\hat y - \\bar y\\Vert^2$, 残方差 (residual some of squares) ${\\rm SSR} = \\Vert y - \\hat y\\Vert^2$. \n",
    "\n",
    "若 $X$ 有一列是 $1$, 可以证明 \n",
    "${\\rm SST} = {\\rm SSE}+{\\rm SSR}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "定义 $R^2  = \\dfrac{\\rm SSE}{\\rm SST} = 1 - \\dfrac{\\rm SSR}{\\rm SST}\\in [0,1]$. 或者调整后的 $R^2$:\n",
    "$${\\rm Adjusted\\ }R^2 = 1 - \\frac{\\rm SSR}{\\rm SST}\\cdot \\frac{N-1}{N-p}$$\n",
    "\n",
    "希望 $R^2$ 越大越好, 即 ${\\rm SSR}$ 占比小.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型选择\n",
    "\n",
    "(Model selecetion)\n",
    "\n",
    "### 子集选择\n",
    "\n",
    "实际应用中, 变量可能有很多特征 (维度 $d$ 很大), 但是有些特征和需要预测的目标无关或不成线性关系. 因此要筛选一部分特征, 即子集选择 (subset selection).\n",
    "\n",
    "为此, 首先定义 AIC, BIC 指标为 ${\\rm AIC} = -\\dfrac{2}{N}\\mathcal L(\\beta)  +\\dfrac{2p}{N}$, ${\\rm BIC} = -2\\mathcal L(\\beta) + p\\log N $. 其中 $\\mathcal L$ 表示对数似然 (log-likelihood), $N$ 为数据量, $p$ 为模型参数量. \n",
    "\n",
    "希望模型使得 AIC, BIC 越小越好. 由于 BIC 在参数量 $p$ 上有额外的 $\\log N$ 权重, 它更倾向于选择小的模型.\n",
    "\n",
    "<br>\n",
    "\n",
    "注: 因为 $\\sum_{k=1}^N \\Vert y_k - x_k^T\\beta\\Vert^2 =(N-p)s^2$, 故\n",
    "\n",
    "$$\\mathcal L(\\beta) = \\log \\left(\\frac{1}{\\sqrt{2\\pi s^2}^N}\\prod_{k=1}^N \\exp\\{-\\frac{1}{2s^2}\\Vert y_k - x_k^T\\beta\\Vert\\}\\right)\n",
    " = -\\frac{N}{2}\\log (2\\pi s^2)-\\frac{N-p}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def AIC(X, y, beta = None):\n",
    "    if beta is None:\n",
    "        beta = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "    N, p = X.shape\n",
    "    norm = np.sum(np.square(y - X @ beta))\n",
    "    s2 = norm / (N - p)\n",
    "    logL = -N/2. * np.log(2*np.pi*s2) - .5 * (N-p)\n",
    "    return (p - logL) * 2. / N\n",
    "\n",
    "def BIC(X, y, beta = None):\n",
    "    if beta is None:\n",
    "        beta = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "    N, p = X.shape\n",
    "    norm = np.sum(np.square(y - X @ beta))\n",
    "    s2 = norm / (N - p)\n",
    "    logL = -N/2. * np.log(2*np.pi*s2) - .5 * (N-p)\n",
    "    return -2 * logL + p * np.log(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "子集选择则有如下方法:\n",
    "\n",
    "1. 最佳子集选择 (best-subset selection): 考虑所有特征的 $2^d$ 个子集, 逐个比较模型的 AIC, BIC. 不过此方法时间复杂度呈指数增长, 不实用.\n",
    "\n",
    "2. 前向逐步回归 (forward-stepwise selection): 从没有特征开始, 每次选择一个特征做拟合使得 AIC 或 BIC 最小, 加入到选择的特征集合中.\n",
    "\n",
    "3. 反向逐步回归 (backward-stepwise selection): 从全部特征开始, 每次删除一个特征做拟合使得 AIC 或 BIC 最小.\n",
    "\n",
    "### 收缩方法\n",
    "\n",
    "收缩方法 (shrinkage method) 抑制模型过拟合.\n",
    "\n",
    "1. 岭回归 (ridge regression): 引入一个参数 $\\lambda$, 求得如下极值点\n",
    "$$\\hat\\beta_{\\rm ridge} = {\\rm argmin}_\\beta \\left\\{\\Vert X\\beta - y\\Vert^2 + \\lambda \\Vert \\beta\\Vert^2\\right\\}$$\n",
    "可得 $\\hat\\beta_{\\rm ridge} = (X^TX + \\lambda I)^{-1}X^Ty$.\n",
    "\n",
    "2. LASSO: 在岭回归中改为使用 1 范数\n",
    "$$\\hat\\beta_{\\rm ridge} = {\\rm argmin}_\\beta \\left\\{\\Vert X\\beta - y\\Vert^2 + \\lambda \\Vert \\beta\\Vert_{1}^2\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackwardStepwiseSelection(X, y, metric = BIC):\n",
    "    selection = list(range(X.shape[1]))\n",
    "    all_bics  = [(metric(X, y), tuple(selection))]\n",
    "    for i in range(X.shape[1] - 1):\n",
    "        bics = []\n",
    "        # iterate through all remaining variables to seek for minimum metric\n",
    "        for index in selection[:]:\n",
    "            selection.remove(index)\n",
    "            bics.append((metric(X[:,selection], y), index))\n",
    "            selection.append(index) # restore\n",
    "\n",
    "        bic, best = min(bics)\n",
    "        all_bics.append((bic, tuple(selection)))\n",
    "        # remove the best redundant candidate and move on\n",
    "        selection.remove(best)\n",
    "    return min(all_bics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
