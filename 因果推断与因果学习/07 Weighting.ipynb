{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 Weighting\n",
    "\n",
    "Exchangeability assumption implies that $\\mathbb E(Y^a) = \\mathbb E(Y|A=a)$. However, this is not guaranteed in observational studies.\n",
    "\n",
    "\n",
    "## Survey Weights\n",
    "\n",
    "\n",
    "### Re-Weighting\n",
    "\n",
    "If we can survey everybody (the whole population), then the estimated outcome is simply the average: $\\widehat{\\mathbb E}(Y)= \\frac 1N \\sum_{i=1}^N Y_i$. The problem is that we can cover part of the population. Assume we have $n$ samples observed. Ideally, it would be \n",
    "$$\\widehat{\\mathbb E}(Y) = \\frac 1n \\sum_{i=1}^n Y_i.$$\n",
    "\n",
    "However, when the confounding variables are taken into consideration, we need to be aware that the confounding variables of the sampled distribution is not identical to the general population. Therefore, we need to re-weigh  the samples to make the confounding variables of the sampled distribution identical to the general population.\n",
    "\n",
    "Confounding variables that are sampled frequently should be given less weight, and confounding variables that are sampled less frequently should be given more weight.\n",
    "\n",
    "### Horvitz-Thompson Estimator\n",
    "\n",
    "Assume the probability of unit $w_i\\ (i=1,\\dotsc,N)$ being sampled is $p_i$. Assume the sample size is $n \\ (n<N)$. Then the Horvitz-Thompson estimator is\n",
    "$$\\widehat{\\mathbb E}(Y) =\\frac 1n \\sum_{i\\ {\\rm is \\ sampled}}\\frac{1}{p_i}\\cdot \\frac{n}{N}\\cdot Y_i= \\frac{1}{N}\\sum_{i=1}^N \\mathbb I_{i\\ {\\rm is \\ sampled}}\\cdot \\frac{Y_i}{p_i}.$$\n",
    "\n",
    "The weight $\\frac{1}{p_i}\\cdot \\frac{n}{N}$ adjusts the sample distribution. The estimator is unbiased.\n",
    "\n",
    "**Proof** $$\\mathbb E\\left[\\widehat{\\mathbb E}(Y) \\right]= \\frac{1}{N}\\sum_{i=1}^N \\mathbb E(\\mathbb I_{i\\ {\\rm is \\ sampled}})\\cdot \\frac{Y_i}{p_i}= \\frac{1}{N}\\sum_{i=1}^N Y_i.$$\n",
    "\n",
    "<br>\n",
    "\n",
    "We can see that the Horvitz-Thompson estimator is degenerated when each unit has identical probability to be sampled, i.e. $p_1 = \\dotsc = p_N = \\frac {n}{N}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g-Formula\n",
    "\n",
    "### Re-Weighting\n",
    "\n",
    "We compare the g-formula with the idea of Horvitz-Thompson estimator. If we want to estimate $\\mathbb E(Y^a)$, the  alternative choice is $\\mathbb E(Y|A=a)$ when exchangeability holds:\n",
    "\n",
    "$$\\mathbb E(Y|A=a) = \\sum_{c\\in C}\\mathbb E(Y|C=c,A=a)\\mathbb P(C=c|A=a) .$$\n",
    "\n",
    "However, for each unit with attribute $c$, the probability of it being treated $\\mathbb P(A=1|C=c) = e(c)$ is not $\\frac12$ when exchangeability does not hold. Therefore, we should assign an inverse weight $\\frac{1}{e(c)}\\cdot \\mathbb P(A=1)$ to each unit with attribute $c$:\n",
    "\n",
    "$$\\begin{aligned}\\mathbb E(Y^1) &= \\sum_{c\\in C}\\mathbb E(Y|C=c,A=1)\\mathbb P(C=c|A=1)\\cdot  \\frac{\\mathbb P(A=1)}{e(c)}\n",
    "\\\\&= \\sum_{c\\in C}\\mathbb E(Y|C=c,A=1)\\mathbb P(C=c).\n",
    "\\end{aligned}$$\n",
    "\n",
    "The second equation is obtained by Bayes' rule.\n",
    "\n",
    "### Estimator \n",
    "\n",
    "As the Horvitz-Thompson estimator has the second form $\\frac{1}{N}\\sum_{i=1}^N \\mathbb I_{i\\ {\\rm is \\ sampled}}\\cdot \\frac{Y_i}{p_i}$, the g-formula also has another form. It sums over the whole sample but extracts the treated units:\n",
    "\n",
    "$$\\widehat{\\mathbb E}(Y^a) = \\frac{1}{N}\\sum_{i=1}^N \\mathbb I_{A_i=1}\\cdot \\frac{Y_i}{e(c_i)}.$$\n",
    "\n",
    "Here $e(c)$ is the propensity score, $e(c) = \\mathbb P(A=1|C=c)$. The estimator is unbiased.\n",
    "\n",
    "**Proof** We will show that it is unbiased:\n",
    "\n",
    "$$\\begin{aligned}\\mathbb E\\left[\\widehat{\\mathbb E}(Y^a) \\right]&=  \\frac{1}{N}\\sum_{i=1}^N \\mathbb E\\left[ \\mathbb I_{A_i=1}\\cdot \\frac{Y_i}{e(c_i)}\\right]=   \\frac{1}{N}\\sum_{i=1}^N \\frac{\\mathbb P(A_i=1|C=c_i)Y^1}{e(c_i)}\n",
    "=   \\frac{1}{N}\\sum_{i=1}^N Y^1 =\\mathbb E(Y^1).\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "As a result, we can see that the average treatment effect, ACE, can be estimated by\n",
    "\n",
    "$$\\begin{aligned}\\widehat{\\mathbb E}(\\text{ACE}) &=  \\frac{1}{N}\\sum_{i=1}^N \\mathbb I_{A_i=1}\\cdot \\frac{Y_i}{e(c_i)}\\ -\\ \\frac{1}{N}\\sum_{i=1}^N \\mathbb I_{A_i=0}\\cdot \\frac{Y_i}{1-e(c_i)}\\\\ &=  \\frac{1}{N}\\sum_{i=1}^N \\mathbb \\cdot \\frac{A_iY_i}{e(c_i)}\\ -\\ \\frac{1}{N}\\sum_{i=1}^N \\cdot \\frac{(1-A_i)Y_i}{1-e(c_i)}.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Probability Weighting\n",
    "\n",
    "If we denote \n",
    "$$W_i = \\left\\{\\begin{array}{ll}\\frac{1}{e(c_i)} & A_i=1,\\\\ \\frac{1}{1-e(c_i)} & A_i=0\\end{array}\\right.\n",
    "=\\frac{1}{e(c_i)^{A_i}(1-e(c_i))^{1-A_i}},$$\n",
    "then we can write \n",
    "$$\\widehat{\\text{ACE}} = \\frac{1}{N}\\sum_{i=1}^N W_iA_iY_i - \\frac{1}{N}\\sum_{i=1}^N W_i(1-A_i)Y_i.$$\n",
    "\n",
    "This is called the inverse probability (IP) weighting.\n",
    "\n",
    "### Expectation\n",
    "\n",
    "Recall $A_i\\sim B(e(c_i))$ is a Bernoulli random variable with probability $e(c_i)$. $W_i$ is a function of $A_i$. We have formulas\n",
    "$$\\mathbb E(W_iA_i)=1,\\quad \\mathbb E(W_i(1-A_i))=1.$$\n",
    "\n",
    "\n",
    "### Pseudo-Population\n",
    "\n",
    "\n",
    "### Stablized Inverse Probability Weighting\n",
    "\n",
    "We use a new estimator:\n",
    "$$\\widehat{\\text{ACE}} = \\frac{1}{\\sum_{i=1}^N W_iA_i}\\sum_{i=1}^N W_iA_iY_i - \\frac{1}{\\sum_{i=1}^N W_i(1-A_i)}\\sum_{i=1}^N W_i(1-A_i)Y_i.$$\n",
    "\n",
    "Since $\\frac{\\sum_{i=1}^N W_iA_i}{N}\\rightarrow_\\mathbb P 1$ by law of large numbers, we can see that the new estimator has similar asymptotic properties as the original IP weighting estimator."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
