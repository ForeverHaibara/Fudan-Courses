{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Variogram and Kriging\n",
    "\n",
    "## Variogram\n",
    "\n",
    "Variogram is a generalization of the autocovariance in the time series. Suppose $Z(s)$ is a random field with respect to the location $s\\in\\mathbb R^n$, then we define \n",
    "$$2\\gamma(s_1,s_2) = {\\rm Var}(Z(s_1) - Z(s_2)).$$\n",
    "And we call $2\\gamma(s_1,s_2)$ the variogram and $\\gamma(s_1,s_2)$ the semi-variogram. \n",
    "\n",
    "### Stationarity\n",
    "\n",
    "When $2\\gamma(s_1,s_2)$ only depends on the difference $h = s_1 - s_2$, i.e.\n",
    "$$2\\gamma(h) =  2\\gamma(s_1,s_2)= {\\rm Var}(Z(s+h) - Z(s)),$$\n",
    "then we call it is stationary. Clearly $2\\gamma(h) = 2\\gamma(-h)$. \n",
    "\n",
    "Also, In the case of stationarity, we have \n",
    "$$2\\gamma(h) = {\\rm Var}(Z(s+h) - Z(s)) = {\\rm Cov}(Z_{s+h},Z_{s+h})-2{\\rm Cov}(Z_{s+h},Z_s)+{\\rm Cov}(Z_{s+h},Z_{s+h}).=2k(0) - 2k(h)$$\n",
    "where $k(h) = {\\rm Cov}(Z_{s+h}, Z_s)$ is the covariance.\n",
    "\n",
    "### Isotropicity\n",
    "\n",
    "Further, if $2\\gamma(s_1,s_2)$ only depends on the distance $h = \\Vert s_1 - s_2\\Vert\\in\\mathbb R_+$, then we call it is \n",
    "isotropic. Otherwise it is anisotropic.\n",
    "$$2\\gamma(h) = 2\\gamma(\\Vert s_1 - s_2\\Vert).$$\n",
    "\n",
    "### Conditional Negativity\n",
    "\n",
    "A function $\\gamma(s,t)$ is a semi-variogram if and only if it is conditionally negative, that is, given $w\\in \\mathbb R^m$ such that $\\sum_{i=1}^m w_i = 0$ we have for whatever locations $x_1,\\dotsc,x_m$ that \n",
    "$$\\sum_{i=1}^n\\sum_{j=1}^n w_i\\gamma(x_i,x_j)w_j\\leqslant 0.$$\n",
    "\n",
    "Proof: When $\\gamma(s,t)$ is a semi-variogram, \n",
    "$$\\begin{aligned}2\\sum_{i=1}^n\\sum_{j=1}^n w_i\\gamma(x_i,x_j)w_j\n",
    "&=\\sum_{i=1}^n \\sum_{j=1}^n w_i{\\rm Var}(Z(x_i) - Z(x_j))w_j  \\\\ \n",
    "&=\\sum_{i=1}^n \\sum_{j=1}^n \\left(-2w_i{\\rm Cov}(Z(x_i),Z(x_j))w_j + \\left({\\rm Var}(Z(x_i))+{\\rm Var}(Z(x_j))\\right)w_iw_j\\right) \\\\ \n",
    "&= \\sum_{i=1}^n \\sum_{j=1}^n -2w_i{\\rm Cov}(Z(x_i),Z(x_j))w_j\\\\\n",
    "&=-2{\\rm Var}\\left(\\sum_{i=1}^n   w_iZ(x_i)\\right)\\leqslant 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Some parameters of variogram are listed below. \n",
    "\n",
    "* Sill:$\\quad$\n",
    "Sill is the limit of variogram when $h\\rightarrow \\infty$, $\\lim_{h\\rightarrow \\infty}\\gamma(h)$.\n",
    "\n",
    "* Range:$\\quad$\n",
    "Range is literally defined by the before variogram (approximately) reaches the sill.\n",
    "\n",
    "* Nugget Effect:$\\quad$\n",
    "If two points are very close, then in practice the variogram is hard to accurately measure because of the dominating measurement error by the noise, $c_0 = \\lim_{h\\rightarrow 0}\\gamma(h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Variogram\n",
    "\n",
    "Denote $N(h) = \\{(i,j):\\ s_i - s_j=h\\}$ and \n",
    "$$2\\hat \\gamma(h) = \\frac{1}{|N(h)|}\\sum_{(i,j)\\in N(h)}(Z(s_i) - Z(s_j))^2.$$\n",
    "We can plot it in the form of variogram cloud, or in bar charts. \n",
    "\n",
    "1. To study the variogram, one can first plot variogram is various directions to check whether the model is isotrophic.\n",
    "\n",
    "2. Nonstationarity might be observed by diagnosing variogram at different subsets in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kriging\n",
    "\n",
    "Kriging stands for the Best Linear Unbiased Prediction (BLUP) for spatial statistics. In a geostatistical process, we use the hierachical modeling given by the two equations:\n",
    "$$\\begin{aligned}\n",
    "Z(s_i) \\  | \\ Y(s_i) &\\sim N(Y(s_i),\\sigma^2)\\\\\n",
    "Y(s)\\  & \\sim \\   {\\rm GP}(m(s)^T\\beta, k(s,s'))\\end{aligned}.\n",
    "$$\n",
    "Or equivalently, if we denote the observed data points by $S = [s_1,\\dotsc,s_m]\\in \\mathbb R^{n\\times m}$,\n",
    "$$Z(S)\\sim m(S)^T\\beta +\\epsilon(S).$$\n",
    "The noise $\\epsilon (S)\\sim N(0,\\Sigma)$ but $\\Sigma$ is not diagonal. This indicates that, the result $Z(\\cdot)$ is a linear combination (with coefficients $\\beta$) of certain features $m(\\cdot)$ plus some errors $\\epsilon(\\cdot)$. Obviously it is some generalization of the linear regression.\n",
    "\n",
    "### Prediction\n",
    "\n",
    "If we have now observed $Z(S)$ on various $S$, then we can predict the value at $s_0$ by a linear combination, \n",
    "$$\\hat Z(s_0) = \\lambda_0 + \\sum_{i=1}^n \\lambda_i Z(S_i) = \\lambda_0 + Z(S)\\lambda.$$\n",
    "\n",
    "#### Unbiasedness\n",
    "\n",
    "We claim that $\\hat Z(s_0)$ is an unbiased estimator for $Z(s_0) = m(s_0)^T\\beta + w$ iff $\\lambda_0 = 0$ \n",
    "\n",
    "Proof: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
