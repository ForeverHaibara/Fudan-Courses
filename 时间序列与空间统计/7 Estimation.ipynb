{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Estimation\n",
    "\n",
    "Suppose we have several observations $x_1,\\dotsc,x_n$ and we want to estimate the parameters of ${\\rm ARMA}(p,q)$ where $p,q$ are known. That is, to estimate\n",
    "$$\\phi_1,\\dotsc,\\phi_p,\\theta_1,\\dotsc,\\theta_q,\\sigma_w^2$$\n",
    "based on the observations.\n",
    "\n",
    "There are three methods, the method of moments, the least square method and the maximum likelihood method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Moments\n",
    "\n",
    "Method of moments are optimal for AR models, because it is only a linear system and we will show that it is equivalent to a least-square model. But for general ARMA models, the method of moments is sub-optimal.\n",
    "\n",
    "### AR Models\n",
    "\n",
    "For causal AR models $x_t = \\phi_1 x_{t-1} + \\dotsc +\\phi_p x_{t-p}+w_t$, we have the following equations on covariance \n",
    "called the Yule Walker equations,\n",
    "$$\\begin{aligned}\\gamma(h) &= \\phi_1\\gamma(h-1) + \\dotsc + \\phi_p \\gamma(h-p)\\ (i=1,\\dotsc,p)\\\\ \n",
    "\\sigma_w^2 &= \\gamma(0) -\\phi_1\\gamma(1) - \\dotsc - \\phi_p \\gamma(p).\\end{aligned}$$\n",
    "It has matrix form \n",
    "$$\\Gamma_p \\phi = \\gamma_p\\quad\\quad \\sigma_w^2 = \\gamma(0) - \\phi^T \\gamma_p.$$\n",
    "For estimation, we have \n",
    "$$ \\hat \\phi = \\hat \\Gamma_p^{-1}\\hat \\gamma_p\\quad\\quad \\hat \\sigma_w^2 =\\hat \\gamma(0) -\\hat \\phi^T\\hat \\gamma_p.$$\n",
    "The variance estimator $\\hat \\sigma_w^2$ is consistent and it has asympototical normality that \n",
    "$$\\sqrt n(\\hat \\phi - \\phi)\\stackrel{d}{\\rightarrow}N(0,\\sigma_w^2\\Gamma_p^{-1})$$\n",
    "\n",
    "Recall that it has the same equation form with one-step forecasting, one can solve the linear system with Durbin-Levinson algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconditional Estimator\n",
    "\n",
    "### AR(1)\n",
    "For biased causal ${\\rm AR}(1)$ models $x_t =\\mu+\\phi(x_{t-1} - \\mu) + w_t$, we begin with the likelihood function\n",
    "$$L(\\mu,\\phi,\\sigma_W^2;X_n)  =f(X_n|X_{n-1})f(X_{n-1}|X_{n-2})\\dotsm f(X_1).$$\n",
    "Note that $X_t\\sim N(\\mu+\\phi(x_{t-1} - \\mu),\\sigma_w^2)$ and by causality the last term can be written in \n",
    "$$X_1=\\mu + \\sum_{j=0}^\\infty \\phi^j w_{1-j}\\sim N(\\mu, \\frac{1}{1 - \\phi^{2}}\\sigma_w^2),$$ \n",
    "we learn \n",
    "$$L(\\mu,\\phi,\\sigma_W^2;X_n)  =\\prod_{t=2}^{n}\\frac{1}{\\sqrt{2\\pi \\sigma_w^2}}e^{-\\frac{(X_t-\\mu-\\phi(x_{t-1} - \\mu))^2}{2\\sigma_w^2}}\\cdot \\frac{\\sqrt{1-\\phi^2}}{\\sqrt{2\\pi \\sigma_w^2}}e^{-\\frac{(1-\\phi^2)(X_1-\\mu)^2}{2\\sigma_w^2}}.$$\n",
    "And \n",
    "$$\\log L = -\\frac n2\\log (2\\pi \\sigma_w^2)+\\frac12\\log(1-\\phi^2)-\\frac{1}{2\\sigma_w^2}\\left(-(1-\\phi^2)(X_1-\\mu)^2+\n",
    "\\sum_{t=2}^n(X_t-\\mu-\\phi(x_{t-1} - \\mu))^2\\right).$$\n",
    "\n",
    "To maximize the log-likelihood, we shall first compute the optimal estimator for variance $\\sigma_w^2$ via derivative. Simple calculation leads to $\\hat\\sigma_w^2 = S(\\hat \\mu,\\hat \\theta)/n$ where $$S(\\mu,\\phi) = -(1-\\phi^2)(X_1-\\mu)^2+\n",
    "\\sum_{t=2}^n(X_t-\\mu-\\phi(x_{t-1} - \\mu))^2.$$\n",
    "\n",
    "Plug back the $\\hat\\sigma_w^2$ and we desire to maximize\n",
    "$$\\ell(\\mu,\\theta) = \\log\\left(\\frac 1nS(\\mu,\\theta)\\right) - \\frac1n\\log(1-\\phi^2).$$\n",
    "\n",
    "A suggested numerical algorithm is by iteratively fixing $\\phi$ to modify \n",
    "$\\mu$ and fixing $\\mu$ to modify $\\phi$ until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Estimator\n",
    "\n",
    "### AR(1)\n",
    "\n",
    "Rather than minimizing $\\ell(\\mu,\\theta) = \\log\\left(\\frac 1nS(\\mu,\\theta)\\right) - \\frac1n\\log(1-\\phi^2)$ defined as above, one can alternatively simply minimize the $S (\\mu,\\phi)$ part, of which the closed-form solution is given by \n",
    "$$\\hat \\phi= \\frac{\\sum_{t=2}^n (x_t - \\bar x_{(2)})(x_{t-1} - \\bar x_{(1)})}{\\sum_{t=2}^n (x_{t-1} - \\bar x_{(1)})^2}\n",
    "\\quad\\quad \\hat \\mu = \\frac{\\bar x_{(2)} - \\hat \\phi \\bar x_{(1)}}{1 - \\hat \\phi}$$\n",
    "where\n",
    "$$\\bar x_{(1)} =  \\frac{1}{n-1}\\sum_{t=1}^{n-1}x_t\\quad\\quad \\bar x_{(2)} =\\frac{1}{n-1}\\sum_{t=2}^{n}x_t.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares for ARMA\n",
    "\n",
    "If we view each $w_t$ as a prediction error in an ${\\rm ARMA}(p,q)$, we can write \n",
    "$$w_t(\\beta) = x_t - \\sum_{j=1}^p \\phi_j x_{t-j} - \\sum_{k=1}^q\\theta_q w_{t-k}(\\beta)$$\n",
    "where $\\beta = [\\phi_1,\\dotsc,\\phi_p,\\theta_1,\\dotsc,\\theta_q]^T$ is our model and $w_{t-k}$ are errors in the past. \n",
    "\n",
    "Given observations $x_1,\\dotsc,x_n$, we construct the loss function, the conditional least squares, by \n",
    "$$L(\\beta) = \\sum_{t=p+1}^n w_t(\\beta)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA\n",
    "\n",
    "ARIMA is short for autoregressive integrated moving average.\n",
    "\n",
    "**Definition** A process $x_t$ is ${\\rm ARIMA}(p,d,q)$ if $\\nabla^d x_t$ is ${\\rm ARMA}(p,q)$.\n",
    "\n",
    " It is just a matter of differencing. For example, random walk with shift can be converted to a stationary ARMA after detrending by differencing. For prediction, we can first work on ARMA models and then integrate back to ARIMA.\n",
    "\n",
    "\n",
    "### IMA(1,1)\n",
    "\n",
    "${\\rm IMA}(1,1)$ (or ${\\rm ARIMA}(0,1,1)$) is a special case of ARIMA, and is also known as exponentially weighted moving averages (EWMA) in the field of finance. It has the form \n",
    "$$x_t - x_{t-1} = w_t - \\lambda w_{t-1}\\quad \\quad |\\lambda|<1.$$\n",
    "\n",
    "Take the difference $y_t = x_t - x_{t-1}$ and inverse the MA to AR, we have \n",
    "$$w_t = y_t  + \\sum_{j=1}^{\\infty}\\lambda^jy_{t-j} = (x_t - x_{t-1})+\\sum_{j=1}^{\\infty}\\lambda^j (x_{t-j}-x_{t-j-1})\n",
    "=x_t - \\sum_{j=1}^{\\infty}(\\lambda^{j-1} - \\lambda^j)x_{t-j}.$$\n",
    "Replace $t$ with $t+1$ and we obtain \n",
    "$$x_{t+1} = w_{t+1} + \\sum_{j=1}^{\\infty}(\\lambda^{j-1} - \\lambda^j)x_{t+1-j}.$$\n",
    "When $x_{t+1}$ is unknown while $x_t,x_{t-1},\\dotsc$ are already observed, we would predict $x_{t+1}$ by infinite past, where we ignore the random $w_{t+1}$ and preserve the knowns that $\\tilde x_{t+1} = \\sum_{j=1}^{\\infty}(\\lambda^{j-1} - \\lambda^j)x_{t+1-j} = (1 - \\lambda)x_t+ \\sum_{j=2}^{\\infty}(\\lambda^{j-1} - \\lambda^j)x_{t+1-j}$. Now the trick lies in the observation that $\\tilde x_t = \\sum_{j=2}^{\\infty}(\\lambda^{j-2} - \\lambda^{j-1})x_{t+1-j}$, and we yield \n",
    "$$\\tilde x_{t+1} = (1 - \\lambda)x_t +\\lambda \\tilde x_t.$$\n",
    "The prediction for the future is a linear combination of the current and the prediction for the current (previous forecast)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
