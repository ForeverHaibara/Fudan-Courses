{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Bootstrap and Jackknife\n",
    "\n",
    "For an estimator, we can estimate its standard error (which is useful in confidence level) by Bootstrap or Jackknife.\n",
    "\n",
    "## Bootstrap\n",
    "\n",
    "Given a sample $x_1, x_2, \\cdots, x_n$, we can generate a new sample $x_1^*, x_2^*, \\cdots, x_n^*$ by sampling **with replacement** from the original sample. Then we can calculate the estimator $\\hat{\\theta}^*$ from the new sample. Repeat this process $B$ times, we can get $B$ estimators $\\hat{\\theta}_1^*, \\hat{\\theta}_2^*, \\cdots, \\hat{\\theta}_B^*$. The standard error of $\\hat{\\theta}$ is estimated by the standard deviation of $\\hat{\\theta}_1^*, \\hat{\\theta}_2^*, \\cdots, \\hat{\\theta}_B^*$.\n",
    "\n",
    "$$\\widehat{\\text{SE}}_B(\\hat\\theta) = \\sqrt{\\frac{1}{B-1}\\sum_{i=1}^B(\\hat\\theta_i^*-\\bar{\\hat\\theta}^*)^2}$$\n",
    "\n",
    "Oftentimes $B$ does not to be very large, for example $B = 50$ might be enough.\n",
    "\n",
    "<!-- <br>\n",
    "\n",
    "For instance, with sample $(X_1,Y_1)$, $(X_2,Y_2)$, $\\cdots$, $(X_n,Y_n)$, we can estimate the standard error of the correlation coefficient $\\rho$ by Bootstrap. If we assume $Y = kX+b+\\epsilon=W+\\epsilon$, where $\\epsilon$ is the noise, the estimator is given by\n",
    "\n",
    "$$\\rho = \\frac{\\frac 1n \\sum X_i\\cdot (W_i+\\epsilon_i) - \\frac {1}{n}\\bar X_i\\sum (W_i+\\epsilon_i)}{\\sqrt{\\frac 1n \\sum (X_i - \\bar X_i)^2}\\sqrt{\\frac 1n \\sum (W_i - \\bar W_i+\\epsilon_i)^2}}.$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated SE = 0.12520332405358106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lsat = np.array([576,635,558,578,666,580,555,661,651,605,653,575,545,572,594])\n",
    "gpa = np.array([3.39,3.30,2.81,3.03,3.44,3.07,3.00,3.43,3.36,3.13,3.12,2.74,2.76,2.88,2.96])\n",
    "\n",
    "# estimator for the correlation coefficient\n",
    "def corr(x,y):\n",
    "    x, y = x - x.mean(), y - y.mean()\n",
    "    return (x*y).mean() / np.sqrt(x.var() * y.var())\n",
    "\n",
    "# bootstrap for standard error\n",
    "def bootstrap_se(data, statistic, B = 200):\n",
    "    n = data.shape[0]\n",
    "    idx = np.random.randint(0, n, (B, n))\n",
    "    stat = np.zeros(B)\n",
    "    for i in range(B):\n",
    "        stat[i] = statistic(data[idx[i]])\n",
    "    return np.var(stat, ddof = 1) ** .5\n",
    "\n",
    "data = np.stack((lsat, gpa), axis = -1)\n",
    "statistic = lambda x: corr(x[...,0], x[...,1])\n",
    "\n",
    "np.random.seed(1)\n",
    "print('Estimated SE =', bootstrap_se(data, statistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Correction\n",
    "\n",
    "Bootstrap can also help estiamte the bias of an estimator $\\hat\\theta$. The bias of an estimator is defined by $\\text{bias}(\\hat\\theta) = \\mathbb E\\hat\\theta - \\theta$. However, the real $\\theta$ is often unknown.\n",
    "\n",
    "To use bootstrap to estimate the bias, we first compute $\\hat\\theta$ to estimate $\\theta$. And then we use bootstrap to estimate $\\mathbb E\\hat\\theta\\approx \\frac 1B\\sum_{i=1}^B  \\hat{\\theta}_i^*$. Then we have bias estimation $\\widehat{\\text{bias}}(\\hat\\theta) = \\frac 1B\\sum_{i=1}^B  \\hat{\\theta}_i^* - \\hat\\theta$.\n",
    "\n",
    "The bias-corrected estimator is given by $\\tilde\\theta = \\hat\\theta - \\widehat{\\text{bias}}(\\hat\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated bias = -0.20687985833105405\n",
      "True      bias = -0.225\n",
      "Estimated mean = 2.338105501250709\n",
      "Corrected mean = 2.544985359581763\n",
      "True      mean = 2.25\n"
     ]
    }
   ],
   "source": [
    "# estimate bias of moment estimator\n",
    "import numpy as np\n",
    "mu, sigma, n = 2, 1.5, 10\n",
    "\n",
    "def bootstrap(data, statistic, B = 200):\n",
    "    n = data.shape[0]\n",
    "    idx = np.random.randint(0, n, (B, n))\n",
    "    stat = 0\n",
    "    for i in range(B):\n",
    "        stat += statistic(data[idx[i]])\n",
    "    return stat / B - statistic(data)\n",
    "\n",
    "statistic = lambda x: ((x - x.mean())**2).mean()\n",
    "statistic = lambda x: x.var(ddof = 1)\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(n) * sigma + mu\n",
    "bias_ = bootstrap(x, statistic)\n",
    "print('Estimated bias =', bias_)\n",
    "print('True      bias =', -sigma**2/n)\n",
    "print('Estimated mean =', statistic(x))\n",
    "print('Corrected mean =', statistic(x) - bias_)\n",
    "print('True      mean =', sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval\n",
    "\n",
    "Apart from $\\hat\\theta\\pm z_{\\alpha/2} \\widehat{\\text{SE}}_B(\\hat\\theta)$, we can also use other confidence intervals by Bootstrap. Assume we have obtained $\\hat\\theta_1^*, \\hat\\theta_2^*, \\cdots, \\hat\\theta_B^*$ by Bootstrap already. We sort them as\n",
    "\n",
    "$$\\hat\\theta_{(1)}^* \\leqslant \\hat\\theta_{(2)}^* \\leqslant \\cdots \\leqslant \\hat\\theta_{(B)}^*.$$\n",
    "\n",
    "Then we can use the following confidence intervals.\n",
    "\n",
    "#### Percentile Bootstrap Confidence Interval\n",
    "\n",
    "Use the percentile $[\\hat\\theta_{((B+1)\\alpha/2)}^*, \\hat\\theta_{((B+1)(1-\\alpha/2))}^*]$. It covers $(1 - \\alpha)$-percentage of $\\hat\\theta^*$.\n",
    "\n",
    "A bias-corrected and accelerated version known as BCa is given by [1, pp. 184-186] $(\\hat\\theta_{(f(\\alpha/2))}, \\hat\\theta_{(f(1-\\alpha_2))})$ where\n",
    "\n",
    "$$\\left\\{\\begin{aligned}\n",
    "\\hat a &= \\frac{\\sum_{i=1}^B (\\hat \\theta_i^* - \\bar{\\hat\\theta^*})^3}{6\\left(\\sum_{i=1}^B (\\hat \\theta_i^* - \\bar{\\hat\\theta^*})^2\\right)^{3/2}}\\\\\n",
    "\\hat z &= \\Phi^{-1}\\left(\\frac 1B  \\sum_{i=1}^B\\mathbb I_{\\hat\\theta_i^* <\\hat\\theta} \\right)\\\\ \n",
    "f(w) &= \\Phi\\left(\\hat z + \\frac{\\hat z + \\Phi^{-1}(w)}{1 -\\hat a(\\hat z + \\Phi^{-1}(w))}\\right)\n",
    "\\end{aligned}\\right.$$\n",
    "\n",
    "with $\\Phi$ being the CDF of standard normal distribution.\n",
    "\n",
    "`scipy.stats.bootstrap((x,), lambda x, axis: np.var(x, ddof=1, axis=axis), method = 'BCa')`\n",
    "\n",
    "#### Basic Bootstrap Confidence Interval\n",
    "\n",
    "It is a combination of bias correction and percentile trick: $[2\\hat\\theta - \\hat\\theta_{((B+1)(1-\\alpha/2))}^*, 2\\hat\\theta - \\hat\\theta_{((B+1)\\alpha/2)}^*]$. It is equivalent to\n",
    "\n",
    "$$\\mathbb P( \\hat\\theta_{((B+1)\\alpha/2)}^* -\\hat\\theta\\leqslant \\hat\\theta -\\theta \\leqslant \\hat\\theta_{((B+1)(1-\\alpha/2))}^*-\\hat\\theta) \\approx 1 - \\alpha.$$\n",
    "\n",
    "#### Bootstrap t Confidence Interval \n",
    "\n",
    "Bootstrap t is much more complicated. Recall we have $x_1^*, \\dotsc, x_n^*$ (with replacement) in each of the $B$ bootstrap samples. We can compute the t-statistics using nested Bootstrap over the sample:\n",
    "\n",
    "$$t^{(b)} = (\\hat\\theta^{(b)} - \\hat\\theta) / \\widehat{\\text{SE}}(\\hat\\theta^{(b)}).$$\n",
    "\n",
    "Finally, we extract the percentile $t_{\\alpha/2}^*$ and $t_{1-\\alpha/2}^*$ and use the following confidence interval:\n",
    "\n",
    "$$\\hat\\theta \\pm \\widehat{\\text{SE}}(\\hat\\theta) \\cdot t_{(\\cdot)}^*.$$\n",
    "\n",
    "The time complexity is $O(B^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def bootstrap_se(data, statistic, B = 200):\n",
    "    n = data.shape[0]\n",
    "    idx = np.random.randint(0, n, (B, n))\n",
    "    stat = np.zeros(B)\n",
    "    for i in range(B):\n",
    "        stat[i] = statistic(data[idx[i]])\n",
    "    return np.var(stat, ddof = 1) ** .5\n",
    "\n",
    "def bootstrap_ci(data, statistic, alpha = 0.05, B = 200, method = 'normal'):\n",
    "    \"\"\"\n",
    "    Compute estimated CI for a statistic using bootstrap with various methods.\n",
    "    Accepted methods are 'normal', 'percentile', 'BCa', 'basic', 't'.\n",
    "    \"\"\"\n",
    "    theta = statistic(data)\n",
    "    n = data.shape[0]\n",
    "    idx = np.random.randint(0, n, (B, n))\n",
    "    stat = np.zeros(B)\n",
    "    for i in range(B):\n",
    "        stat[i] = statistic(data[idx[i]])\n",
    "    if method == 'normal':\n",
    "        z = norm.ppf(1-alpha/2)\n",
    "        se = np.std(stat, ddof = 1)\n",
    "        ci = theta + z * np.array([-se, se])\n",
    "    elif method == 'percentile':\n",
    "        ci = np.quantile(stat, [alpha/2, 1-alpha/2])\n",
    "    elif method == 'basic':\n",
    "        ci = 2 * theta - np.quantile(stat, [1-alpha/2, alpha/2])\n",
    "    elif method == 'BCa':\n",
    "        stat2 = stat - stat.mean()\n",
    "        a = (stat2 ** 3).mean() / (6 * ((stat2 ** 2).mean()) ** 1.5)\n",
    "        z = norm.ppf((stat < theta).mean())\n",
    "        f = lambda w: norm.cdf(z + (z + norm.ppf(w)) / (1 - a * (z + norm.ppf(w))))\n",
    "        ci = np.quantile(stat, [f(alpha/2), f(1-alpha/2)])\n",
    "    elif method == 't':\n",
    "        se = [bootstrap_se(data[idx[i]], statistic, B = B) for i in range(B)]\n",
    "        t = (stat - theta) / np.array(se)\n",
    "        se0 = np.var(stat, ddof = 1) ** .5\n",
    "        ci = theta + np.quantile(t, [alpha/2, 1-alpha/2]) * se0\n",
    "    return ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    normal (1.3930, 3.1028) Length = 1.7098\n",
      "percentile (1.3291, 2.8736) Length = 1.5445\n",
      "       BCa (1.5176, 2.9209) Length = 1.4033\n",
      "     basic (1.6223, 3.1668) Length = 1.5445\n",
      "         t (0.6318, 2.7977) Length = 2.1660\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "x = np.random.randn(100)\n",
    "x = x**2 # chi-squared distribution\n",
    "\n",
    "def statistic(x, axis = None):\n",
    "    # skewness\n",
    "    x = x - x.mean(axis=axis, keepdims=True)\n",
    "    return (x**3).mean(axis=axis) / (x**2).mean(axis=axis)**1.5\n",
    "\n",
    "# skewness of chi-squared distribution is sqrt(8/d) where d is degree of freedom\n",
    "for method in ['normal', 'percentile', 'BCa', 'basic', 't']:\n",
    "    np.random.seed(2)\n",
    "    ci = bootstrap_ci(x, statistic, method = method, alpha = .05, B = 50)\n",
    "    print(method.rjust(10, ' '), '(%.4f, %.4f) Length = %.4f'%(ci[0], ci[1], ci[1] - ci[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackknife\n",
    "\n",
    "Suppose we have observed sample $x_1,\\dotsc,x_n$. We can discard one of them to obtain $(x_1,\\dotsc, x_{i-1}, x_{i+1},\\dotsc,x_n)$. Let $i$ run through $1,\\dotsc,n$ and we can get $n$ different estimators $\\hat\\theta_1,\\dotsc,\\hat\\theta_n$.\n",
    "\n",
    "The bias of the estimator is given by\n",
    "\n",
    "$$\\widehat{\\text{bias}}_{\\text{jack}}(\\hat\\theta) = (n-1)\\sum_{i=1}^n(\\hat\\theta_i - \\hat\\theta).$$\n",
    "\n",
    "While the standard error is given by\n",
    "\n",
    "$$\\widehat{\\text{SE}}_{\\text{jack}}(\\hat\\theta) = \\sqrt{\\frac{n-1}{n}\\sum_{i=1}^n(\\hat\\theta_i - \\bar{\\hat\\theta})^2}.$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Jackknife might fail when the statistic is not smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated (bias, SE) = (0.06401990686485959, 0.1055711231014818)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def jackknife(data, statistic):\n",
    "    n = data.shape[0]\n",
    "    theta = statistic(data)\n",
    "    data = np.concatenate([data, data]) # cyclize\n",
    "    stat = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        stat[i] = statistic(data[i+1:n+i])\n",
    "    bias = (n-1) * (stat.sum() - n*theta)\n",
    "    se = ((n-1) * ((stat - theta)**2).mean()) ** .5\n",
    "    return bias, se\n",
    "\n",
    "# Full data available at https://rdrr.io/cran/bootstrap/man/patch.html\n",
    "patch_data = np.array([\n",
    "    [    1,  9243, 17649, 16449,  8406, -1200],\n",
    "    [    2,  9671, 12013, 14614,  2342,  2601],\n",
    "    [    3, 11792, 19979, 17274,  8187, -2705],\n",
    "    [    4, 13357, 21816, 23798,  8459,  1982],\n",
    "    [    5,  9055, 13850, 12560,  4795, -1290],\n",
    "    [    6,  6290,  9806, 10157,  3516,   351],\n",
    "    [    7, 12412, 17208, 16570,  4796,  -638],\n",
    "    [    8, 18806, 29044, 26325, 10238, -2719]\n",
    "])\n",
    "\n",
    "statistic = lambda x: x[:,-1].mean() / x[:,-2].mean()\n",
    "\n",
    "print('Estimated (bias, SE) =', jackknife(patch_data, statistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Median SE = 11.217010504690238\n",
      "Jackknife Median SE = 25.5\n"
     ]
    }
   ],
   "source": [
    "# Unsmooth statistic might fail Jackknife. E.g. median\n",
    "np.random.seed(0)\n",
    "x = np.random.randint(1, 101, 10)\n",
    "\n",
    "print('Bootstrap Median SE =', bootstrap_se(x, np.median, B = 2000))\n",
    "print('Jackknife Median SE =', jackknife(x, np.median)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] B. Efron, R. Tibshirani, [An Introduction to the Bootstrap](https://www.semanticscholar.org/paper/An-Introduction-to-the-Bootstrap-Efron-Tibshirani/85a8a97f614b2b6823e035bcc9abcb0f3d27be4d), 1994"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
