{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honor 4 High Dimensional Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "\n",
    "### Rise of Dimensionality\n",
    "\n",
    "High dimensional data have a wide range of application, for example, microarray or proteomics data in disease classification, spatial-temporal data and high-resolution images...\n",
    "\n",
    "Sometimes we want to construct a method that can effectively predict the future observations, then a black-box model like neural networks works well. However, if we want to gain insight into the underlying relation of features like gene relations, it would be quite challenging.\n",
    "\n",
    "### Impact of Dimensionality\n",
    "\n",
    "High dimensional data are very challenging.\n",
    "\n",
    "* Expensive Computation (large time complexity, slow convergence)\n",
    "* Expensive Storage\n",
    "* Hard Optimiztation (trapped in local minima, overfitting)\n",
    "* Numerical Instability (accumulated noises, ...)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spurious Correlation\n",
    "\n",
    "Spurious correlation refers to the \"wrong\" relation due to overfitting in high dimensional data.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Consider $n$ samples $(x,y)$ where $x\\in\\mathbb R^p$ and $y\\in\\mathbb R$. Here each entry of $x$ is sampled from independent Gaussian distribution $N(0,1)$ and also $y$ is independently sampled from $N(0,1)$.\n",
    "\n",
    "Theoretically, $y$ is independent of $x$. However, when we face undersampling in high dimension: $n\\ll p$, even a linear regression will fit well and report strong correlation. This is called the spurious correlation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Now we review the concept of multiple linear regression. \n",
    "\n",
    "\n",
    "\n",
    "Let $X\\in\\mathbb R^{n\\times p}$ be the covariate matrix and $y\\in\\mathbb R^n$ be the responses. The true model is $y = X\\beta + \\epsilon$ while $\\epsilon$ is the noise and $\\beta$ is unknown coefficients. We want to estimate\n",
    "$$\\hat\\beta = {\\rm argmin}_{\\hat\\beta} \\mathbb E\\Vert y - X\\beta\\Vert^2$$\n",
    "\n",
    "### Heteroskedasticity\n",
    "\n",
    "When $\\mathbb E(\\epsilon_i) = 0$ but ${\\rm Var}(\\epsilon_i) = \\sigma^2w_i$ where $w_i$ is a known positive constant and $\\sigma^2$ is unknown (i.e. the scale of noises are different), the problem is said to be heteroskedastic.\n",
    "\n",
    "We can reweight the problem, $\\tilde y_i = w_i^{-\\frac 12}y_i$ and $\\tilde x_i = w_i^{-\\frac 12}x_i$ and $\\tilde \\epsilon_i = w_i^{-\\frac 12}\\epsilon_i\\sim (0,\\sigma^2)$. Now the noise $\\tilde\\epsilon$ is homoskedastic and we can use OLS (ordinary least squares):\n",
    "\n",
    "$$\\hat \\beta = (\\tilde X^T\\tilde X)^{-1}\\tilde X^T\\tilde y=(X^TW_0^{-1}X)^{-1}X^TW_0^{-1}y.$$\n",
    "\n",
    "Here $W_0 = {\\rm diag}[w_1,\\dotsc,w_n]={\\rm Cov}(\\epsilon)\\in\\mathbb R^{n\\times n}$. This is called the weighted least squares.\n",
    "\n",
    "#### Generalized Least Squares\n",
    "\n",
    "In the heteroskedastic problem, we note that the scaling of the variance, the true $W_0$ is not known in advance. However, whatever **wrong** $W$ we choose, using $\\hat\\beta = (X^TW^{-1}X)^{-1}X^TW^{-1}y$ is unbiased, since\n",
    "$$\\mathbb E\\hat\\beta = \\mathbb E\\left\\{(X^TW^{-1}X)^{-1}X^TW^{-1}y\\right\\} = \\mathbb E\\left\\{(X^TW^{-1}X)^{-1}X^TW^{-1}X\\beta\\right\\}=\\beta.$$\n",
    "\n",
    "\n",
    "So it suffices to look into the variance of $\\hat\\beta$. It is not surprising that it converges as $n\\rightarrow \\infty$ if assuming some mild conditions.\n",
    "$${\\rm Var}(\\hat\\beta) =(X^TW^{-1}X)^{-1}(X^TW^{-1}W_0W^{-1}X)(X^TW^{-1}X)^{-1}\\approx O(n^{-1}) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge regression can handle the case when $X$ is singular or has small singular values by $\\hat\\beta = (X^TX+\\lambda I)^{-1}X^Ty$. Also, it trades some unbiasedness for smaller MSE than OLS.\n",
    "\n",
    "#### Brige Regression\n",
    "\n",
    "A generalization of ridge regression is the bridge regression:\n",
    "$$\\min_\\beta \\left\\{\\Vert y -X\\beta\\Vert^2 + \\lambda \\sum_j |\\beta_j|^q\\right\\}.$$\n",
    "\n",
    "Here $q>1$ guarantees the convexity. When $q =2$ it is the ridge regression. When $q = 1$ it degenerates to LASSO."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Selection\n",
    "\n",
    "Subset selection might outperform ridge selection in sparse models.\n",
    "\n",
    "\n",
    "Assume in true model $y = f(x) +\\epsilon$ where $\\epsilon \\sim (0,\\sigma^2)$ are uncorrelated and homoskedastic. Let $p$ be the dimension of fitted model $\\hat f$. And we have $n$ samples in total.\n",
    "\n",
    "\n",
    "Common criterion for subset selections include $C_p$, AIC, BIC...\n",
    "\n",
    "\n",
    "### $C_p$\n",
    "\n",
    "Criterion $C_p$ is given by $C_p = \\frac{\\rm SSE_P}{\\sigma^2}- (n - 2p)$. \n",
    "\n",
    "In multiple linear regression $f(x) = x^T\\beta$,  let $P$ be the chosen subset of features and $X_P$ be the chosen covariates matrix, then we have the following theorem:\n",
    "\n",
    "**Theorem** Let $\\hat y_P = (X_P^TX_P)^{-1}X_P^Ty$ be the least squares estimator with the feature subset $P$. But in actual $y = X\\beta +\\epsilon$ and $\\epsilon\\sim (0,\\sigma^2)$, then\n",
    "$$\\mathbb E C_p =\\mathbb E \\left\\{ \\frac{1}{\\sigma^2} \\Vert \\hat y_P - \\mathbb E(y)\\Vert^2\\right\\}$$\n",
    "\n",
    "**Proof** On the one hand, let $H_P = X_P(X_P^TX_P)^{-1}X_P^T$ be the hat matrix, then $H_P$ is idempotent and $\\hat y = H_P y$,\n",
    "$$\\begin{aligned}\\mathbb E {\\rm SSE}_P &=\\mathbb E\\Vert y - H_Py\\Vert^2 =\n",
    "\\mathbb E\\Vert (I - H_P)(X\\beta + \\epsilon)\\Vert^2\n",
    "\\\\ & = \\beta^T X^T(I - H_P)^2X\\beta +\\sigma^2 {\\rm tr}\\left\\{(I - H_P)^2\\right\\}\n",
    "\\\\ & = \\beta^T X^T(I - H_P)X\\beta +\\sigma^2 (n-p).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This implies $\\mathbb E C_p = \\sigma^{-2}  \\beta^T X^T(I - H_P)X\\beta+p$.\n",
    "\n",
    "On the other hand,\n",
    "$$\\begin{aligned}\\mathbb E \\left\\{ \\frac{1}{\\sigma^2} \\Vert \\hat y_P - \\mathbb E(y)\\Vert^2\\right\\}\n",
    "&=\\frac{1}{\\sigma^2}\\mathbb E\\Vert H_P (X\\beta  + \\epsilon) - X\\beta\\Vert^2\n",
    "\\\\ &= \\frac{1}{\\sigma^2}\\left(\\beta^TX^T(I - H_P)^2X\\beta + \\sigma^2{\\rm tr}(H_P^2)\\right)\\\\ &= \\sigma^{-2}  \\beta^T X^T(I - H_P)X\\beta+p.\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### AIC\n",
    "\n",
    "Here the \"IC\" in \"AIC\" is abbreviation for \"information criterion\". Let $\\mathcal L$ be the likelihood of the observations evaluated on the fitted model, then\n",
    "\n",
    "$${\\rm AIC} = -2\\log \\mathcal L + 2p$$\n",
    "\n",
    "In multiple linear regression, when using the same $\\sigma^2$ and assuming the noise has independent Gaussian distribution $N(0,\\sigma^2)$, then minimizing AIC and minimizing $C_p$ criterion are equivalent.\n",
    "\n",
    "### BIC\n",
    "\n",
    "BIC refers to Bayesian information criterion.\n",
    "\n",
    "$${\\rm AIC} = -2\\log \\mathcal L + p\\log n$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_0$ Penalized Least Squares\n",
    "\n",
    "In multiple linear regression, consider the general form of $L_0$ penalty,\n",
    "$$\\min \\left\\{ \\Vert y - X\\beta\\Vert^2 + \\lambda \\Vert \\beta\\Vert_0\\right\\}.$$\n",
    "\n",
    "Here the $0$-norm $\\Vert \\beta\\Vert_0$ is the count of nonzero entries of $\\beta$. Using the notations above, $\\Vert \\beta\\Vert_0 = p$.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can easily see that $C_p$ criterion is equivalent to using $\\lambda = 2\\sigma^2$. As mentioned earlier, AIC and $C_p$ are equivalent if the noise are Gaussian and using the same $\\sigma^2$, so AIC is also equivalent to $\\lambda = 2\\sigma^2$.\n",
    "\n",
    "Anologously, BIC is equivalent to $\\lambda = \\sigma^2\\log n$ in the penalty when assuming the same condition.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4 (tags/v3.8.4:dfa645a, Jul 13 2020, 16:46:45) [MSC v.1924 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
