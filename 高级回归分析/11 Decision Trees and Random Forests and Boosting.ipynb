{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Decision Trees, Random Forests and Boosting\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "Partition the data by a tree of if-else clauses into regions $R_1,R_2,\\dotsc,R_T$. Predict a new data point by the average of training data (regression) / the class that contains most training data (classification) in the corresponding region. This is the decision tree (DT).\n",
    "\n",
    "\n",
    "\n",
    "### Complexity Pruning\n",
    "\n",
    "Let $R_1,\\dotsc,R_T$ be the terminal partition regions. Define the loss by \n",
    "$$\\mathcal L = \\sum_{m=1}^{|T|} \\sum_{(x_i,y_i)\\in R_m}(y_i - \\bar y_i)^2 \\quad +\\quad |T|.$$\n",
    "\n",
    "Here is an extra penalty term $|T|$, on the number of regions.\n",
    "\n",
    "\n",
    "### Purity Measure\n",
    "\n",
    "If in region $R$ there are $K$ classes. And each of them has proportion $p_1,\\dotsc,p_K$. Then we can define following measure of purity. \n",
    "\n",
    "* Error Rate: $E = 1 - \\max_{1\\leqslant k\\leqslant K}p_k$.\n",
    "* Gini Index: $E = \\sum_{k = 1}^K p_k (1 - p_k)$.\n",
    "* Cross Entropy: $E =-\\sum_{i=1}^K p_k \\log(1 - p_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "### Bagging\n",
    "\n",
    "Assume $(x_i,y_i)\\ (i=1,\\dotsc,n)$ are the data. We can fit many models and take the average: by bootstrap-sampling $n$ a new dataset with replacement of data $(x_i',y_i') (i=1,\\dotsc,n)$ and fit a model $f_1$. Repeat this many times to obtain models $f_1,\\dotsc,f_B$. Then we use \n",
    "$$f = \\frac 1n \\sum_{j=1}^B f_J.$$\n",
    "\n",
    "This method is called bagging. It is often applied on models with low bias but high variance (e.g. trees).\n",
    "\n",
    "\n",
    "### Variable Importance\n",
    "\n",
    "When perform bagging on decision trees. We can count the number of split of each variable in total. The total counts show the importance of the variable.\n",
    "\n",
    "\n",
    "### Random Forest \n",
    "\n",
    "In each run of bagging, when there are $p$ variables to split from, we randomly select a subset of size $p'$ (e.g. $p'\\approx \\sqrt p$) to choose from. This reduces the correlation between trees and boost the diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Fit multple models and get the weighted average as the final model.\n",
    "\n",
    "\n",
    "\n",
    "### Friedman's Gradient Boosting Algorithm\n",
    "\n",
    "\n",
    "\n",
    "Other boosting method: GBDT, XGBoost, LightGBM.\n",
    "\n",
    "\n",
    "### Forward Stagewise Regression\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
