{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 James-Stein Estimation and Ridge Regression\n",
    "\n",
    "James-Stein estimator sacrifices some biasedness to trade for overall performance in variance. Oftentimes it has  smaller variance than MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## James-Stein Estimation \n",
    "\n",
    "### Posterior Mean \n",
    "\n",
    "First suppose $\\mu\\sim N(M,A)$ and $(x|\\mu)\\sim N(\\mu,1)$ and $M,A,x\\in\\mathbb R$ are known and now $\\mu$ is fixed but unknown. Then we can estimate $\\mu$ by solving that \n",
    "$(\\mu|x)\\sim N(M+B(x-M), B)$ where $B = A(A+1)^{-1}<1$. We thus define the posterior mean of $\\mu$, or Bayes estimator of $\\mu$, by \n",
    "$$\\hat\\mu^{\\rm Bayes} = M+B(x-M).$$\n",
    "\n",
    "Then \n",
    "$$\\mathbb E\\left\\{(\\hat\\mu^{\\rm Bayes} - \\mu)^2\\right\\} = B.$$\n",
    "\n",
    "However, if given $x$ we use the MLE estimator to estimate $\\mu$, which is $\\hat\\mu^{\\rm MLE} = x$, then \n",
    "$$\\mathbb E\\left\\{(\\hat\\mu^{\\rm MLE})^2\\right\\} = 1>B.$$\n",
    "\n",
    "Hence here the Bayes estimator is better in variance, as we have utilized the priori information $\\mu\\sim N(M,A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown Posterior Parameters\n",
    "\n",
    "Suppose $\\mu\\sim N(M,A)$ and $(x|\\mu)\\sim N(\\mu,1)$ and only $x\\in\\mathbb R$ is known while $M,A$ are fixed but unknown. Still we have \n",
    "$(\\mu|x,M,A)\\sim N(M+B(x-M),B)$ where $B = A(A+1)^{-1}$. It suffices to estimate the parameters $M,A$.\n",
    "\n",
    "When we have $n>3$ samples $x_1,\\dotsc,x_n$, then we can use the following to provide unbiased estimators:\n",
    "$$\\hat M =\\bar x\\quad\\quad \\hat B =1 - \\frac{n-3}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar x)^2}}.$$\n",
    "\n",
    "\n",
    "**Proof** It can be shown that $x\\sim N(M,A+1)$ so $\\mathbb E(\\hat M) = \\mathbb E(\\bar x) = M$. For $\\hat B$, we note that $\\hat B =  1 - \\frac{n-3}{(A+1)\\chi_{n-1}^2}$ and\n",
    "$$\\mathbb E(\\hat B ) = 1 - \\frac{n-3}{A+1}\\int_{0}^\\infty \\frac{1}{x}\\frac{1}{2^{\\frac{n-1}{2}}\\Gamma(\\frac{n-1}{2})}x^{\\frac{n-1}{2}-1}e^{-\\frac{x}{2}}dx= 1 - \\frac{n-3}{A+1}\\frac{2^{\\frac{n-3}{2}}\\Gamma(\\frac{n-3}{2})}{2^{\\frac{n-1}{2}}\\Gamma(\\frac{n-1}{2})}=\\frac{A}{A+1}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## James-Stein Theorem\n",
    "\n",
    "Suppose $(x_i|\\mu_i)\\sim N(\\mu_i,1)$ independently for $i=1,2,\\dotsc,n$ and $n>3$. Then \n",
    "$$\\mathbb E\\left\\{\\Vert \\hat\\mu^{\\rm JS} - \\mu\\Vert^2\\right\\} < \\mathbb E\\left\\{\\Vert\\hat\\mu^{\\rm MLE} - \\mu\\Vert^2\\right\\}$$\n",
    "where $\\mu = [\\mu_1,\\dotsc,\\mu_n]^T$. Each $\\mu_i$ can be a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "In linear regression ${\\argmin}_{\\hat\\beta} \\Vert y - X\\hat\\beta\\Vert^2$, we can add a regularization term to form a new problem, called ridge regression term:\n",
    "\n",
    "$${\\argmin}_{\\hat\\beta}\\left\\{\\Vert y - X\\hat\\beta\\Vert^2 + \\lambda\\Vert\\hat\\beta\\Vert^2\\right\\}$$\n",
    "\n",
    "### Bayesian Rationale\n",
    "\n",
    "We can explain the idea of ridge regression by Bayes. Assume the true parameter follows a Gaussian prior,\n",
    "$$\\beta\\sim N (0,\\Sigma).$$\n",
    "\n",
    "Together with the assumption $\\epsilon\\sim N(0,\\sigma^2I_n)$ and $y = X\\beta+\\epsilon$ we learn that \n",
    "$$\\left[\\begin{matrix}y\\\\ \\beta\\end{matrix}\\right]\n",
    "\\sim N\\left(0,\\left[\\begin{matrix} X\\Sigma X^T+\\sigma^2I_n & X\\Sigma  \\\\ \\Sigma  X^T & \\Sigma\\end{matrix}\\right]\\right).\n",
    "$$\n",
    "\n",
    "Then the conditional Gaussian distribution is given by \n",
    "$$(\\beta|y)\\sim N\\big(\\Sigma X^T(X\\Sigma X^T+\\sigma^2I_n)^{-1}y,\\ \\dotsc\\big)$$\n",
    "\n",
    "When $\\Sigma = \\frac{2}{\\lambda}\\sigma^2I_p$, we can estimate $\\hat\\beta$ with the conditional expectance,\n",
    "$$\\hat\\beta = \\Sigma X^T(X\\Sigma X^T+\\sigma^2I_n)^{-1}y=X^T(X X^T+\\frac{\\lambda}{2}I_n)^{-1}y.$$\n",
    "\n",
    "Note that we have the equation\n",
    "$$(X^TX+\\frac{\\lambda}{2}I_p)X^T = X^T(XX^T+\\frac{\\lambda}{2} I_n)\\quad\\Rightarrow\\quad  X^T(XX^T+\\frac{\\lambda}{2} I_n)^{-1} = (X^TX+\\frac{\\lambda}{2}I_p)^{-1}X^T.$$\n",
    "\n",
    "Thus, $\\hat\\beta = (X^TX+\\frac{\\lambda}{2}I_p)^{-1}X^Ty$ is exactly the minimizer of the ridge regression ${\\rm argmin}\\Vert y - X\\beta\\Vert^2+\\frac \\lambda 2\\Vert \\beta\\Vert^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.32667268e-17, -1.11022302e-15, -2.08166817e-16, -1.11022302e-16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(8,4)\n",
    "y = np.random.randn(X.shape[0])\n",
    "b1 = X.T @ np.linalg.inv(X @ X.T + .5 * np.diag(np.ones(X.shape[0]))) @ y\n",
    "b2 = np.linalg.inv(X.T @ X + .5 * np.diag(np.ones(X.shape[1]))) @ X.T @ y\n",
    "b1 - b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage\n",
    "\n",
    "On the other hand, when there is outlier, these estimations will badly impact the estimation of the outlier. One had better remove the outliers from the training data first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
