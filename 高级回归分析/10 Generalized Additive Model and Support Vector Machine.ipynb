{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Generalized Additive Model and Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Additive Model\n",
    "\n",
    "Denote $x = [x_1,\\dotsc,x_n]^T\\in\\mathbb R^n$ is a piece of datum and if model $f$ has following form:\n",
    "$$f(x) = f_1(x_1)+\\dotsc+f_n(x_n),$$\n",
    "i.e. sum of functions with respect to each variable, then we call $f$ is a generalized additive model (GAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "If we are performing binary classification: $(x_i,y_i)\\in\\mathbb R^p\\times \\mathbb R$, we can try finding a linear $f:\\ \\mathbb R^p\\rightarrow \\mathbb R$:\n",
    "$$f(x_i) = \\beta_0 +\\beta_1 x_{i1}+\\dotsc+\\beta_p x_{ip}=[1,x_i^T]\\beta$$\n",
    "where $x_i = [x_{i1},\\dotsc,x_{ip}]^T$.\n",
    "\n",
    "Aim: find $\\beta_0,\\dotsc,\\beta_p$ such that \n",
    "$$f(x_i) = \\left\\{\\begin{aligned}>0 &\\quad {\\rm if}\\ y_i>0\\\\<0 &\\quad {\\rm if}\\ y_i<0\\end{aligned}\\right.\\quad \\quad (i=1,2,\\dotsc,n).$$\n",
    "\n",
    "**If such coefficient** $\\beta$ **exists**, then for a new data point $x$ if we have $f(x)>0$, then we predict $\\hat y(x) >0$, otherwise $\\hat y(x)<0$.\n",
    "\n",
    "This is the support vector machine (SVM).\n",
    "\n",
    "### Hard Margin\n",
    "\n",
    "When such coefficient $\\beta$ exists, we say that the data are (linearly) separable. And there often exists infinitely many such $\\beta$. Then we must select a best one. \n",
    "\n",
    "We define the best by \"margin\" by the extremum of the following.\n",
    "\n",
    "$$\\hat\\beta ={\\rm \\argmax}_{\\beta}\\left\\{\\gamma:\\ \\Vert\\beta\\Vert^2=1,\\ \\forall i,\\ y_i[1,x_i^T]\\beta\\geqslant \\gamma\\right\\}\n",
    "={\\rm \\argmax}_{\\Vert \\beta\\Vert=1}\\left\\{\\min_{1\\leqslant i\\leqslant n}y_i[1,x_i^T]\\beta\\right\\}.$$\n",
    "\n",
    "### Equivalent Reformulation\n",
    "\n",
    "When the following has at least an extremum $\\beta$, then it is unique and $\\frac{\\beta}{\\Vert\\beta\\Vert}$ is one of the extrema of the hard margin.\n",
    "$$\\hat\\beta ={\\rm \\argmin}_{\\beta}\\left\\{\\frac 12\\Vert \\beta\\Vert^2:\\quad \\forall i, \\ y_i[1,x_i^T]\\beta\\geqslant 1\\right\\}.$$\n",
    "\n",
    "**Proof** Easy to show the uniqueness by verifying the Slater's condition of convex optimization. Assume $\\beta$ is the minimizer and we show that it is one of the extrema of the hard margin by showing contradictions otherwise. Assume $\\beta'$ $(\\Vert\\beta\\Vert=1)$ is strictly better than $\\beta$ in the sense of hard margin, i.e.\n",
    "$$m = \\min_{1\\leqslant i\\leqslant n}y_i[1,x_i]^T\\beta' >\\min_{1\\leqslant i\\leqslant n}y_i[1,x_i]^T\\frac{\\beta}{\\Vert\\beta\\Vert}\\geqslant \\frac{1}{\\Vert\\beta\\Vert}.$$\n",
    "\n",
    "Take $\\beta'' = \\frac{\\beta'}{m}$, then we have \n",
    "$$y_i[1,x_i^T]\\beta'' = y_i[1,x_i^T]\\beta'\\frac{1}{m}\\geqslant 1\n",
    "$$\n",
    "and \n",
    "$$\\Vert\\beta''\\Vert^2 = \\frac{\\Vert \\beta'\\Vert^2}{m^2}=\\frac{1}{m^2}<\\Vert\\beta\\Vert^2,$$\n",
    "contradicting the selection of $\\beta$.\n",
    "\n",
    "### Lagrangian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin\n",
    "\n",
    "But $\\beta$ for the \"equivalent reformulation\" mentioned above may not exist! In this case, we say that the data are not **separable**.\n",
    "\n",
    "(Even if it exists, it might be overfitting.) We can use the soft margin:\n",
    "\n",
    "$$\\beta  ={\\rm argmin}_\\beta \\left\\{\\frac12\\Vert \\beta\\Vert^2 +C\\sum_{i=1}^n \\epsilon_i\\quad {\\rm s.t.\\ \\ }  \\forall i, \\ y_i  [1,x_i]^T\\beta\\geqslant 1-\\epsilon_i {\\rm\\  and\\ } \\epsilon_i\\geqslant 0 \\right\\}$$\n",
    "where $C$ is a hyperparameter.\n",
    "\n",
    "It is clear that the optimal $\\epsilon_i$ should be $\\epsilon_i = \\max\\{0,\\ 1-y_i  [1,x_i]^T\\beta\\}$. Thus, we can write in the equivalent form:\n",
    "$$\\beta  ={\\rm argmin}_\\beta \\left\\{\\frac12\\Vert \\beta\\Vert^2 +C\\sum_{i=1}^n \\max\\{0,\\ 1- y_i[1,x_i]^T \\beta \\} \\right\\}$$\n",
    "From the first equation we can see that, it allows $y_i \\neq {\\rm sgn}([1,x_i]^T\\beta )$ as long as $\\epsilon_i> 1$. So $\\beta$ exists even if the data are not linearly separable.\n",
    "\n",
    "Also, it penalizes the occurence of this case by the term $C\\sum_{i=1}^n \\epsilon_i$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "#### One-versus-one \n",
    "\n",
    "One-versus-one is an approach to handle multiclass classification. Assume there are $K$ classes and we get $\\binom K2$ pairs of classes. We fit $\\binom K2$ SVMs for each pair of classes (one-versus-one). We claim the prediction to be the class that wins the most.\n",
    "\n",
    "#### One-versus-all \n",
    "\n",
    "For each class $k$, run a SVM to judge whether the data belong to class $k$ or not. Since SVM outputs a margin by computing $[1,x^T]\\beta$, we classify the data to the class with most predominant margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mercer Kernel\n",
    "\n",
    "When the data are not separable, we can use a mapping apart from the soft margin. We can map $x_i$ to $\\phi(x_i) = z_i\\in \\mathbb R^q$, or infinite Hilbert space $\\phi(x_i) = z_i\\in \\mathcal H$ so that $z_i$ is separable or has better separation result in the sense of soft margin.\n",
    "\n",
    "Define the kernel of the mapping $\\phi$ to be inner product of two mapping results:\n",
    "$$K(x,y)= \\langle \\phi(x),\\phi(y)\\rangle.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reproducing Kernel Hilbert Space \n",
    "\n",
    "#### Hilbert Space\n",
    "\n",
    "A complete (which has Cauchy's limit theorem) inner product space is a Hilbert space.\n",
    "\n",
    "#### Reproducing Kernel Hilbert Space\n",
    "\n",
    "(RKHS)\n",
    "\n",
    "#### Bounded Norm\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "**Lemma** (Riesz Representation)\n",
    "\n",
    "#### Representer Theorem \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
