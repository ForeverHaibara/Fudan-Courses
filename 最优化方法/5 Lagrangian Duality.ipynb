{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Stephen Boyd and Lieven Vandenberghe, Convex Optimization, Cambridge University Press, 2009.\n",
    "2. https://www.cse.iitk.ac.in/users/rmittal/prev_course/s14/notes/lec7.pdf\n",
    "3. A. Ben-Tai, and A. Nemirovski. Lectures on modern convex optimzation: analysis, algorithms, and engineering applications. Vol. 2. SIAM, 2001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Lagrangian Duality\n",
    "\n",
    "### Lagrangian Multiplier\n",
    "For a general optimization problem (no need to be convex) in the form of \n",
    "$$v_p^* = \\min \\{f(x):\\quad g_i(x)\\leqslant 0,\\quad h_j(x)\\leqslant 0,\\quad x\\in X\\},$$\n",
    "we write\n",
    "$$L(x,\\lambda,\\mu) = f(x) + \\lambda^T G(x)+\\mu^T H(x)$$ \n",
    "where $G(x) = [g_1(x), \\dotsc,g_m(x)]^T$ and $H(x) = [h_1(x),\\dotsc,h_p(x)]^T$ and call $\\lambda,\\mu$ the Lagrangian multipliers. It is clear that\n",
    "\n",
    "Note that \n",
    "$$\\sup_{\\lambda\\geqslant 0, \\forall \\mu}L(x,\\lambda,\\mu) = \\left\\{\\begin{array}{ll}f(x) & G(x)\\leqslant 0,\\ H(x) =0 ,\\ \\\\ \n",
    "+\\infty& {\\rm otherwise}\\end{array}\\right.$$\n",
    "characterizes the inequalities and the equalities, it indicates the primal optimization problem is equivalent to \n",
    "$$v_p^* = \\inf_{x\\in X}\\sup_{\\lambda\\geqslant 0, \\forall \\mu}L(x,\\lambda,\\mu)$$\n",
    "\n",
    "### Dual Function\n",
    "\n",
    "Define $\\theta:\\ \\mathbb R_+^m\\times \\mathbb R^p\\rightarrow \\mathbb R$ the dual function of the original optimization problem by\n",
    "$$\\theta(\\lambda,\\mu) = \\inf_{x\\in X}L(x,\\lambda,\\mu).$$\n",
    "The dual function $\\theta$ is always concave.\n",
    "\n",
    "Proof: Recall that $L(x,\\lambda,\\mu)$ is affine with regard to $\\lambda$ and $\\mu$, so $\\theta$ being a pointwise infimum, \n",
    "$\\theta(x,\\lambda,\\mu) = \\inf_{x\\in X}L(x,\\lambda,\\mu)$ must be concave.\n",
    "\n",
    "\n",
    "## Weak Duality \n",
    "\n",
    "We have\n",
    "\n",
    "$$v_d^* = \\sup_{\\lambda\\geqslant 0, \\forall \\mu}\\theta(\\lambda,\\mu)\\leqslant \\inf_{x\\in X}\\sup_{\\lambda\\geqslant 0, \\forall \\mu}L(x,\\lambda,\\mu) = v_p^*,$$\n",
    "meaning that the supremum of the dual function provides a lower bound for the primal optimization problem. We call the new maximizing problem $$v_d^*=\\sup_{\\lambda\\geqslant 0, \\forall \\mu}\\theta(\\lambda,\\mu)$$\n",
    "as a Lagrangian dual of the original problem. \n",
    "\n",
    "\n",
    "\n",
    "Proof: For whatever $\\lambda_0,\\mu_0$, we have\n",
    "$$\\theta(\\lambda_0,\\mu_0)=\\inf_{x\\in X}L(x,\\lambda_0,\\mu_0)\\leqslant\\inf_{x\\in X}\\sup_{\\lambda\\geqslant 0, \\forall \\mu}L(x,\\lambda,\\mu).$$\n",
    "\n",
    "Since the right hand side is a constant, taking the supremum of the left yields the desired inequality.\n",
    "\n",
    "\n",
    "### Inspirations\n",
    "\n",
    "The weak duality theorem implies for arbitrary $\\lambda_0,\\mu_0$ and $x_0$ in the feasible set, \n",
    "$$\\theta(\\lambda_0,\\mu_0)\\leqslant v_d^*\\leqslant v_p^*\\leqslant f(x_0).$$\n",
    "Hence if in practice we can find $\\theta(\\lambda_0,\\mu_0)$ and $f(x_0)$ are near, the $v_d^*$ and $v_p^*$ are bounded inbetween.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "#### Linear Programming\n",
    "\n",
    "Determine the dual problem of \n",
    "$$\\min\\{c^Tx:\\quad Ax =b,\\quad x\\geqslant 0\\}$$\n",
    "\n",
    "Solution: Set $\\theta(\\lambda,\\mu) = \\inf_{x\\in\\mathbb R}\\left\\{c^Tx - \\lambda^T x +\\mu^T(Ax-b)\\right\\}$ and\n",
    "$$\\theta(\\lambda, \\mu) = \\left\\{\\begin{array}{ll}-\\mu^Tb & A\\mu -\\lambda +c = 0,\\\\ -\\infty & A\\mu -\\lambda +c \\neq 0.\\end{array}\\right.$$\n",
    "\n",
    "Thus, its dual is given by \n",
    "$$\\max_{\\lambda\\geqslant 0,\\mu} \\theta = \\max_{\\lambda = A\\mu+c\\geqslant 0}-\\mu^Tb\n",
    "=\\max\\{-\\mu^Tb:\\quad A\\mu+c\\geqslant 0\\}.$$\n",
    "If denote $y = -\\mu$, we obtain the standard form\n",
    "$$\\max\\{b^Ty:\\quad Ay\\leqslant c\\}.$$\n",
    "\n",
    "#### Semidefinite Programming\n",
    "\n",
    "Determine the dual problem of \n",
    "$$\\inf_Z \\ \\left\\{C\\bullet Z:\\quad  A_j\\bullet Z = b_j\\ (1\\leqslant j\\leqslant m)\\quad Z\\in \\mathcal S^n_{+}\\right\\}$$\n",
    "where $C$ and all $A_j$ are symmetric.\n",
    "\n",
    "Solution: Set $\\theta(\\mu) = \\inf_{Z\\in X=\\mathcal S_{++}^n} C\\bullet Z+\\sum\\mu_j\\left(b_j - A_j\\bullet Z  \\right) $. When $P=C-\\sum \\mu_jA_j$ is positive semidefinite, \n",
    "$$P\\bullet Z = {\\rm tr}(P^TZ) \\xlongequal{Cholesky} {\\rm tr}(LL^*RR^*)= {\\rm tr}(L^*RR^*L)\\geqslant 0.$$\n",
    "The equality holds, for example, at $Z = 0$.\n",
    "Thus in this case, $\\theta(\\mu) = b^T\\mu$.\n",
    "\n",
    "When the symmetric matrix $P=C-\\sum\\mu_jA_j$ is not positive semidefinite, it has at least one negative eigenpair $(\\lambda,v)$ where $\\lambda<0$ and $\\Vert v \\Vert = 1$. Also, $v$ is orthogonal to any other eigenvectors because $P$ is diagonalizable. Therefore, for $r>0$,\n",
    "$$P\\bullet (rvv^*) = {\\rm tr}(rPvv^*) = {\\rm tr}(r(\\lambda vv^*)(vv^*)) = \\lambda r\\rightarrow-\\infty.$$\n",
    "\n",
    "Hence we conclude that $$\\theta(\\mu) = \\left\\{\\begin{array}{ll}  b^T\\mu & C-\\sum \\mu_jA_j\\succeq 0,\\\\ -\\infty & {\\rm otherwise.}\n",
    "\\end{array}\\right.$$\n",
    "And the dual problem is given by the semidefinite programming\n",
    "$$\\max\\{b^T\\mu:\\quad \\sum \\mu_jA_j\\preceq C\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fenchel Dual\n",
    "\n",
    "Consider the optimization problem \n",
    "$$\\inf_{x\\in X_1\\cap X_2}\\left\\{f_1(x) + f_2(x)\\right\\},$$\n",
    "where $X_1,X_2\\in \\mathbb R^n\\ (X_1\\cap X_2 \\neq \\emptyset)$ and $f_1,f_2:\\ \\mathbb R^n\\rightarrow \\mathbb R$. Then the problem above is equivalent to \n",
    "$$\\inf_{(y,z)\\in\\mathbb (X_1,X_2)}\\{f_1(y)+f_2(z):\\quad y = z\\}.$$\n",
    "Find the dual problem of above.\n",
    "\n",
    "Solution: Introduce $\\theta(\\mu) = \\inf_{(y,z)\\in\\mathbb (X_1,X_2)}\\{f_1(y)+f_2(z) + \\mu^T(y-z)\\}$. Recall the definition of a conjugate function,\n",
    "$$f^*(u) = \\sup_x u^Tx - f(x) .$$\n",
    "Hence, \n",
    "$$\\theta(\\mu) = \\sup_{y}\\{-\\mu^Ty - f_1(y)\\} - \\sup_{z}\\{\\mu^Tz - f_2(z)\\}= f_1^*(-\\mu)+f_2^*(\\mu).$$\n",
    "\n",
    "And the dual problem is (by the equivalence one can exchange $f_1^*$ and $f_2^*$)\n",
    "$$\\sup_{\\mu\\in\\mathbb R^n}\\{f_1^*(-\\mu)+f_2^*(\\mu)\\}.$$\n",
    "\n",
    "#### LASSO\n",
    "\n",
    "Find the dual problem of\n",
    "$$\\min \\{\\frac12\\Vert Ax-b\\Vert^2+\\Vert x \\Vert_1\\}$$\n",
    "\n",
    "Solution: Mimic the Fenchel dual, rewrite the problem as $\\min \\{f_1(y)+f_2(z):\\quad  y=Az\\}.$ Introducing \n",
    "$\\theta(\\mu) = \\inf_{(y,z)}\\{f_1(z)+f_2(x) +\\mu^T(z - Ax)\\}$ leads to the dual\n",
    "$$\\sup_{\\mu}\\{f_1^*(-\\mu) +f_2^*(A^T\\mu)\\}.$$\n",
    "\n",
    "It suffices to compute the conjugate functions of the both. Recall the dual norm theory claims that $\\Vert x\\Vert_1$ has conjugate\n",
    "$\\left\\{\\begin{array}{ll}0 & \\Vert x\\Vert_{\\infty}\\leqslant 1\\\\ +\\infty& \\Vert x\\Vert_{\\infty}>1\\end{array}\\right.$. It remains to compute the \n",
    "conjugate of $\\frac12 \\Vert  x-b\\Vert^2$, which is given by  \n",
    "$$\\sup_y\\{x^Ty- \\frac 12\\Vert y-b\\Vert^2\\} = \n",
    "\\sup_y\\{-\\frac 12 \\Vert y - b - x\\Vert^2 +\\frac12 (b^Tx+x^Tx)\\}.$$\n",
    "\n",
    "Hence, the dual problem is\n",
    "$$\\sup_{\\Vert \\mu\\Vert_\\infty\\leqslant 1}\\frac12 \\left(b^T\\mu+\\mu^T\\mu\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QCQP\n",
    "\n",
    "Find the dual problem of \n",
    "$$\\min \\{x^TA_0x +2b_0^Tx +c_0:\\quad x^TA_ix+2b_i^Tx+c_i\\leqslant 0,\\ i=1,\\dotsc,m\\}$$\n",
    "where $A_0\\in\\mathcal S_{++}^n$ is positive and $A_i\\in\\mathcal S^n\\ (i=1,\\dotsc,m)$ are symmetric.\n",
    "\n",
    "Solution: The dual function is given by\n",
    "$$\\begin{aligned}\\theta(\\lambda) &= \\inf_x \\left\\{x^TA_0x +2b_0^Tx +c_0+\\sum_{i=1}^m\\lambda_i(x^TA_ix+2b_i^Tx+c_i)\\right\\}\n",
    "\\\\ &=\\sup_z\\left\\{x^TA_0x +2b_0^Tx +c_0+\\sum_{i=1}^m\\lambda_i(x^TA_ix+2b_i^Tx+c_i)-z\\geqslant 0,\\ \\forall x\\right\\}\n",
    "\\\\&=\\sup_z\\left\\{z:\\quad \\left[\\begin{matrix}A_0+\\sum_{i=1}^m \\lambda_i A_i & b_0+\\sum_{i=1}^m  \\lambda_i b_i\\\\ b^T+\\sum_{i=1}^m  \\lambda_i b_i^T & c_0 + \\sum_{i=1}^m c_i-z \\end{matrix}\\right]\\succeq 0 \\right\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "The dual problem is thus given by \n",
    "$$\\begin{aligned}\\sup_{\\lambda\\geqslant 0} \\theta(\\lambda)  \n",
    "&=\\sup_{\\lambda\\geqslant 0} \\sup_z\\left\\{z:\\quad\\left[\\begin{matrix}A_0+\\sum_{i=1}^m \\lambda_i A_i & b_0+\\sum_{i=1}^m  \\lambda_i b_i\\\\ b^T+\\sum_{i=1}^m  \\lambda_i b_i^T & c_0 + \\sum_{i=1}^m c_i-z \\end{matrix}\\right]\\succeq 0 \\right\\}\\\\ \n",
    "&=  \\sup_z\\left\\{z:\\quad\\left[\\begin{matrix}A_0+\\sum_{i=1}^m \\lambda_i A_i & b_0+\\sum_{i=1}^m  \\lambda_i b_i\\\\ b^T+\\sum_{i=1}^m  \\lambda_i b_i^T & c_0 + \\sum_{i=1}^m c_i-z \\end{matrix}\\right]\\succeq 0,\\ \\exists \\lambda\\geqslant0 \\right\\},\n",
    "\\end{aligned}\n",
    "$$\n",
    "which is  a SDP problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong Duality\n",
    "\n",
    "If in the weak duality theorem the inequality turns to an equality, i.e.\n",
    "$$v_d^* = \\sup_{\\mu,\\lambda}\\inf_{x} L =\\inf_{x}\\sup_{\\mu,\\lambda}L=v_p^*,$$\n",
    "we call it a strong duality. \n",
    "\n",
    "### Saddle Point\n",
    "\n",
    "Call $(\\bar x,\\bar \\lambda,\\bar \\mu)$ a saddle point of Lagrangian $L$ if $\\bar x \\in X$, $\\lambda \\geqslant 0$ and \n",
    "for all $x\\in X$ and $(\\lambda,\\mu)\\in \\mathbb R^m_+\\times \\mathbb R^n$ we have \n",
    "$$L(\\bar x,\\lambda,\\mu)\\leqslant L(\\bar x,\\bar \\lambda,\\bar \\mu)\\leqslant L(x,\\bar \\lambda,\\bar \\mu).$$\n",
    "\n",
    "It is the relative minimum with respect to $x$ while maximum with respect to $\\lambda,\\mu$.\n",
    "\n",
    "### Strong Duality\n",
    "\n",
    "Lagrangian $L$ exists a saddle point $(\\bar x,\\bar \\lambda,\\bar \\mu)$ iff the problem has strong duality and the saddle point is an optimal solution of the original problem.\n",
    "\n",
    "Proof: $\\Rightarrow$ side: For a saddle point $(\\bar x,\\bar \\lambda,\\bar \\mu)$, on the one hand we have \n",
    "$$v_d^*=\\sup_{\\lambda,\\mu}\\inf_x L(x,\\lambda,\\mu)\\geqslant \\inf_x L(x,\\bar \\lambda,\\bar\\mu)\n",
    "=L(\\bar x,\\bar \\lambda,\\bar \\mu).$$\n",
    "On the other,\n",
    "$$v_p^* = \\inf_x\\sup_{\\lambda,\\mu} L(x,\\lambda,\\mu) \\leqslant \\sup_{\\lambda,\\mu} L(\\bar x,\\lambda,\\mu)\n",
    "=L(\\bar x,\\bar \\lambda,\\bar \\mu).$$\n",
    "So $v_d^*\\geqslant v_p^*$, which implies that $v_d^*=v_p^*=L(\\bar x,\\bar \\lambda,\\bar \\mu)$.\n",
    "\n",
    "$\\Leftarrow$ side: The optimal solution under strong duality is certainly the global minimizer of \n",
    "$L(x,\\bar \\lambda,\\bar \\mu)$ and the maximizer of $L(\\bar x,\\lambda,\\mu)$.\n",
    "\n",
    "\n",
    "### Optimality Conditions\n",
    "\n",
    "The point $(\\bar x,\\bar \\lambda,\\bar \\mu)$ is a saddle point of the Lagrangian of the primal problem\n",
    "$$\\inf_x \\{f(x):\\quad G(x)\\leqslant 0,\\quad H(x)=0,\\quad x\\in X\\}$$ \n",
    "iff the following three conditions hold:\n",
    "$$\\begin{aligned}\n",
    "\\bar x\\in X,\\quad G(\\bar x)\\leqslant 0,\\quad H(\\bar x)&=0\\\\ \n",
    "\\bar \\lambda\\geqslant 0,\\quad \\bar x &=  {\\rm argmin}_{x\\in X}L(x,\\bar \\lambda,\\bar \\mu)\\\\\n",
    "\\bar \\lambda^T G(\\bar x) &= 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "Proof: $\\Rightarrow$ side: Given a saddle point  $(\\bar x,\\bar \\lambda,\\bar \\mu)$  it is by definition that\n",
    "$\\bar \\lambda\\geqslant 0,\\ \\bar x =  {\\rm argmin}_{x\\in X}L(x,\\bar \\lambda,\\bar \\mu)$. Also, we know that the saddle point is the optimal solution so $\\bar x\\in X,\\ G(\\bar x)\\leqslant 0,\\ H(\\bar x)=0$. Finally if $\\lambda_j g_j (\\bar x)\\neq 0$, there exists some perturbation $\\epsilon\\neq 0$ such that $\\epsilon g_j(\\bar x)>0$ and $\\lambda_j +\\epsilon>0$. This concludes that replacing $\\lambda_j$ with $\\lambda_j+\\epsilon$ results in a greater Lagrangian, a \n",
    "contradiction.\n",
    "\n",
    "$\\Leftarrow$ side: It suffices to show that $L(\\bar x,\\bar \\lambda,\\bar \\mu)\\geqslant L(\\bar x,\\lambda,\\mu)$. Since \n",
    "$\\bar \\lambda^TG(\\bar x) = 0 = H(\\bar x)=0$, the inequality is equivalent to $f(\\bar x)\\geqslant f(\\bar x)+\\lambda^T G(\\bar x)$, which certainly holds as $\\lambda\\geqslant 0$ and $G(\\bar x)\\leqslant 0$.\n",
    "\n",
    "### Convexity\n",
    "\n",
    "Consider a convex optimization with $X$ being open,\n",
    "$$\\inf_x \\{f(x):\\quad G(x)\\leqslant 0,\\quad H(x)=0,\\quad x\\in X\\}.$$\n",
    "If all $f,g,h$ are continuously differentiable and there is an optimal solution that satisfies the Slater condition, it has strong duality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Inequalities\n",
    "\n",
    "### Proper Cone \n",
    "\n",
    "Call a cone $K$ a proper cone if\n",
    "1. $K$ is closed.\n",
    "2. $K$ is convex.\n",
    "3. $K$ is solid (meaning it has nonempty interior).\n",
    "4. $K$ is pointed (meaning it contains no lines but rays).\n",
    "\n",
    "We define the partial ordering by $x\\succeq_K y\\Leftrightarrow x-y\\in K$ and\n",
    " $x\\succ_K y\\Leftrightarrow x-y\\in {\\rm int}K$. If not confusing, we use the simlified notation $\\succeq $ and $\\succ$. Then one can verify it has the following properties:\n",
    " 1. Transitive: $x\\succeq y,\\ y\\succeq z\\Rightarrow x \\succeq z$. In addition if $x\\succ y$ is strict, so is $x\\succ z$.\n",
    "   \n",
    "Proof: The convexity guarantees that $x - z=(x-y)+(y-z)\\in K$. When $x\\succ y$ is strict, there must be some sufficiently small $r>0$ such that $O(x-y,r)\\in K$. Hence for any $\\delta$ that $\\Vert \\delta\\Vert<r$, we have \n",
    "$$x-z+\\delta = (x-y+\\delta)+(y -z)\\in K.$$\n",
    "This implies that $x-z\\in {\\rm int}K$.\n",
    "\n",
    "2. Additive: $x_1\\succeq y_1$ and $x_2\\succeq y_2$, then $x_1+x_2\\succeq y_1+y_2$. In addition if \n",
    "   $x_1\\succ y_1$ is strict, so is $x_1+x_2\\succ y_1+y_2$.\n",
    "\n",
    "Proof: Follow the proof to the transitivity.\n",
    "\n",
    "3. Scaling-preserved: $x\\succeq y$ and $\\lambda>0$, then $\\lambda x\\succeq \\lambda y$. In addition if \n",
    "$x\\succ y$ is strict, so is $\\lambda x\\succ \\lambda y$.\n",
    "\n",
    "4. Limit-preserved: $\\{x_k\\}\\succeq \\{y_k\\}$, then $\\lim x_k\\succeq \\lim y_k$.\n",
    "\n",
    "\n",
    "\n",
    "### Dual Cone\n",
    "\n",
    "$K$ is a cone and we define its dual by $K^*  = \\{y:\\ \\langle x,y\\rangle \\geqslant 0,\\ \\forall x\\in K\\}$. Dual cones are the normals of all the supporting hyperplanes at the origins. In a finite dimensional space, we can show that the dual  of proper cones are also proper cones.\n",
    "\n",
    "Proof: Suppose $y_1,y_2\\in K^*$ and $\\alpha \\in [0,1]$. Then $\\alpha y_1+(1-\\alpha)y_2\\in K^*$ because \n",
    "$$\\langle x,\\alpha y_1+(1-\\alpha)y_2\\rangle = \\alpha \\langle x,y_1\\rangle +(1-\\alpha)\\langle x,y_2\\rangle\\geqslant 0\\\n",
    "\\quad \\forall x\\in K.$$ \n",
    "\n",
    "Besides convex, $K^*$ is closed since for any convergent $\\{y_k\\}\\in K^*$ and for all $x\\in K$ we have \n",
    "$\\langle x,\\lim y_k\\rangle \\geqslant 0$ since $\\langle x,y_k\\rangle\\geqslant 0$.\n",
    "\n",
    "Next, consider all the supporting hyperplanes... (too hard?)\n",
    "\n",
    "\n",
    "### Self-dual Cone\n",
    "\n",
    "A self-dual cone is a cone that it is the dual itself. Several examples are \n",
    "1. Non-negative Orthant: $\\mathbb R_+^n$\n",
    "2. Lorentz Cone (Second-order Cone / Icecream Cone): $\\{(t,x):\\ t\\geqslant \\Vert x\\Vert\\}$.\n",
    "3. Positive Semidefinite Cone: $\\mathcal S_+^n$.\n",
    "   \n",
    "### Generalized Convexity\n",
    "\n",
    "Call a function $K$-convex if ${\\rm dom}f$ is convex and \n",
    "$$\\alpha f(x)+(1-\\alpha)f(y)\\succeq f(\\alpha x+(1-\\alpha)y)$$\n",
    "for all $x,y\\in{\\rm dom}f$ and $\\alpha \\in [0,1]$.\n",
    "\n",
    "### Generalized Optimzation Problem\n",
    "\n",
    "The following optimization can be viewed as a generalized optimization.\n",
    "$$\\min\\{f(x):\\quad 0\\succeq_{K_i} g_i(x),\\quad H(x)=0,\\quad x\\in X\\}$$\n",
    "\n",
    "Examples:\n",
    "1. CLP (Cone Linear Programming) is the generalization of LP. It is in the following form,\n",
    "$$\\min\\{c\\bullet x:\\quad a_i\\bullet x = b_i,\\quad  x\\succeq_{K_i}0 \\}.$$\n",
    "The $\\bullet$ symbol stands for the generalized inner product.\n",
    "\n",
    "2. SOCP, the following problem, is a special case of CLP. \n",
    "$$\\min\\{f^Tx:\\quad \\Vert A_i x+b_i\\Vert \\leqslant c_i^Tx +d_i,\\quad g_i^Tx = h_i\\}$$\n",
    "Because it can be rewritten as, if denote by $K_i$ the second-order cone \n",
    "$K_i = \\{(w,t)\\in\\mathbb R^{n_i}\\times \\mathbb R:\\ \\Vert w\\Vert \\leqslant t\\}$,\n",
    "$$\\min_{x,u}\\{f^Tx:\\quad g_i^Tx = h_i,\\quad w_i - A_i x= b_i,\\quad t_i = c_i^Tx+d_i,\\quad  (w_i,t_i)\\succeq_{K_i}  0 \\}.$$\n",
    "\n",
    "3. SDP can be treated as a generalized optimization problem if $K = \\mathcal S_+^n$,\n",
    "$$\\min\\{c^Tx:\\quad 0\\succeq_K -B + \\sum_{i=1}^m x_i A_i\\}$$\n",
    "\n",
    "### Generalized Lagrangian Multipliers\n",
    "\n",
    "Consider the generalized optimzation problem \n",
    "$$\\min\\{f(x):\\quad 0\\succeq_{K_i} g_i(x),\\quad H(x)=0,\\quad x\\in X\\}$$\n",
    "\n",
    "Now the corresponding lagrangian multiplies leads to the generalized dual problem,\n",
    "$$\\sup_{\\lambda_i \\in K_i^*, \\mu}\\inf_x \\left\\{f(x)+\\sum_{i=0}^m \\lambda_i\\bullet g_i(x) + \\mu^TH(x)\\right\\}.$$\n",
    "\n",
    "The weak duality still holds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "#### SDP Relaxation for QCQP\n",
    "\n",
    "The QCQP, $(A_0\\in \\mathcal S_+^n,\\ A_i\\in\\mathcal S^n (i\\geqslant 1))$\n",
    "$$\\min\\{x^TA_0 x+2b_0^Tx:\\quad x^TA_ix+2b_i^Tx+c_i\\leqslant 0\\}$$\n",
    "can be rewritten as\n",
    "$$\\min\\{A_0\\bullet xx^T+2b_0^Tx:\\quad A_i\\bullet xx^T+2b_i^Tx+c_i\\leqslant 0\\}$$\n",
    "\n",
    "Introduce $X\\in\\mathcal S_+^n$, then $X = xx^T\\Rightarrow \\left[\\begin{matrix} X&x\\\\x^T & 1\\end{matrix}\\right]\\succeq 0$ presents a necessary condition. Therefore,\n",
    "$$\\begin{aligned}\\min\\{A_0\\bullet xx^T+2b_0^Tx:\\quad A_i\\bullet xx^T+2b_i^Tx+c_i\\leqslant 0\\}\\\\ \n",
    "\\geqslant \\min_{X\\in\\mathcal S_+^n,x}\\{A_0\\bullet X+2b_0^Tx:\\quad A_i\\bullet X+b_i^Tx+c_i\\leqslant 0,\\quad \\left[\\begin{matrix} X&x\\\\x^T & 1\\end{matrix}\\right]\\succeq 0\\}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now we show that the dual of the two problems above are equivalent. Since we have computed the dual of QCQP above, it remains to the follows.\n",
    "\n",
    "$$\\begin{aligned} &\\sup_{\\Lambda\\in \\mathcal S_+^n,\\ \\lambda\\geqslant 0} \\inf_x\n",
    "\\left\\{A_0\\bullet X+2b_0^Tx+\\sum_{i=1}^m\\lambda_i(A_i\\bullet X+2b_i^Tx + c_i)-\\Lambda\\bullet \n",
    "\\left[\\begin{matrix}X&x\\\\x^T &1 \\end{matrix}\\right]\\right\\}\\\\ \n",
    " =& \\sup_{Y,w,z,\\lambda\\geqslant 0}\\inf_x\n",
    "\\left\\{A_0\\bullet X+2b_0^Tx+\\sum_{i=1}^m\\lambda_i(A_i\\bullet X+2b_i^Tx + c_i)-\n",
    "\\left[\\begin{matrix}Y&w\\\\w^T &z \\end{matrix}\\right]\\bullet \n",
    "\\left[\\begin{matrix}X&x\\\\x^T &1 \\end{matrix}\\right],\\quad\\left[\\begin{matrix}Y&w\\\\w^T &z \\end{matrix}\\right]\\in \\mathcal S_+^n\\right\\}\\\\\n",
    " =& \\sup_{Y,w,z,\\lambda\\geqslant 0} \n",
    "\\left\\{ \\sum_{i=1}^m\\lambda_i c_i-z:\\quad \\quad Y =A_0+\\sum_{i=1}^m\\lambda A_i ,\\quad w\n",
    "=b_0+\\sum_{i=1}^m\\lambda b_i,\\quad \\left[\\begin{matrix}Y&w\\\\w^T &z \\end{matrix}\\right]\\in \\mathcal S_+^n\\right\\}\\\\ \n",
    " =& \\sup_{z} \n",
    "\\left\\{ z:\\quad    \\left[\\begin{matrix}A_0+\\sum_{i=1}^m\\lambda A_i&b_0+\\sum_{i=1}^m\\lambda b_i\\\\b_0^T+\\sum_{i=1}^m\\lambda b_i^T &\\sum_{i=1}^m\\lambda_i c_i-z \\end{matrix}\\right]\\in \\mathcal S_+^n\\right\\}\n",
    ".\\end{aligned}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Programming\n",
    "\n",
    "### LP Strong Duality\n",
    "\n",
    "The LP, $\\min\\{c^Tx:\\ A^Tx=b,\\ x\\geqslant 0\\}$ has dual $\\max\\{b^T\\mu:\\ A^T\\mu\\leqslant c\\}$ and it is a strong duality.\n",
    "\n",
    "Proof: Since the inequality constraints are affine, it satisfies the Slater condition and has strong duality.\n",
    "\n",
    "### CLP Strong Duality\n",
    "\n",
    "For CLP problems, we require explicit Slater condition getting satisfied, i.e.\n",
    "$\\inf\\{c\\bullet x:\\ a_i\\bullet x = b_i\\ x\\succeq_K 0\\}$ and its dual \n",
    "$\\sup\\{b^T\\mu:\\ c\\succeq_{K^*}\\sum a_i\\mu_i\\}$. \n",
    "\n",
    "If the primal one is bounded below and satisfies the Slater condition, for example there is a strictly feasible point $x\\succeq_K 0$, then it is a strong duality and the dual supremum is achieveable. If the dual one is bounded above and satisfies the Slater condition,  the it is a strong duality and the primal infimum is achieveable. [3]\n",
    "\n",
    "Example: The CLP $\\inf\\{x_1:\\ \\sqrt{(x_1 - x_2)^2 +1}\\leqslant x_1+x_2\\}$ has dual\n",
    "$$\\sup_{\\sqrt{v_1^2+v_2^2}\\leqslant t}\\inf_x\\{x_1 - t(x_1+x_2) - v_1(x_1 - x_2) - 1\\}.$$\n",
    "It is clear that in the dual problem it suffices to consider $1 - t - v_1 =0,\\ -t +v_1 = 0$, which gives \n",
    "$v_1 = t = 1/2$ and $v_2 = 0$ as a consequence. It is thus not strictly feasible and the primal problem cannot reach the optimal value zero despite $f(1/n,n/4)\\rightarrow 0$ as $n\\rightarrow \\infty$.\n",
    "\n",
    "### RLP\n",
    "\n",
    "Recall that RLP stands for the robust linear programming. As a special case we consider the RLP \n",
    "$\\inf \\{c^Tx:\\ \\forall a_i\\in \\mathcal U_i, a_i^T x\\leqslant b_i\\}$. Suppose $\\mathcal U_i$ is defined by \n",
    "$\\mathcal U_i = \\{a_i:\\ D_ia_i\\leqslant e_i\\}$.\n",
    "\n",
    "Proof: For each constraint $\\forall a_i\\in \\mathcal U_i, a_i^Tx \\leqslant b_i$ can be rewritten as \n",
    "$\\min\\{-x^Ta_i:\\ D_ia_i\\leqslant e_i\\}\\geqslant -b_i$. We begin with translating the minimum by its dual, \n",
    "$\\max\\{-e_i^T\\mu_i:\\ D_i^T\\lambda=x\\}$. Thus the original RLP is equivalent to \n",
    "$$\\begin{aligned}&\\inf\\{c^Tx:\\ \\max\\{-e_i^T\\mu_i:\\ D_i^T\\lambda=x,\\ \\lambda\\geqslant 0\\}\\geqslant -b_i\\}\\\\\n",
    "=\\ &\\inf_{x,\\lambda} \\{c^Tx:\\ e_i^T\\lambda \\leqslant b_i,\\ D_i^T\\lambda = x,\\ \\lambda \\geqslant 0\\}.\\end{aligned}$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1403262caf5c96c562d23c2ec1d0e9409dfef636549d4d9dd4887bed656bec38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
