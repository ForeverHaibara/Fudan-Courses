{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Newton's Method\n",
    "\n",
    "In this section we shall assume $f$ is twice continuously differentiable and convex. Intuitively, for a small step attempt $d_k$ we have \n",
    "$$f(x_k+d_k)\\approx f(x_k) + \\nabla f(x_k)^Td_k + \\frac 12 d_k^T \\nabla^2 f(x_k)d_k.$$\n",
    "The right hand side is a quadratic form of  $p_k$ with minimizer  \n",
    "at zero derivative \n",
    "$$\\nabla^2 f(x_k)d_k + \\nabla f(x_k)=0\\quad \\Leftrightarrow \\quad d_k = - \\nabla^2f(x_k)^{-1} \\nabla f(x_k).$$\n",
    "The step size $d(x_k) = d_k = - \\nabla^2f(x_k)^{-1} \\nabla f(x_k)$ is called the Newton step or Newton direction. \n",
    "\n",
    "### Affine Invariance\n",
    "Newton method is invariant under affine transformation on the function.\n",
    "\n",
    "### Newton's Decrement\n",
    "Define $\\lambda(x) = \\Vert \\nabla f(x_k)^Td(x_k)\\Vert^\\frac 12$. Then $\\frac 12\\lambda ^2$ is the upper bound of decrement in the step, i.e.\n",
    "$$\\begin{aligned}f(x_k+d_k)- f(x_k)&\\geqslant  \\nabla f(x_k)^Td_k + \\frac 12 d_k^T \\nabla^2 f(x_k)d_k\\\\ &\n",
    "=\\frac 12\\left(\\nabla ^2f(x_k)^{-1}\\nabla f(x_k)- d_k\\right)\n",
    "\\nabla^2 f(x_k)\\left(\\nabla ^2f(x_k)^{-1}\\nabla f(x_k)- d_k\\right)\\\\ \n",
    "&\\quad\\quad - \\frac 12\\nabla f(x_k)^T\\nabla ^2 f(x_k)^{-1}\\nabla f(x_k)\\\\ \n",
    "&\\geqslant -\\frac 12\\lambda^2\\end{aligned}.$$\n",
    "\n",
    "## Damped Newton's Method\n",
    "\n",
    "Plug the Armoji line search into Newton's method, we determine $\\alpha\\in (0,\\frac 12),\\beta\\in (0,1]$ in the beginning and in each step we  take \n",
    "$t_k = \\beta^{r_k}\\ (r_k\\in \\mathbb N)$ such that $x_{k+1} = x_k +t_kd_k$ has the property that \n",
    "$$f(x_{k+1}) - f(x_k)\\leqslant -\\alpha |\\nabla f(x_k)^T(x_{k+1} - x_k)| =\\alpha t_k \\nabla f(x_k)^T d_k.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Rate\n",
    "\n",
    "To analyze the convergence rate of the damped Newton's method, we further assume that $f$ is strongly convex where $\\nabla ^2 f(x)^T\\succ mI\\ (m>0)$ and $\\nabla f$ is Lipschitz continuous by $\\Vert \\nabla f(x) - \\nabla f(y)\\Vert \\leqslant L\\Vert x-y\\Vert$ so that $mI\\prec \\nabla ^2 f(x) \\preceq LI$. Also we require that $\\Vert \\nabla^2 f(x) - \\nabla^2 f(y)\\Vert \\leqslant L_2\\Vert x-y\\Vert$.\n",
    "\n",
    "Then there exists some $y,r$ such that $0<y<\\frac {m^2}{L}$ and $r>0$ and \n",
    "* Global convergence: if $\\Vert \\nabla f(x_k)\\Vert >y$, then $f(x_{k+1}) - f(x_k) \\leqslant -r$. \n",
    "* Local quadratic convergence: if $\\Vert \\nabla f(x_k)\\Vert \\leqslant y$, we have $t_k = 1$ and $\\frac{ L_2}{2m^2}\\Vert \\nabla f(x_{k+1})\\Vert \\leqslant \\left(\\frac{ L_2}{2m^2}\\Vert \\nabla f(x_k)\\Vert \\right)^2$.\n",
    "\n",
    "Proof: First we plug in $x_{k+1} - x_k = t_kd_k$ and $d_k = -\\nabla^2 f(x_k)^{-1}\\nabla f(x_k)$, and then we have \n",
    "$$\\begin{aligned}f(x_{k}+t_k d_k) - f(x_k)&\\leqslant t_k \\nabla f(x_k)^T d_k + \\frac L2 t_k^2 d_k^T d_k\\end{aligned}$$\n",
    "and \n",
    "$$f(x_{k}+t_k d_k) - f(x_k)-\\alpha t_k\\nabla f(x_k)^T d_k\\leqslant \n",
    " (1-\\alpha )t_k \\nabla f(x_k)^T d_k + \\frac L2 t_k^2 d_k^T d_k .$$\n",
    "Thus, when $0\\leqslant t_k \\leqslant \\frac{-(1-\\alpha ) \\nabla f(x_k)^T d_k }{\\frac L2  d_k^T d_k }$, we have $f(x_{k}+t_k d_k) - f(x_k)-\\alpha t_k\\nabla f(x_k)^T d_k \\leqslant 0$. Now we note that all the eigenvalues of \n",
    "$(\\nabla^2)^{-1}- m(\\nabla ^2)^{-2}$ are nonnegative, we learn \n",
    "$$\\nabla f(x_k)^T\\nabla^2f(x_k)^{-1}\\nabla f(x_k) \\geqslant m \\nabla f(x_k)^T\\nabla^2f(x_k)^{-2}\\nabla f(x_k)\n",
    "\\quad\\Rightarrow\\quad -\\nabla f(x_k)^T d_k \\geqslant m  d_k^Td_k.$$\n",
    "So, \n",
    "$$\\frac{(1-\\alpha )\\cdot (- \\nabla f(x_k)^T d_k) }{\\frac L2  d_k^T d_k }\\geqslant \\frac{m(1-\\alpha)}{\\frac L2}\n",
    "\\geqslant \\frac{\\frac m2}{\\frac L2}= \\frac mL.$$\n",
    "\n",
    "Therefore, any $t_k\\leqslant \\frac mL$ must be a valid step. So the damped Newton's method in each iteration would return a step size no smaller than $\\beta \\frac{m}{L}$ by interrupted line search. Hence\n",
    "$$ \\begin{aligned}f(x_{k}+t_k d_k) - f(x_k) &\\leqslant -\\alpha t_k \\cdot (- \\nabla f(x_k)^Td_k) \n",
    "= -\\alpha t_k \\nabla f(x_k)^T\\nabla^2 f(x_k)^{-1}\\nabla f(x_k)\\\\ &\n",
    "\\leqslant - \\frac{\\alpha t_k}{L} \\nabla f(x_k)^T \\nabla f(x_k)\n",
    " \\leqslant - \\frac{\\alpha \\beta m}{L^2} \\nabla f(x_k)^T \\nabla f(x_k)\\\\ &\\leqslant - \\frac{\\alpha \\beta m}{L^2}y^2\n",
    "\\quad\\quad (\\Vert \\nabla f(x_k)\\Vert \\geqslant y)\\end{aligned} $$\n",
    "\n",
    "Moreover, we have the $3$ rd order Taylor expansion that \n",
    "$$\n",
    "\\begin{aligned}f(x_k + t_kd_k) - f(x_k)&\\leqslant \\nabla f(x_k)^Td_k +\\frac 12 d_k^T\\nabla ^2 f(x_k)d_k + \\frac {L_2}{6} \\Vert d_k\\Vert^3\\\\ &\\leqslant \\frac12  \\nabla f(x_k)^Td_k   + \\frac {L_2}{6} (\\frac 1m (-\\nabla f(x_k)^Td_k) )^\\frac 32\n",
    "\\end{aligned} $$\n",
    "When $- \\nabla f(x_k)^Td_k <m\\left(\\frac{3(1 - 2\\alpha)}{L}\\right)^2$, we further deduce  \n",
    "$$f(x_k + t_kd_k) - f(x_k)\\leqslant \\alpha \\nabla f(x_k)^Td_k\\quad\\Rightarrow\\quad t_k = 1.$$\n",
    "So, as long as $\\Vert \\nabla f(x_k)\\Vert$ is small enough, it will \n",
    "lead to $- \\nabla f(x_k)^Td_k <m\\left(\\frac{3(1 - 2\\alpha)}{L}\\right)^2$ and thus $t_k = 1$.  In this case, \n",
    "$$\\begin{aligned}\\Vert \\nabla f(x_k+d_k)\\Vert &\n",
    " = \\left\\Vert \\nabla f(x_k+d_k) - \\nabla f(x_k) -\\nabla ^2 f(x_k) d_k\\right\\Vert \\\\ \n",
    " &= \\left\\Vert \\int_0^1 \\left( \\nabla^2 f(x_k + ud_k) -  \\nabla^2 f(x_k)\\right)d_kdu\\right\\Vert\\\\ \n",
    " &\\leqslant \\int_0^1 \\left\\Vert \\nabla^2 f(x_k + ud_k) -  \\nabla^2 f(x_k)\\right\\Vert\\left\\Vert d_k\\right\\Vert du\\\\ \n",
    "  &\\leqslant  L_2 \\int_0^1 u \\left\\Vert d_k\\right\\Vert^2 du\\\\ &\n",
    "   \\leqslant \\frac{ L_2}{2} \\cdot \\frac{1}{m^2}\\left\\Vert \\nabla f(x_k)\\right\\Vert^2 .\n",
    " \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inexact Newton's Method\n",
    "\n",
    "There are cases when we solve the  linear system $\\nabla^2 f(x_k) d_k =- \\nabla f(x_k)$ by \n",
    "**iterative method** like CG (conjugate gradient) to reduce time consumption. And suppose that there are residuals between the exact solution and our \n",
    "$d_k$, i.e.\n",
    "$$r_k = \\nabla^2 f(x_k)d_k + \\nabla f(x_k),$$\n",
    "the Newton's method still performs well as long as $r_k$ is sufficiently small. This is called inexact Newton's Method. The convergence rate is characterized as follows. \n",
    "\n",
    "<br>\n",
    "\n",
    "Before we start, we assume that $f$ is twice continuously differentiable and $\\nabla^2 f(x)\\succ 0$, and \n",
    "$\\Vert r_k\\Vert \\leqslant \\eta_k\\Vert \\nabla f(x_k)\\Vert$. \n",
    "\n",
    "### Linearity\n",
    "\n",
    "If $\\eta_k \\in [0,t)$ is bounded and $t<1$. Then it has local linear convergence.\n",
    "\n",
    "Proof: Since  $\\nabla^2 f(x)\\succ 0$, $\\Vert \\nabla^2 f(x)\\Vert $ is positive on a closed area and has nonzero infimum $L$. Then \n",
    "$$\\Vert d_k\\Vert = \\Vert \\nabla^2f(x_k)^{-1}(r_k - \\nabla f(x_k) )\\Vert\n",
    "\\leqslant \\Vert \\nabla^2 f(x_k)^{-1}\\Vert \\cdot \\Vert r_k - \\nabla f(x_k)\\Vert \\leqslant\n",
    "\\frac 1L(1+\\eta_k)\\Vert \\nabla f(x_k)\\Vert$$\n",
    "is bounded locally.\n",
    " \n",
    "Also,\n",
    "$$\\begin{aligned}\\Vert \\nabla f(x_{k+1})\\Vert &\n",
    "\\leqslant \\Vert \\nabla f(x_k) + \\int_0^1 \\nabla ^2 f(x_k + td_k)d_kdt\\Vert\\\\&\n",
    "= \\Vert \\nabla f(x_k) + \\nabla^2f(x_k)d_k+ \\int_0^1 (\\nabla ^2 f(x_k + td_k) - \\nabla^2 f(x_k))d_kdt\\Vert  \\\\ \n",
    "&\\leqslant \\Vert\\nabla f(x_k) +  \\nabla^2f(x_k)d_k\\Vert + o(\\Vert d_k\\Vert)\\\\\n",
    "&= \\Vert r_k \\Vert + o(\\Vert d_k\\Vert).\n",
    "\\end{aligned}\n",
    "$$\n",
    "Thus, as long as $\\Vert d_k\\Vert$ is small enough to be omitted,\n",
    "$$\\Vert \\nabla f(x_{k+1})\\Vert  \\leqslant  \\Vert r_k\\Vert +\\Vert o(\\Vert d_k\\Vert)\\Vert \\leqslant \\frac{1+\\eta_k}{2}\\Vert \\nabla f(x_k)\\Vert<\\frac{1+t}{2}\\Vert \\nabla f(x_k)\\Vert\n",
    ". $$ \n",
    "\n",
    "\n",
    "### Superlinearity\n",
    "\n",
    "If $\\lim \\eta_k= 0$, then it has local superlinear convergence.\n",
    "\n",
    "Proof: Because\n",
    "$$\\frac{\\Vert \\nabla f(x_{k+1})\\Vert}{\\Vert \\nabla f(x_{k})\\Vert}\\leqslant \n",
    "\\frac{  \\Vert r_k\\Vert +\\Vert o(\\Vert d_k\\Vert)\\Vert}{\\Vert \\nabla f(x_{k})\\Vert}\\rightarrow 0.$$\n",
    "\n",
    "### Quadratic Convergence\n",
    "\n",
    "If $\\nabla^2f(x_k)$ is Lipschitz continuous around $x_*$ and $\\eta_k = O(\\Vert \\nabla f(x_k)\\Vert)$, then it has local quadratic convergence.\n",
    "\n",
    "Proof: Modify the proof above by \n",
    "$$\\begin{aligned}\\Vert \\nabla f(x_{k+1})\\Vert &\n",
    "\\leqslant \\Vert \\nabla f(x_k) + \\int_0^1 \\nabla ^2 f(x_k + td_k)d_kdt\\Vert\\\\&\n",
    "= \\Vert \\nabla f(x_k) + \\nabla^2f(x_k)d_k+ \\int_0^1 (\\nabla ^2 f(x_k + td_k) - \\nabla^2 f(x_k))d_kdt\\Vert  \\\\ \n",
    "&\\leqslant \\Vert\\nabla f(x_k) +  \\nabla^2f(x_k)d_k\\Vert + O(\\Vert d_k\\Vert^2)\\\\\n",
    "&=\\Vert r_k \\Vert + O(\\Vert d_k\\Vert^2)\\\\\n",
    "&\\leqslant \\eta_k \\Vert \\nabla f(x_k)\\Vert +  + O(\\Vert d_k\\Vert^2)\\\\ &= O(\\Vert \\nabla f(x_k)\\Vert^2).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-Newton's Method\n",
    "\n",
    "There are also cases when the Hessian is difficult to compute, and we turn to approximate the Hessian by \n",
    "alternative methods. This is called the quasi-Newton's method.\n",
    "\n",
    "Suppose we approximate $\\nabla^2 f(x_k)$ by $B_k$ in the $k$-th step, then we choose $p_k = -B_k^{-1}p_k$ and update \n",
    "$x_{k+1} = x_k +t_kp_k$. \n",
    "\n",
    "Assume $B_{k+1}$ is obtained by slightly updating $B_k$ after having solved $x_{k+1}$ at the $k$-th step. It should have the property that it imitates the Hessian, \n",
    "$$B_{k+1}(x_{k+1} - x_k) = \\nabla f(x_{k+1}) - \\nabla f(x_k).$$\n",
    "\n",
    "For simplification, we write $s_k = x_{k+1} - x_k$ and $y_k =  \\nabla f(x_{k+1}) - \\nabla f(x_k)$, and then \n",
    "$B_{k+1}s_k = y_k$. If we add the constraint that $B_{k+1} - B_k$ is small (with respect to some norm), it leads to the DFP.\n",
    "\n",
    "### DFP\n",
    "\n",
    "A good choice of $B_{k+1}$ is solved by \n",
    "$$\\min\\{\\Vert B - B_k\\Vert_{F,W}:\\quad B = B^T\\quad Bs_k = y_k\\},$$\n",
    "where $W = \\left(\\int_0^1 \\nabla ^2f(x_k + \\tau t_k x_k)d\\tau\\right)^{-1}$ is the inverse of the average of Hessian, $  Wy_k =s_k$ and  the norm $\\Vert X\\Vert_{F,W}$ stands for $\\Vert W^\\frac 12 XW^\\frac 12\\Vert_F$.\n",
    "\n",
    "Then,\n",
    "$$B_{k+1}  = \\left(I -\\frac{y_ks_k^T}{y_k^Ts_k}\\right)B_k\\left(I -\\frac{s_ky_k^T}{y_k^Ts_k}\\right)+\\frac{y_k y_k^T}{y_k^Ts_k}$$\n",
    "and its inverse can be updated by Sherman-Morrison-Windbury formula,\n",
    "$$B_{k+1}^{-1} = B_k^{-1} - \\frac{B_k^{-1}y_ky_k^TB_k^{-1}}{y_k^TB_k^{-1}y_k}+\\frac{s_ks_k^T}{y_k^Ts_k}.$$\n",
    "\n",
    "(The updating formula does not require the computation of $W$.)\n",
    "\n",
    "<br>\n",
    "\n",
    "Proof: Denote $\\hat B = W^\\frac 12 BW^\\frac 12$ and   $\\hat B_k = W^\\frac 12 B_kW^\\frac 12$, and the problem becomes minimizing $$\\Vert B - B_k\\Vert_{F,W}^2=\\Vert W^\\frac 12B W^\\frac 12- W^\\frac 12B_kW^\\frac 12\\Vert_{F}^2=\\Vert \\hat B - \\hat B_k\\Vert_F^2.$$\n",
    "Now we let $ \\hat s_k = W^{-\\frac 12} s_k$. Then by $Bs_k = W^{-1}s_k$ we learn that \n",
    "$\\hat B \\hat s_k = \\hat s_k$.\n",
    "Let the standardlized orthogonal complement of $\\hat s_k$ be $Q\\in \\mathbb R^{n\\times (n-1)}$. Then $(\\hat B- I)\\hat s_k =0\\Rightarrow B-I= QPQ^T $ where $P\\in \\mathbb R^{(n-1)\\times (n-1)}$. Then, \n",
    "$$ \\Vert \\hat B - \\hat B_k\\Vert_F^2=\\Vert QPQ^T +I- \\hat B_k\\Vert_F^2=\\Vert QPQ^T  + A\\Vert_F^2$$\n",
    "where $A = I-\\hat B_k$. \n",
    "\n",
    "Note that the we can rewrite  bilinear problem into an OLS (ordinary least square),\n",
    "$$ \\Vert {\\rm vec}(QPQ^T+A)\\Vert_F^2 =  \\Vert\n",
    "{\\rm vec}(QPQ^T)+{\\rm vec}(A)\\Vert_F^2=\\Vert\n",
    "(Q\\otimes Q){\\rm vec}(P)+{\\rm vec}(A)\\Vert_F^2.\n",
    "$$\n",
    "So the minimizer $P$ has the property that \n",
    "$$(Q\\otimes Q)^T(Q\\otimes Q){\\rm vec}(P) +(Q\\otimes Q)^T {\\rm vec}(A) = 0.$$\n",
    "Observe that \n",
    "$$(Q\\otimes Q)^T(Q\\otimes Q)=(Q^T\\otimes Q^T)(Q\\otimes Q) = (Q^TQ\\otimes Q^TQ) = I,$$\n",
    "so\n",
    "$$ {\\rm vec}(P) =-(Q\\otimes Q)^T  {\\rm vec}(A)=-(Q^T\\otimes Q^T)  {\\rm vec}(A)=-{\\rm vec}(Q^TAQ) $$\n",
    "Plug in $A =I -\\hat  B_k$ and $B = I+QPQ^T$, we have \n",
    "$$B = I - QQ^T(I -\\hat  B_k)QQ^T = I - QQ^T + QQ^T\\hat B_kQQ^T.$$\n",
    "Lastly, we recall that $QQ^T + \\frac{\\hat s_k\\hat s_k^T}{\\Vert \\hat s_k\\Vert^2} = I$, it yields\n",
    "$$\\begin{aligned}B &= \\frac{\\hat s_k\\hat s_k^T}{\\Vert \\hat s_k\\Vert^2} +(I -\\frac{\\hat s_k\\hat s_k^T}{\\Vert \\hat s_k\\Vert^2})\\hat  B_k(I -\\frac{\\hat s_k\\hat s_k^T}{\\Vert \\hat s_k\\Vert^2}) \\\\&\n",
    "= \\frac{\\hat s_k\\hat s_k^T}{\\Vert \\hat s_k\\Vert^2} +(I -\\frac{\\hat s_k\\hat s_k^T}{\\Vert \\hat s_k\\Vert^2})W^\\frac 12 B_kW^\\frac 12(I -\\frac{\\hat s_k\\hat s_k^T}{\\Vert \\hat s_k\\Vert^2}) \\\\\n",
    "&= \\frac{y_k y_k^T}{y_k^Ts_k}+(I -\\frac{y_ks_k^T}{y_k^Ts_k})B_k(I -\\frac{s_ky_k^T}{y_k^Ts_k})\\end{aligned} $$\n",
    "Here we have use the fact that\n",
    "$\\Vert \\hat s_k\\Vert^2 = s_k^TW^{-\\frac 12}W^{-\\frac 12}s_k = y_k^Ts_k$ and  $\\hat s_k\\hat s_k^TW^{\\frac 12} = W^{-\\frac 12}s_k s_k^T =y_ks_k^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.41419904346273\n",
      "[[ 1.80411242e-16]\n",
      " [ 5.55111512e-17]\n",
      " [-7.28583860e-17]\n",
      " [ 3.46944695e-18]\n",
      " [ 1.87350135e-16]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "n = 5 \n",
    "Bk = np.random.randn(n,n)\n",
    "Bk = (Bk + Bk.T) * .5  \n",
    "W = np.random.randn(n,n)\n",
    "Wroot = W @ W.T \n",
    "W = Wroot @ Wroot\n",
    "Q = np.linalg.qr(np.random.randn(n,n))[0]\n",
    "Q , s = Q[:,:-1], Q[:,-1:]\n",
    "y = np.linalg.solve(W, s) \n",
    "invW = np.linalg.inv(W)\n",
    "f = lambda x: np.linalg.norm(Wroot @ (x - Bk)@ Wroot)\n",
    "\n",
    "B = (np.eye(n) - y @ s.T / (y.T @ s)) @ Bk @ (np.eye(n) - s @ y.T / (y.T @ s)) + y @ y.T / (s.T @ y)\n",
    "print(f(B))\n",
    "print(B @ s - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:19<00:00, 782.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.75312404, -0.85536334, -0.57761351,  0.00325915, -1.1953545 ],\n",
       "       [-0.85536334,  0.71285175, -0.28895747, -0.94136126,  0.51795151],\n",
       "       [-0.57761351, -0.28895747, -1.49348758, -1.58969607, -0.36784199],\n",
       "       [ 0.00325915, -0.94136126, -1.58969607, -0.61380877, -1.03107502],\n",
       "       [-1.1953545 ,  0.51795151, -0.36784199, -1.03107502,  0.39440174]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch     \n",
    "torch.manual_seed(314159)\n",
    "Wroot_ = torch.tensor(Wroot,dtype=torch.float64) \n",
    "x = torch.randn((n,n),requires_grad = True,dtype=torch.float64) \n",
    "Bk_ = torch.tensor(Bk, dtype=torch.float64)\n",
    "s_ = torch.tensor(s, dtype=torch.float64)\n",
    "y_ = torch.tensor(y, dtype=torch.float64)\n",
    "opt = torch.optim.Adam([x])\n",
    "losses = []\n",
    "from tqdm import tqdm \n",
    "for i in tqdm(range(15000)):\n",
    "    z = (x + torch.transpose(x,1,0)) * .5\n",
    "    l = torch.norm(Wroot_ @ (z - Bk_) @ Wroot_) * .002\n",
    "    l +=  torch.norm(z @ s_ - y_)\n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    losses.append(l.item())\n",
    "    opt.step()\n",
    "z = (x + torch.transpose(x,1,0)) * .5\n",
    "x2 = z.detach().numpy() \n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.88024551187263\n",
      "[[-3.49189956e-05]\n",
      " [ 1.45350404e-04]\n",
      " [-5.99087877e-05]\n",
      " [ 1.88689245e-04]\n",
      " [-3.62102133e-04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15e5ed280a0>]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "print(f(x2))\n",
    "print(x2 @ s - y)\n",
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Wolfe Condition\n",
    "\n",
    "If there exists $0<c_1<c_2<1$ such that\n",
    "$$\\begin{aligned} f(x_k+t_kp_k)&\\leqslant f(x_k) + c_1t_k \\nabla f(x_k)^Tp_k\n",
    "\\\\ \\nabla f(x_k+t_kp_k)^Tp_k &\\geqslant c_2\\nabla f(x_k)^Tp_k,\\end{aligned}$$\n",
    "in the iteration strategy $x_{k+1} = x_k +t_kp_k$,\n",
    "we call it satisfies the Wolfe condition.\n",
    "\n",
    "The first inequality is the same as line search.\n",
    "\n",
    "### Zoutendijk's Theorem\n",
    "\n",
    "Suppose $p_k$ satisfies the Wolfe condition, $f$ is bounded below and $f$ has Lipschitz continuous gradient $\\nabla f$ over the domain (a level set) $\\{x:\\ f(x)\\leqslant f(x_0)\\}$, i.e.\n",
    "$$\\Vert \\nabla f(x) - \\nabla f(y)\\Vert \\leqslant L\\Vert x - y\\Vert.$$\n",
    "Then, \n",
    "$$\\sum_{k=0}^\\infty \\cos^2\\theta_k \\Vert \\nabla f(x_k)\\Vert^2 \\leqslant \\frac{L}{c_1(1-c_2)}\\left(f(0) - f_*\\right) < + \\infty$$\n",
    "where $\\cos\\theta_k = -\\frac{\\nabla f(x_k)^Tp_k}{\\Vert \\nabla f(x_k)\\Vert \\Vert p_k\\Vert}$.\n",
    "\n",
    "Proof: Recall the Wolfe condition given by \n",
    "\n",
    "$$\\begin{aligned} f(x_k+t_kp_k)&\\leqslant f(x_k) + c_1t_k \\nabla f(x_k)^Tp_k\n",
    "\\\\ \\nabla f(x_k+t_kp_k)^Tp_k &\\geqslant c_2\\nabla f(x_k)^Tp_k.\\end{aligned}$$\n",
    "On the one hand we derive $\\left(\\nabla f(x_k+t_kp_k)^T - \\nabla f(x_k)^T\\right)p_k\\geqslant (c_2 - 1)\\nabla f(x_k)^T p_k$. On the other by Lipschitz continuity we yied\n",
    "$\\left(\\nabla f(x_k+t_kp_k)^T - \\nabla f(x_k)^T\\right)p_k\\leqslant Lt_k\\Vert p_k\\Vert^2$. Hence we have that \n",
    "$$t_k \\geqslant \\frac{(1 - c_2)\\cdot ( - \\nabla f(x_k)^Tp_k)}{L \\Vert p_k\\Vert^2}$$\n",
    "and thus \n",
    "$$f(x_{k+1}) - f(x_k)\\leqslant - c_1t_k (  - \\nabla f(x_k)^Tp_k) \\leqslant \n",
    "-\\frac{c_1(1 -c_2)\\Vert f(x_k)^Tp_k\\Vert^2}{L\\Vert p_k\\Vert^2}.$$\n",
    "Sum it up to obtain \n",
    "$$f(x_*) - f(0)\\leqslant \\sum_{k=0}^\\infty -\\frac{c_1(1-c_2)}{L}\\cdot \\frac{(f(x_k)^Tp_k)^2}{\\Vert p_k\\Vert^2}\n",
    " = -\\frac{c_1(1-c_2)}{L}\\sum_{k=0}^\\infty \\cos^2\\theta_k \\Vert \\nabla f(x_k)\\Vert^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS\n",
    "\n",
    "BFGS (Broyden, Fletcher, Goldfarb, Shanno) is a modification of DFP where we directly update on $B_k^{-1}$ rather $B_k$. It has superceded DFP by recent experience.\n",
    "The choice of $B_{k+1}^{-1}$ is solved by \n",
    "$$\\min\\{\\Vert B^{-1}- B_k^{-1}\\Vert_{F,W}:\\quad B ^{-1}= B^{-T}\\quad s_k = B^{-1}y_k\\},$$\n",
    "solution of which is given by \n",
    "$$B_{k+1}^{-1} = \\left(I - \\frac{s_ky_k^T}{y_k^Ts_k}\\right)B_k^{-1}\\left(I - \\frac{y_ks_k^T}{y_k^Ts_k}\\right)+\\frac{s_ks_k^T}{y_k^Ts_k}.$$\n",
    "\n",
    "#### BFGS Convergence\n",
    "\n",
    "Suppose the level set $\\{x:\\ f(x)\\leqslant f(x_0)\\}$ is convex and $f$ is twice continuously differentiable over it. Also,  the Hessian of $f$ is bounded by the inequality $MI\\succeq \\nabla^2 f(x)\\succeq mI$. The initial $B_0$ is positive definite. Then BFGS converges.\n",
    "\n",
    "\n",
    "#### L-BFGS\n",
    "\n",
    "L-BGFS is limited memory BFGS where we do not store the large matrix $B_k^{-1}$. In each step we update from recent $m$ steps."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
