{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Proximal Gradient Method\n",
    "\n",
    "In the last section we have introduced the subgradient method, which has a slow convergence. Now we introduce the proximal gradient method, a better method for solving \n",
    "$$\\min \\left\\{g(x)+h(x)\\right\\}$$\n",
    "where $g$ is differentiable over $\\mathbb R^n$ while $h$ is convex but not necessarily differentiable. We shall see how to solve it when $h(x)$ is somehow special.\n",
    "\n",
    "\n",
    "### Proximal Gradient Method\n",
    "\n",
    "In each step we iterate by\n",
    "$$x_{k+1} = \\argmin_x\\left\\{\\nabla g(x_k)^T(x - x_k) + \\frac{1}{2t_k}\\Vert x - x_k\\Vert^2 + h(x)\\right\\}.$$\n",
    "\n",
    "### Special Cases\n",
    "\n",
    "#### Newton's Method\n",
    "\n",
    "When $h(x)\\equiv 0$, then it degenerates to the Newton's method. \n",
    "\n",
    "Proof:\n",
    "$$\\nabla g(x_k)^T(x - x_k) + \\frac{1}{2t_k}\\Vert x - x_k\\Vert^2 \n",
    "= \\frac{1}{2t_k} \\Vert t_k\\nabla g(x_k)  +( x - x_k)\\Vert^2 - \\frac{t_k}{2}\\Vert g(x_k)\\Vert^2\n",
    "$$\n",
    "So we choose $x_{k+1} = x_k -  t_k\\nabla g(x_k)$, precisely the iteration in Newton's method.\n",
    "\n",
    "#### Projected Subgradient Method\n",
    "\n",
    "When $C$ is closed convex and $h(x)  = I_C(x) =\\left\\{\\begin{array}{ll} 0 & x\\in C\\\\ +\\infty & x \\notin C\\end{array}\\right.$ is the indicator, then it degenerates to the projected subgradient method.\n",
    "\n",
    "Proof: Since $h(x) = +\\infty$ as long as $x\\notin C$, it suffices to consider the cases where $x\\in C$, which is solving for\n",
    "$$\\argmin_{x\\in C}\\{\\frac{1}{2t_k} \\Vert t_k\\nabla g(x_k)  +( x - x_k)\\Vert^2 - \\frac{t_k}{2}\\Vert g(x_k)\\Vert^2\\}.$$\n",
    "And it is clear that the minimizer should be the projection, i.e.\n",
    "$$x_{k+1} = \\prod_C (x_k - t_k \\nabla g(x_k)).$$\n",
    "\n",
    "\n",
    "### Proximal Mapping \n",
    "\n",
    "As claimed above, the key is to solve the minimization problem\n",
    "$$\\argmin_x\\left\\{\\frac12 \\Vert t_k\\nabla g(x_k)  +( x - x_k)\\Vert^2 +t_k h(x)\\right\\}. $$\n",
    "If we use the notation\n",
    "$${\\rm prox}_f(x_0)= \\argmin_x\\{f(x) +\\frac 12 \\Vert x -x_0\\Vert^2  \\},$$\n",
    "then the problem is equivalent to finding ${\\rm prox}_{t_kh}(x_k)$. The notation is called the proximal mapping. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
