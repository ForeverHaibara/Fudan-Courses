{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "1. Ruszczy≈Ñski, Andrzej. Nonlinear Optimization. Princeton University Press, Princeton, New Jersey, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Subgradient Method\n",
    "\n",
    "In this chapter we develop optimization startegies dealing with non-differentiable functions.  In the discussions below we would assume that $f$ is <font color=red> convex</font>.\n",
    "\n",
    "### Subgradient\n",
    "\n",
    "Recall that for a differentiable convex function $f$, the gradient $\\nabla f$ has the property that \n",
    "$$f(y)\\geqslant f(x) + \\nabla f(x)^T(y - x).$$\n",
    "\n",
    "In a same manner, we define the **subgradient** $g$ of a non-differentiable but convex function $f$ if $g$ satisfies that \n",
    "$$f(y)\\geqslant f(x)+g(x)^T(y-x).$$\n",
    "It is also a supporting hyperplane of the epigraph at $x$. Hence the subgradient (for convex functions) must exist.\n",
    "\n",
    "### Subdifferential\n",
    "\n",
    "The subdifferential of $f$ at $x$ is the set of all subgradients:\n",
    "$$\\partial f(x) = \\{g:\\ g^T(y - x)\\leqslant f(y) - f(x)\\quad  \\forall y\\in {\\rm dom}(f )\\}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Rules\n",
    "\n",
    "\n",
    "### Calculus\n",
    "\n",
    "The proof to the basic properties below can be found in [1]. If $f$ is convex and has nonempty and open domain, then\n",
    "\n",
    "1. $\\partial (\\alpha f) = \\alpha \\partial f$ for $\\alpha>0$. \n",
    "2. $\\partial (f_1 + f_2) = \\partial f_1 + \\partial f_2$. \n",
    "3. If $g(x) = f(Ax+b)$ is an affine transformation, then $$\\partial g = A^T\\partial f(Ax+b).$$\n",
    "4. If $f(x) = \\max_{1\\leqslant i\\leqslant n}f_i(x)$ is a pointwise maximum, then\n",
    "$$\\partial f(x) = {\\rm Conv\\ Hull\\ }\\bigcup_{ f_i(x)= f(x)}  \\partial f_i.$$\n",
    "\n",
    "### Optimality Conditions\n",
    " \n",
    "A point $x$ minimizes $f(x)$ if and only if $0\\in \\partial f(x)$. The proof is trivial.\n",
    "\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Maximum of Linear\n",
    "\n",
    "Let $f(x) = \\max_{1\\leqslant i\\leqslant m}a_i^Tx+b_i$, characterize the minimizer of $f$.\n",
    "\n",
    "Solution: It is clear that $f$ is convex. So for arbitrary $x$, \n",
    "$$\\partial f(x) = {\\rm Conv\\ Hull\\ }\\bigcup_{ f_i(x)= f(x)}  \\{a_i\\}.$$\n",
    "Assume $0\\in \\partial f(x)$, then $0$ lies in the convex hull of the active sets means that it is the convex combination of the active $\\{a_i\\}$. Thus there exists $\\lambda_i\\geqslant 0$ such that\n",
    "$$0 = \\sum_{i=1}^m \\lambda_i a_i\\quad{\\rm and}\\quad \\lambda_i = 0 {\\ \\rm if\\ }f_i(x)\\neq f(x)\\quad{\\rm and}\\quad \\sum_{i=1}^m\\lambda_i = 1.$$\n",
    "\n",
    "It is equivalent to KKT (after introducing the slackness variable) and the second constraint corresponds to the complementary slackness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection\n",
    "\n",
    "### Indicator\n",
    "\n",
    "Given a <font color=red>convex</font> set $C$, define an indicator $I_C(x)$ by $I_C(x) = \\left\\{\\begin{array}{ll}0 & x\\in C\\\\ +\\infty & x\\notin C\\end{array}\\right.$, then for $x\\in C$, $\\partial I_C(x) = N_C(x)$ is the normal cone where $$N_C(x) = \\{g:\\ g^T(y - x)\\leqslant 0\\quad \\forall y\\in C\\}.$$\n",
    "\n",
    "Here the convexity of $C$ guarantees the convexity of $I_C(x)$. In the following sections, $C$ is always convex.\n",
    "\n",
    "Proof: As we require $f(y)\\geqslant f(x) + g^T(y - x) = g^T(y-x)$. When $y\\in C$, it turns to $0\\geqslant g^T(y-x)$.\n",
    "\n",
    "### Minimum on a Set\n",
    "Suppose $f$ is convex over a domain $C$, then $\\min_{x\\in C}f(x)$ = $\\min_x f(x) + I_C(x)$.\n",
    "\n",
    "\n",
    "### Projection\n",
    "\n",
    "Let $\\prod_C(x)$ be the projection of $x$ on a set $C$, a point $y$ in $C$ that has minimum distance to $x$,\n",
    "$$y = \\prod_C(x) = {\\rm argmin}_{y\\in C}\\frac12 \\Vert y - x\\Vert_2^2\n",
    "= {\\rm argmin}_{y}\\frac12 \\Vert y - x\\Vert_2^2 + I_C(y).$$\n",
    "Hence it has subdifferential over $C$, \n",
    "$$\\{y - x+g:\\quad g^T(u-y)\\leqslant 0\\quad\\forall u\\in C\\}.$$\n",
    "Let $0$ fall in the set to reach a minimum. Now $g = x-y$ and \n",
    "$$(x - y)^T(u - y)\\leqslant 0 \\quad \\forall u \\in C.$$ \n",
    "\n",
    "Also, if we let $f(x) = d_C(x) = \\min_{y\\in C}\\Vert y - x\\Vert = \\Vert \\prod_C(x) - x\\Vert$ where $C$ is convex, then $\\frac{x - \\prod_C(x)}{\\Vert x - \\prod_C(x)\\Vert}\\in \\partial f$ at $x$.\n",
    "\n",
    "Proof: Follow the definition of a subgradient, it suffices to prove that \n",
    "$$\\frac{(x - \\prod(x))^T}{\\Vert x - \\prod(x)\\Vert}(y - x)\\leqslant \\Vert \\prod (y) - y\\Vert -\\Vert x - \\prod(x)\\Vert.$$\n",
    "This is because \n",
    "$$\\begin{aligned}(x - \\prod(x))^T(y - x) + \\Vert x - \\prod(x)\\Vert^2&\n",
    "=(x - \\prod(x))^T(y - \\prod(x))\\\\ \n",
    "&= (x - \\prod(x))^T(\\prod(y) - \\prod(x))+(x - \\prod(x))^T(y - \\prod(y) )\\\\ \n",
    "&\\leqslant (x - \\prod(x))^T(y - \\prod(y) )\\\\ \n",
    "&\\leqslant \\Vert x - \\prod(x)\\Vert \\Vert y - \\prod(y)\\Vert.\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Law of Cosines\n",
    "\n",
    "For $x\\in C$ and arbitrary $y$, we have \n",
    "$$\\Vert \\prod_C(y) - x\\Vert \\leqslant \\Vert y - x\\Vert.$$\n",
    "Proof: Since we have seen that $\\left(y - \\prod_C(y)\\right)^T\\left(x - \\prod_X(y)\\right)\\leqslant 0$, \n",
    "$$\\Vert y - x\\Vert^2 = \\Vert y - \\prod_C(y)\\Vert^2 + \\left(y - \\prod_C(y)\\right)^T\\left(\\prod_C(y) - x\\right)\n",
    "+\\Vert  \\prod_C(y) - x\\Vert^2\\geqslant \\Vert \\prod_C(y) - x\\Vert^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projected Subgradient Method\n",
    "\n",
    "Consider the optimization problem $\\min_{x\\in C}f(x)$ where $f$ is convex but not necessarily differentiable and $C$ is a closed convex set and $C$ is not a single point.\n",
    "We solve it by iterating as follows:\n",
    "$$y_{k+1} = x_k - \\alpha_k g_k\\quad\\quad x_{k+1} = \\prod_C(y_{k+1})$$\n",
    "where $g_k$ is an arbitrary subgradient from $\\partial f(x_k)$ while $\\prod_C(y_{k+1})$ stands for the projection. The parameter $\\alpha_k$ determines the step size, which can be a constant or diminishing to zero.\n",
    "\n",
    "Further, if $C = \\mathbb R^n$ is the whole real plane, then we also call it the subgradient method (where the projection is unnecessary).\n",
    "\n",
    "<br> \n",
    "\n",
    "<br>\n",
    "\n",
    "In the following part, we assume that the target function $f$ is convex and  has finite infimum $f_*$ and the subgradient is bounded,\n",
    "$$\\Vert g \\Vert\\leqslant G\\quad\\quad\\forall g\\in \\partial f.$$\n",
    "We further introduce the notation $R = \\Vert x_1  - x_*\\Vert$.\n",
    "\n",
    "### Lipschitz Continuity\n",
    "\n",
    "In fact, for a convex $f$, we can show that the bounded-subgradient property $\\Vert g \\Vert\\leqslant G\\ \\forall g\\in \\partial f$ required above is equivalent to \n",
    "the Lipschitz continuity $| f(y) - f(x)|\\leqslant G\\Vert y-x \\Vert $.\n",
    "\n",
    "Proof: $\\Rightarrow$: For arbitrary $y$ and $x$, we may assume $f(y) \\leqslant f(x)$. By the convexity of $f$ we learn that $\\partial f_x$ is not empty, we pick arbitrary $g_x\\in \\partial f_x$ so that \n",
    "$$ 0\\geqslant f(y) - f(x)\\geqslant g_x^T(y - x) \\geqslant -G\\Vert y - x\\Vert.$$\n",
    "\n",
    "$\\Leftarrow$: For arbitrary $g\\in \\partial f$, we have \n",
    "$$g^T(y - x)\\leqslant f(y) - f(x) \\leqslant G\\Vert y - x\\Vert.$$\n",
    "Find $y$ such that $g$ and $y - x$ are parallel and thus $\\Vert g\\Vert\\leqslant G$.\n",
    "\n",
    "### Convergence\n",
    "\n",
    "In the projected subgradient method, we have that\n",
    "$$\\Vert x_{i+1} - x_*\\Vert^2 \\leqslant \\Vert x_i - x_*\\Vert^2 - 2\\alpha_i \\left(f(x_i) - f_*\\right) + \\alpha_i^2 \\Vert g_i\\Vert^2.$$\n",
    "\n",
    "If we denote $f_{bs} = \\min_i f(x_i)$, the best solution in our iteration (The projected subgradient method does not always provide monotonically decreasing solution!), then \n",
    "$$f_{bs} - f_*\\leqslant \\frac{\\Vert x_1 -x_*\\Vert^2+\\sum_{i=1}^k \\alpha_i^2 \\Vert g_i\\Vert^2}{2\\sum_{i=1}^k \\alpha_i}\n",
    "\\leqslant \\frac{R^2+G^2\\sum_{i=1}^k \\alpha_i^2 }{2\\sum_{i=1}^k \\alpha_i}.$$\n",
    "\n",
    "Proof: By the iteration $x_{k+1} = \\prod_C(x_k - \\alpha_k g_k)$ and the law of cosines, \n",
    "$$\\begin{aligned}\\Vert x_{i+1} - x_*\\Vert^2 &= \\Vert \\prod_C(x_i - \\alpha_i g_i) - x_*\\Vert^2\n",
    "\\leqslant \\Vert \\ x_i - \\alpha_i g_i  - x_*\\Vert ^2\\\\\n",
    "&= \\Vert x_i-x_*\\Vert^2 +2\\alpha_i g_i^T(x_* - x_i  ) + \\alpha_i^2\\Vert g_i\\Vert^2 \\\\ \n",
    "&\\leqslant \\Vert x_i-x_*\\Vert^2 +2\\alpha_i  (f(x_* )- f(x_i)  ) + \\alpha_i^2\\Vert g_i\\Vert^2.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Sum it up from $i = 1$ to $k$, and replace $f(x_i)$ with $\\geqslant f_{bs}$, we obtain\n",
    "$$0\\leqslant \\Vert x_{k+1}^2 - x_*\\Vert^2\\leqslant \\Vert x_1 - x_*\\Vert^2 - 2\\sum_{i=1}^k \\alpha_i^2 \\left(f_{bs} - f_*\\right) \n",
    "+ \\sum_{i=1}^k \\alpha_i^2 \\Vert g_i\\Vert^2. $$\n",
    "Sorting the inequality leads to the desired result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Size\n",
    "\n",
    "We further analyze the convergence with different step size strategies. \n",
    "\n",
    "#### Constant Step Size\n",
    "\n",
    "When $\\alpha_i$ is constant $\\alpha$, then \n",
    "$$f_{bs} - f_* \\leqslant \\frac{R^2 +  k\\alpha^2 G^2}{2k\\alpha}\\rightarrow \\frac 12\\alpha G^2.$$\n",
    "Hence our convergence bound does not guarantee reaching the optimal solution. The smaller $\\alpha$ is, the better bound it yields.\n",
    "\n",
    "#### Constant Step Length\n",
    "\n",
    "The constant step length suggests we select $\\alpha$ such that $\\alpha_k \\Vert g_k \\Vert = \\gamma$ is a constant. In this case, \n",
    "$$f_{bs} - f_*\\leqslant \\frac{R^2+k\\gamma^2}{2\\gamma\\sum_{i=1}^k \\frac{1}{\\Vert g_i\\Vert}}\\leqslant \n",
    "G\\frac{R^2+k\\gamma^2}{2k\\gamma }\\rightarrow \\frac 12 G\\gamma . $$\n",
    "\n",
    "It faces similar problem with the constant-step-size startegy. \n",
    "\n",
    "#### Diminishing Step Size\n",
    "\n",
    "If we choose $\\alpha_k\\rightarrow 0$ while $\\sum_{i=1}^{\\infty}\\alpha_i = +\\infty$, then it guarantees the optimal solution.\n",
    "\n",
    "Proof: When $\\sum_{i=1}^{k}\\alpha_i^2$ is bounded, it is clear that \n",
    "$\\frac{R^2+G^2\\sum_{i=1}^k \\alpha_i^2}{2\\sum_{i=1}^k \\alpha_i}\\rightarrow 0$ and $f_{bs}$ converges to $f_*$. When it is not bounded, we can apply the **Stolz** theorem that \n",
    "$$\\lim_{k\\rightarrow \\infty}\\frac{R^2+G^2\\sum_{i=1}^k \\alpha_i^2}{2\\sum_{i=1}^k \\alpha_i}\n",
    "= \\lim_{k\\rightarrow \\infty}\\frac{G^2\\alpha_i^2}{2\\alpha_i} = 0.$$\n",
    "\n",
    "#### Polyak's Step Size\n",
    "\n",
    "When $f_*$ is known (e.g. $f_* = 0$ for some problems), we can choose \n",
    "$$\\alpha_k = \\frac{f_k - f_*}{\\Vert g_k\\Vert^2}$$\n",
    "and the previous inequality $\\Vert x_{i+1} - x_*\\Vert^2 \\leqslant \\Vert x_i - x_*\\Vert^2 - 2\\alpha_i \\left(f(x_i) - f_*\\right) + \\alpha_i^2 \\Vert g_i\\Vert^2$ now reads\n",
    "$$\\Vert x_{i+1} - x_*\\Vert^2 \\leqslant \\Vert x_i - x_*\\Vert^2 -\\frac{\\left(f(x_i) - f_*\\right)^2}{\\Vert g_i\\Vert^2}\n",
    " \\leqslant R^2 -\\frac{\\left(f_{bs} - f_*\\right)^2}{G^2}. $$\n",
    " Hence, the convergence is guaranteed by\n",
    " $$f_{bs} - f_*\\leqslant \\frac{GR}{\\sqrt k}.$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
