{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Hypothesis Testing\n",
    "\n",
    "Assume we have a hypothesis $H_0$. We call it a null hypothesis. We check that whether or not the observed data \n",
    "reject the hypothesis. This is the hypothesis testing. \n",
    "\n",
    "**Not rejecting the hypothesis does not mean that the data accept the hypothesis**. It is simply that the data cannot provide significant counterevidence.\n",
    "\n",
    "### P-Value\n",
    "\n",
    "P-value is the probability that we observe the very data or more extreme ones under $H_0$.\n",
    "\n",
    "Example: Consider a Bernoulli trial with probability $q$. Now we have a sample of size $20$ where there are $17$ ones and $3$ zeros. The hypotheses we make are $H_0:\\ q=0.5$ and $H_1:\\ q\\neq 0.5$. Then, under $H_0$ we shall expect that a trial of size $20$ should result in approximately $10$ ones. Yet there are $17$ ones, and more extreme cases could be $0,1,2,3,17,18,19,20$ ones. Hence there is probability \n",
    "$$\\mathbb P(X\\leqslant 3\\ {\\rm or  }\\ X\\geqslant 17) \\approx 0.0026$$\n",
    "leading to such extreme results. \n",
    "\n",
    "### $\\alpha$-Significance Level\n",
    "\n",
    "We set an $\\alpha$-significance level for a test on $H_0$ and $H_1$. When the P-value is very extreme (very small) and is smaller than $\\alpha$, then we reject the null hypothesis $H_0$. For example, we set  $\\alpha = 0.05$  in the example above and by $0.05>0.0026$ we conclude to reject $H_0$. In other words, the data observed are too abnormal for $q = 0.5$ so we reject the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.00257682800292969"
      ],
      "text/latex": [
       "0.00257682800292969"
      ],
      "text/markdown": [
       "0.00257682800292969"
      ],
      "text/plain": [
       "[1] 0.002576828"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(1 - pbinom(16, 20, 0.5))*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Errors\n",
    "\n",
    "There are four possible states in hypothesis testing as listed below. \n",
    "| Cases|Reject $H_0$|Not reject $H_0$|\n",
    "|--|--|--|\n",
    "|$H_0$ is true|Type I Error|Correct|\n",
    "|$H_1$ is true|Correct|Type II Error|\n",
    "\n",
    "Apparently, $\\alpha {\\rm\\ confidence\\ level}\\geqslant \\mathbb P({\\rm Type\\ I\\ Error})$.\n",
    "\n",
    "Define the power function of a test by \n",
    "$$\\beta(\\theta) = 1 - \\mathbb P_\\theta({\\rm Type \\ II\\ Error}),$$\n",
    "the probability that $H_0$ successfully get rejected when the parameter is $\\theta$ $(\\theta\\in H_1)$. When fixing $\\alpha$, one would choose a test with $\\alpha$-significance level that maximizes the power $\\beta(\\theta)$ in order to minimize the probability of type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wald Test\n",
    "\n",
    "Assume we have now estimated some parameter by $\\theta$. Then here comes some data and we compute $\\hat \\theta$ and $\\widehat{\\rm SE}(\\hat \\theta)$. If $\\hat \\theta$ is  an asympototically normal estimator $\\hat \\theta$, i.e. \n",
    "$$(\\hat\\theta - \\theta)/ \\widehat{\\rm SE}(\\hat \\theta)\\stackrel{d}{\\rightarrow} N(0,1),$$\n",
    "\n",
    "then when $n$ is large and provided the estimator $\\hat\\theta$ is good,  the statistics above should approximate $0$. If the error we observe is large, then we might consider reject the hypothesis.\n",
    "\n",
    "Let $T= (\\hat\\theta - \\theta)/ \\widehat{\\rm SE}(\\theta)$ be the statistics at the significance level $\\alpha$. \n",
    "\n",
    "* If the alternative hypothesis is $\\theta\\neq \\theta_0$, then we reject $H_0$  if $|T|>\\Phi^{-1}(1-\\frac \\alpha 2)$. \n",
    "* When the alternative hypothesis is $\\theta< \\theta_0$, then we reject $H_0$ if $T<\\Phi^{-1}(\\alpha )$. \n",
    "* When the \n",
    "alternative is $\\theta>\\theta_0$, then we reject $H_0$ if $T>\\Phi^{-1}(1-\\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square Tests\n",
    "\n",
    "### Chi-square Distribution\n",
    "Recall the difinition of chi-square distribution: Let $X_1,\\dotsc,X_n\\sim N(0,1)$ are independent standard normal, then \n",
    "$$Z = X_1^2+\\dotsc+X_n^2$$\n",
    "has chi-square distribution with $k$ degrees of freedom. We denote it by $\\chi_k^2$.\n",
    "\n",
    "$$\\mathbb E(\\chi_k^2) = k\\quad {\\rm and}\\quad {\\rm Var}(\\chi_k^2) = 2k.$$\n",
    "\n",
    "It has density $f(x) = \\frac{1}{2^\\frac k2 \\Gamma(k/2)}x^{k/2 - 1}e^{-x/2}$.\n",
    "\n",
    "Proof: First we compute the CDF by high-dimensional spherical coordinates (https://zhuanlan.zhihu.com/p/128580414),\n",
    "\n",
    "$$\\begin{aligned}\\mathbb P(\\chi_k^2\\leqslant t)&=\n",
    "\\int\\dotsi\\int_{x_1^2+\\dotsc +x_k^2\\leqslant t}\\frac{1}{\\sqrt{2\\pi}^k}e^{-\\frac{x_1^2+\\dotsc+x_k^2}{2}}dx_k\\dotsm dx_1\\\\\n",
    "&=\\int_0^{2\\pi} \\int_0^\\pi \\dotsi\\int_0^{\\pi}\\int_0^{\\sqrt t}\n",
    "\\frac{1}{\\sqrt{2\\pi }^k }e^{-\\frac {r^2}{2}}r^{k-1}\\sin^{k-2}\\theta_1\\dotsm\\sin\\theta_{k-2}drd\\theta_1\\dotsm d\\theta_{k-2}d\\theta_{k-1}\\\\\n",
    "&= \\int_0^{2\\pi}d\\theta_{k-1}\\cdot\\prod_{j=2}^{k-1}\\int_0^{\\pi}\\sin^{j-1}\\theta_{k-j}d\\theta_{k-j} \\cdot \\int_0^{\\sqrt t}\\frac{1}{\\sqrt{2\\pi}^k}r^{k-1}e^{-\\frac{r^2}{2}}dr\\\\\n",
    "&=2\\pi \\cdot\\left(\\frac{\\pi}{2}\\right)^{[\\frac{k-2}{2}]}\\cdot 2^{k-2}\\cdot \\prod_{j=1}^{k-2}\\frac{(j-1)!!}{j!!}\\cdot (2\\pi)^{-\\frac k2}\\int_0^{\\sqrt t} r^{k-1}e^{-\\frac{r^2}{2}}dr\\\\\n",
    "&=\\left\\{\\begin{array}{ll}\\frac{1}{(k-2)!!}\\int_0^{\\sqrt t} r^{k-1}e^{-\\frac{r^2}{2}}dr & {\\rm for \\ even\\ }k\\\\\n",
    "\\sqrt{\\frac{2}{\\pi}}\\frac{1}{(k-2)!!}\\int_0^{\\sqrt t} r^{k-1}e^{-\\frac{r^2}{2}}dr & {\\rm for \\ odd\\ }k\\end{array}\\right.\\\\\n",
    "&=\\frac{1}{2^{\\frac k2-1}\\Gamma(k/2)}\\int_0^{\\sqrt t} r^{k-1}e^{-\\frac{r^2}{2}}dr.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Take the derivative with respect to $t$ yields the result. It has mean and variance given by \n",
    "$$\\begin{aligned}\\mathbb E(\\chi_k^2) &= k\\mathbb E(X_1^2)=k{\\rm Var}(X_1) = k\\\\\n",
    "{\\rm Var}(\\chi_k^2) &= \\mathbb E\\left(\\left(\\sum_{j=1}^k X_j^2\\right)^2\\right) - k^2\n",
    "=k\\mathbb E(X_1^4)+k(k-1)\\mathbb E(X_1^2) - k^2= 3k+k(k-1)-k^2 = 2k.\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Chi-square Test for Normal Variance \n",
    "\n",
    "If $X_1,\\dotsc,X_n$ are independent samples from a normal distribution and we want to test $H_0:\\ X_i\\sim N(\\mu,\\sigma^2)$ where we only care about $\\sigma^2$. Then we test the statistic \n",
    "$$T = \\sigma^{-2}\\sum_{i=1}^n (X_i - \\overline X)^2\\sim \\chi_{n-1}^2.$$\n",
    "\n",
    "When $T$ is far away from the mean $\\mathbb E(\\chi_{n-1}^2) = n-1$, we reject the null hypothesis $H_0$. More explicitly, we reject $T$ at the $\\alpha$-confidence level when $T\\notin (F^{-1}(\\frac \\alpha 2),F^{-1}(1 - \\frac \\alpha 2))$ where $F$ is the CDF of $\\chi_{n-1}^2$.\n",
    "\n",
    "This is also to say, \n",
    "$$\\left(\\frac{\\sum_{i=1}^n (X_i - \\overline X)^2}{F^{-1}(1 - \\frac\\alpha 2)},\n",
    "\\frac{\\sum_{i=1}^n (X_i - \\overline X)^2}{F^{-1}(\\frac\\alpha 2)}\\right)$$\n",
    "is an $1 - \\alpha$ confidence level for $\\sigma^2$.\n",
    "\n",
    "### Likelihood Ratio Test\n",
    "\n",
    "Assume $\\theta = [\\theta_1,\\dotsc,\\theta_n]^T\\in \\Theta$ are parameters and we want to test   whether $A\\theta =b$, where $A\\in \\mathbb R^{m\\times n}\\ (m\\leqslant n)$ is of full row rank.\n",
    "\n",
    "Set $\\Theta_0:\\{\\theta:\\ A\\theta =b\\}$ be the target subspace while $\\Theta$ be the whole space. \n",
    "\n",
    "Now given observations $X_1,\\dotsc,X_n$, we let the variable \n",
    "$$\\lambda = \\frac{\\sup_{\\theta\\in \\Theta} L(\\theta;x)}{\\sup_{\\theta\\in\\Theta_0} L(\\theta;x)}\n",
    " = \\frac{\\sup_{\\theta\\in \\Theta}f_\\theta (x)}{\\sup_{\\theta\\in\\Theta_0}f_\\theta (x)} $$\n",
    "be the ratio of two likelihood. Apparently $\\lambda\\geqslant 1$. Our test statistic is given by $2\\log \\lambda$, which has the property below stated by the Wilks' theorem\n",
    "$$2\\log \\lambda\\stackrel{d}{\\rightarrow} \\chi_m^2\\quad {\\rm under\\ }H_0.$$\n",
    "\n",
    "Since there are $m$ equations in the $\\Theta_0$ while there are $n$ variables, so $\\Theta_0$ has $n-m$ degrees of freedom. The degree of freedom of the chi-square equals to the difference of the degree of freedom (dimensionality) of $\\Theta$ and $\\Theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness-of-fit Test\n",
    "\n",
    "If we have data $X_1,\\dotsc,X_n$ categorized into $k$ classes, $Z_1,\\dotsc,Z_k$ where $Z_i\\in\\mathbb N$ is the number of $X_i$ (frequency) in the class. Now we fit it by some model $M(\\hat \\Theta)$, i.e. Poisson and we would like to test whether our model is reasonable. Then we first estimate the expectation of each frequency, $\\hat Z_1,\\dotsc,\\hat Z_k$. Then we test \n",
    "$$T = \\sum_{j=1}^n \\frac{(Z_j - \\hat Z_j)^2}{\\hat Z_j}\\stackrel{d}{\\rightarrow}\\chi_{k-1-d}^2$$\n",
    "where $d$ is the number of parameters in the model $M(\\hat \\Theta)$. We reject $H_0$ when $T$ is too large.\n",
    "\n",
    "Note that we require all $\\hat Z_j\\geqslant 5$ to avoid large errors. Small classes that have small frequencies should be merged into one greater class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation 60.84425 72.40466 43.08077 17.08871 6.581606 \n",
      "df = 3 \n",
      " t = 5.662527 \n",
      " p = 0.1292345"
     ]
    }
   ],
   "source": [
    "x = c(70, 57, 45, 21, 7)\n",
    "n = sum(x)\n",
    "lambda = sum(x * (0:(length(x) - 1))) / n\n",
    "p = dpois((0:(length(x) - 2)), lambda) # compute P(k=0,1,2,3) for Poisson\n",
    "p = c(p, 1 - sum(p))  # compute P(k>=4)      for Poisson\n",
    "cat('Expectation', p * n, '\\n')\n",
    "t = sum((x - p*n)^2 / (p*n))\n",
    "cat('df =' , length(x)-1-1, '\\n',\n",
    "     't =' , t, '\\n',\n",
    "     'p =' , 1 - pchisq(t, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square Tests for Contigency Tables\n",
    "\n",
    "Suppose $(X,Y)$ are two discrete random variables and we want to know whether they are independent. $X$ belongs to $r$ classes while $Y$ belongs to $c$ classes and $(X,Y)$ belongs to $r\\times c$ joint classes.\n",
    "\n",
    "Sum up the frequency in each row and each column as below.\n",
    "$$\n",
    "\\begin{array}{ll|llll|l}\n",
    "                     &   &  & &    Y      &         \\\\\n",
    "          &     & 1        & 2        & \\cdots & c             & {\\rm  sum        }        \\\\ \\hline\n",
    "  & 1                     & Z_{11}      & Z_{12}      & \\cdots & Z_{1c}      & Z_{1\\cdot}       \\\\\n",
    " & 2                     & Z_{21}      & Z_{22}      & \\cdots & Z_{2c}      & Z_{2\\cdot}       \\\\\n",
    "               X    & \\vdots              & \\vdots      & \\vdots      & \\ddots & \\vdots      & \\vdots           \\\\\n",
    " & r                     & Z_{r1}      & Z_{r2}      & \\cdots & Z_{rc}      & Z_{r\\cdot}       \\\\ \\hline\n",
    " & {\\rm sum}                 & Z_{\\cdot 1} & Z_{\\cdot 2} & \\cdots & Z_{\\cdot c} & Z_{\\cdot\\cdot}=N\n",
    "\\end{array}$$\n",
    "\n",
    "On the one hand, if we assume that $X,Y$ are independent, then \n",
    "$$\\mathbb P(X = i,\\ Y = j) = \\mathbb P(X = i)\\mathbb P(Y = j) = \\frac{Z_{i\\cdot}Z_{\\cdot j}}{N^2}$$\n",
    "and the corresponding entry has expectation\n",
    "$$ E_{ij} = N\\cdot \\mathbb P(X = i, y = j) =  \\frac{Z_{i\\cdot}Z_{\\cdot j}}{N}.$$\n",
    "\n",
    "On the other, we estimate $N\\cdot \\mathbb P(X = i,\\ Y = j) = Z_{ij}$. So, follow the idea in the goodness-of-fit test, we test the statistic\n",
    "$$T = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(Z_{ij} - E_{ij})^2}{E_{ij}}\\stackrel{d}{\\rightarrow}\\chi_{(r-1)(c-1)}^2.$$\n",
    "\n",
    "#### Degree of Freedom \n",
    "\n",
    "We can show the chi-square statistic has $(r-1)(c-1)$ degrees of freedom following the idea of  likelihood ratio test.\n",
    "Considering the whole space $\\Theta$ is $r\\times c$ parameters $Z_{ij}$ satisfying the constraint \n",
    "$\\sum_{i,j} Z_{ij} = N$, it has $(rc-1)$ degrees of freedom.\n",
    "\n",
    "Our target subspace $\\Theta_0$ is ${\\rm rank}([Z_{ij}])=1$ and $\\sum_{i,j}Z_{ij}=N$. It has $(r+c-1)-1=r+c-2$ degrees of freedom because $(r+c-1)$ entries are sufficient to  determine a rank-1 matrix.\n",
    "\n",
    "So the chi-square has $(rc-1)-(r+c-2)= (r-1)(c-1)$ degrees of freedom.\n",
    "\n",
    "#### Special Case \n",
    "\n",
    "\n",
    "When $r = c = 2$, and we have the following $2\\times 2$ table \n",
    "$$\\begin{array}{l|ll|l}\n",
    "    & Y   &     & {\\rm sum}     \\\\ \\hline\n",
    "X   & a   & b   & a+b     \\\\\n",
    "    & c   & d   & c+d     \\\\ \\hline\n",
    "{\\rm sum} & a+c & b+d & a+b+c+d\\end{array}$$\n",
    "And one can verify that the test statistic is \n",
    "$$T =K^2= \\frac{(a+b+c+d)(ad-bc)^2}{(a+b)(c+d)(a+c)(b+d)}\\sim \\chi_1^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [,1] [,2] [,3] [,4]\n",
      "[1,]  332  318   29   27\n",
      "[2,] 1360  104   35   18\n",
      "[1] 507.0797\n",
      "[1] 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\tPearson's Chi-squared test\n",
       "\n",
       "data:  x\n",
       "X-squared = 507.08, df = 3, p-value < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = matrix(c(332,1360,318,104,29,35,27,18), nrow = 2)\n",
    "print(x)\n",
    "s = 0\n",
    "for (i in (1:2)){\n",
    "    for (j in (1:4)){\n",
    "        eij = sum(x[i,1:4]) * sum(x[1:2,j]) / sum(x)\n",
    "        s = s + (x[i,j] - eij)^2 / eij\n",
    "    }\n",
    "}\n",
    "print(s)\n",
    "print(pchisq(s, 3))\n",
    "chisq.test(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T Tests\n",
    "\n",
    "### T Distribution\n",
    "\n",
    "T distribution, also known as the Student's t distribution, is defined as below. Let $X\\sim N(0,1)$ and $Z\\sim \\chi_k^2$ are independent, then \n",
    "$$T = \\frac{X}{\\sqrt{\\frac Zk}}$$\n",
    "has t-distribution with $k$ degrees of freedom. We denote it by $t_k$. The distribution is symmetric and it has zero mean as long as it has finite mean.\n",
    "\n",
    "It has heavy tail that $\\mathbb E(|T|^k) = \\infty$. Also, since $\\chi_k^2/k\\stackrel{\\mathbb P}{\\rightarrow} 1$ by LLN, we have $T_k \\stackrel{d}{\\rightarrow}N(0,1)$ by Slutsky's theorem.\n",
    "\n",
    "The density of $t_k$ is given by \n",
    "$f(x) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{k\\pi}\\Gamma(\\frac k2)}\\left(1 + \\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}$.\n",
    "\n",
    "Proof: We first derive the CDF by \n",
    "$$\\begin{aligned}\\mathbb P(T_k\\leqslant t)&= \n",
    "\\iint_{ x\\leqslant t\\sqrt{z/k}}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\cdot \n",
    "\\frac{1}{2^{\\frac k2}\\Gamma(k/2)}z^{\\frac k2 - 1} e^{-\\frac z2}dxdz\\\\&\n",
    "=\\int_{0}^{+\\infty}\\int_{-\\infty}^{t\\sqrt{z/k}}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\cdot \n",
    "\\frac{1}{2^{\\frac k2}\\Gamma(k/2)}z^{\\frac k2 - 1} e^{-\\frac z2}dxdz.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Take the derivative when $t>0$ and thus \n",
    "$$\\begin{aligned}f(t)&\n",
    "=\\int_0^{+\\infty}\\sqrt{\\frac zk}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^2z}{2k}}\\cdot \n",
    "\\frac{1}{2^{\\frac k2}\\Gamma(k/2)}z^{\\frac k2 - 1} e^{-\\frac z2}dz\n",
    "=\\frac{1}{2^{\\frac k2}\\sqrt{2k\\pi}\\Gamma(\\frac k2)}\\int_0^{+\\infty} \n",
    " z^{\\frac{k-1}{2}} e^{-\\frac 12\\left(\\frac{t^2}{k}+1\\right)z}dz\\\\\n",
    " &=\\frac{1}{2^{\\frac k2}\\sqrt{2k\\pi}\\Gamma(\\frac k2)}\\cdot  2^\\frac{k+1}{2}\\left(\\frac{t^2}{k}+1\\right)^{-\\frac{k+1}{2}} \\int_0^{+\\infty} \n",
    " u^{\\frac{k-1}{2}} e^{-u}du\\\\ &= \n",
    " \\frac{\\Gamma(\\frac {k+1}{2})}{\\sqrt{k\\pi}\\Gamma(\\frac k2)}\\left(\\frac{t^2}{k}+1\\right)^{-\\frac{k+1}{2}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### T-test for Normal Mean \n",
    "If $X_1,\\dotsc,X_n$ are independent samples from a normal distribution where the mean is unknown and we do not care about its variance, then we can test the estimate for mean $\\mu$ by assuming $H_0:\\ X_i\\sim N(\\mu,\\sigma^2)$ and testing\n",
    "$$T = \\frac{\\sqrt n (\\overline X - \\mu)}{\\widehat{\\rm SE}(\\overline X)}=\\frac{\\sqrt n (\\overline X - \\mu)}{\\sqrt{\\frac {1}{n-1}\\sum_{i=1}^n (X_i- \\overline X)^2}}\\sim t_{n-1}.$$\n",
    "When the statistics $T$ has large absolute value, then we tend to reject the null hypothesis $H_0$. \n",
    "Here we have used the fact that $\\sigma^{-2}\\sum_{i=1}^n (X_i - \\overline X)^2 \\sim \\chi_{n-1}^2$ and that $\\sigma^{-1}\\sqrt n(\\overline X - \\mu)\\sim N(0,1)$ and they are independent.\n",
    "\n",
    "We reject the null hypothesis $H_0$ when $T$ is far off the origin.\n",
    "\n",
    "### T-test for Two Samples \n",
    "\n",
    "Suppose we have two independent samples $X_1,\\dotsc,X_n$ and $Y_1,\\dotsc,Y_m$. We would test $H_0:\\ \\mu_X-\\mu_Y=\\Delta \\mu$. If we **assume the variance of the  two samples are equal**, then \n",
    "$\\sigma^{-1}\\sqrt{\\frac{nm}{n+m}}(\\overline X - \\overline Y - \\Delta \\mu)\\sim N(0,1)$ and \n",
    "$\\sigma^{-2}\\left( \\sum_{j=1}^n (X_j - \\overline X)^2 +  \\sum_{k=1}^m (Y_k - \\overline Y)^2\\right)\\sim \\chi_{m+n-2}^2$ and they are independent. So we test the statistic\n",
    "$$T = \\sqrt{\\frac{n+m-2}{\\frac{n+m}{nm}}}\\frac{\\overline X - \\overline Y  - \\Delta \\mu}{\\sqrt{ \\sum_{j=1}^n (X_j  - X)^2+ \\sum_{k=1}^m (Y_k - \\overline Y)^2}}\\sim t_{n+m-2}.$$\n",
    "\n",
    "Proof: Since $\\overline X \\sim N(\\mu_1,\\frac 1n \\sigma^2)$ and $\\overline Y\\sim N(\\mu_2,\\frac 1m \\sigma^2)$ are independent, we learn $\\overline X - \\overline Y -\\Delta\\mu \\sim N(0, \\left(\\frac 1n+\\frac 1m\\right)\\sigma^2)$. Therefore, by normalization we have\n",
    "$$\\sigma^{-1}\\sqrt{\\frac{nm}{n+m}}(\\overline X - \\overline Y - \\Delta \\mu)\\sim N(0,1).$$\n",
    "Also, we have known $\\sigma^{-2}\\sum_{j=1}^n (X_j - \\overline X)^2\\sim \\chi^2_{n-1}$, $\\sigma^{-2}\\sum_{k=1}^m (Y_k - \\overline Y)^2\\sim \\chi^2_{m-1}$ are independent with $\\overline X, \\overline Y$, so \n",
    "$$\\sigma^{-2}\\left(\\sum_{j=1}^n (X_j - \\overline X)^2+\\sum_{k=1}^m (Y_k - \\overline Y)^2\\right)\\sim \\chi^2_{n+m-2}$$\n",
    "and is independent with $\\overline X,\\overline Y$. Hence the statistics $T$ is $t_{n+m-2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Tests\n",
    "### Permutation Test\n",
    "\n",
    "Suppose we have two samples $X:\\ X_1,\\dotsc,X_m$ and $Y:\\ Y_1,\\dotsc,Y_n$. We are willing to know whether $X,Y$ are sampled from the same distribution. \n",
    "\n",
    "Under the hypothesis that they are from the same distribution, we merge the two samples into one sample with size $m+n$ and randomly redivide the sample into two (ordered) $X':\\ X_1',\\dotsc,X_m'$ and $Y':\\ Y_1',\\dotsc,Y_n'$. We measure the resemblance by \n",
    "$$T = |\\overline X' - \\overline Y'|\\quad{\\rm or}\\quad T = |\\overline X' - \\overline Y'|^2+|\\frac{1}{m-1}\\sum_{i=1}^n (X_i' - \\overline X')^2\n",
    "-\\frac{1}{n-1}\\sum_{j=1}^n(Y_i' - \\overline Y')^2|.$$\n",
    "\n",
    " Among all possible divisions,  our initial observation $(X,Y)$ should have a high probability being alike as we assume that they have the same distribution. We can count how many divisions $(X',Y')$ presents more significant difference,\n",
    " $$p = \\frac{1}{(m+n)!}\\sum_{{\\rm Divisions\\ }(X',Y')}\\mathbb I_{T(X',Y')>T{(X,Y)}}.$$\n",
    "\n",
    "Here we divide it by $(m+n)!$ because there are $(m+n)!$ different ordered divisions. \n",
    "\n",
    "When $p$ is too small, say $p<\\alpha$, it means that $T(X,Y)$ is larger than a proportion of $1 - \\alpha$ divisions. It implies that $(X,Y)$ has great difference. And in this case we reject $H_0$ at $\\alpha$-significance level.\n",
    "\n",
    "<br>\n",
    "\n",
    "In general, $(m+n)!$ might be large. We can estimate $p$ by randomly sampling some of the  divisions.\n",
    "\n",
    "<br>\n",
    "\n",
    "Permutation test is nonparametric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 6e-04\n"
     ]
    }
   ],
   "source": [
    "# example: test whether two independent samples are from the same distribution\n",
    "# X: (0.225, 0.262, 0.217, 0.240, 0.230, 0.229, 0.235, 0.217)\n",
    "# Y: (0.209, 0.205, 0.196, 0.210, 0.202, 0.207, 0.224, 0.223, 0.220, 0.201)\n",
    "x <- c(225,262,217,240,230,229,235,217) * .001\n",
    "m <- length(x)\n",
    "y <- c(209,205,196,210,202,207,224,223,220,201) * .001\n",
    "t <- abs(mean(x) - mean(y))\n",
    "x <- c(x,y)\n",
    "n <- length(x)\n",
    "s <- 0\n",
    "for (i in (1:10000)){\n",
    "\tz <- sample(x, n) # random permutation, the first m data are X', while the other are Y'\n",
    "\tif (abs(mean(z[1:m]) - mean(z[(m+1):n]) > t)){\n",
    "\t\ts <- s + 1\n",
    "\t}\n",
    "}\n",
    "print(s / 10000) # very small, so reject H0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neymann-Pearson Lemma \n",
    "\n",
    "If a test of size $\\alpha$ has the following form:\n",
    "$$\\begin{aligned}&H_0:\\ \\theta = \\theta_0\\quad {\\rm against}\\quad H_1:\\ \\theta = \\theta_1\\\\ &\n",
    "{\\rm reject \\ }H_0 {\\rm \\ when \\quad\\quad }\n",
    "L(\\theta_1; x) > KL(\\theta_0 x)\\\\ &\n",
    "{\\rm not\\ reject \\ }H_0{\\rm \\ when \\ }L(\\theta_1; x) < KL(\\theta_0 x),\n",
    "\\end{aligned}$$\n",
    "where $L(\\theta;x)$ is the likelihood function and $K>0$ is a constant determined by $\\alpha$, then the test is called the **most powerful test (MPT)**, in the sense of minimizing the probability of Type II Error or maximizing the power $\\beta(\\theta)$.\n",
    "\n",
    "Corollary: The likelihood ratio test is UMPT (uniformly most powerful test)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
