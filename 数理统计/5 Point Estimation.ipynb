{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Point Estimation\n",
    "Point estimation, a fundamental task in the subject of statistics, requires a single 'best' guess for the unknown parameters $\\theta$ by a statistic,\n",
    "$$\\hat \\theta = g(X_1,X_2,\\dotsc,X_n).$$\n",
    "\n",
    "And our estimator $\\hat \\theta$, \n",
    "relying on samples $X_1,X_2,\\dotsc,X_n$, is also a random variable. In spite of the randomness of $\\hat \\theta$, we shall construct an estimator $\\hat \\theta$ that has a high probability to approach $\\theta$.\n",
    "\n",
    "## Methods of Moments Estimation\n",
    "\n",
    "Let $\\mu_k \\equiv \\mu_k(\\theta) =   \\mathbb E(X^k)$ denote the $k$-th moment. Denote the sample moment by \n",
    "$$M_k = \\frac 1n \\sum_{i=1}^n X_i^k.$$\n",
    "\n",
    "The MM estimator (MME) $\\hat \\theta$ satisfies that the sample moments are equal to the theoretical ones. \n",
    "\n",
    "$$\\mu_i (\\hat \\theta) = M_i\\quad (i = 1,2,\\dotsc,k).$$\n",
    "\n",
    "For example when $k = 2$ and we estimate the mean and the variance $(\\mu,\\sigma^2)$. Since \n",
    "$\\hat\\mu = M_1$ and $\\hat\\sigma^2 + \\hat \\mu^2 = M_2$, the MM estimator is given by \n",
    "$$\\hat \\mu = \\overline X\\quad\\quad\\quad \\hat \\sigma^2 = \\frac 1n \\sum_{i=1}^n X_i^2 - (\\overline X)^2\n",
    " = \\frac1n\\sum_{i=1}^n\\left(X_i - \\overline X\\right)^2.$$\n",
    " \n",
    "### Sample Variance\n",
    "\n",
    "Note that in the order-2 MM estimator,\n",
    "$$\\begin{aligned}\\mathbb E(\\hat \\sigma^2)& = \\mathbb E( \\frac 1n \\sum_{i=1}^n X_i^2- (\\overline X)^2) = \n",
    "\\left({\\rm Var}( X)+\\left(\\mathbb E( X)\\right)^2\\right)\n",
    " -\\left({\\rm Var}(\\overline X)+\\left(\\mathbb E(\\overline X)\\right)^2\\right)\\\\ \n",
    " &=  (\\mu^2+\\sigma^2) - \\left(\\frac{1}{n}\\sigma^2+\\mu^2\\right)\n",
    "\\\\&=\\frac{n-1}{n}\\sigma^2\n",
    " \\end{aligned}$$\n",
    " is biased. An unbiaseed estimator for $\\sigma^2$ called the sample variance is given by \n",
    " $$\\hat\\sigma'^2 =\\frac{1}{n-1}\\sum_{i=1}^n X_i^2 - (\\overline X)^2$$\n",
    "\n",
    " ### Asymptotical Normality\n",
    "\n",
    "Often (with some mild conditions), the MME $\\hat \\theta$ converges to $\\theta$  and\n",
    "$$\\frac{\\sqrt {n} (\\hat \\theta - \\theta)}{\\widehat{\\rm SE}(\\hat \\theta)}\\rightarrow N(0,1)\\quad\\quad (n\\rightarrow \\infty).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "For fixed $x$ and variables $\\theta$, $L(\\theta) = f_\\theta(x)$ denotes the density or probability of $x$ \n",
    "from the distribution $F(\\theta)$. \n",
    "\n",
    "Our strategy of maximum likelihood estimation (MLE) is to find a $\\hat \\theta$ to maximize $L(\\hat \\theta)$.\n",
    "\n",
    "### Log-Likelihood Function\n",
    "\n",
    "Often $X_1,\\dotsc,X_n$ are i.i.d. s from $F(\\theta)$, hence\n",
    "$$L(\\theta) = \\prod_{i=1}^n f_\\theta(X_i).$$\n",
    "To maximize $L(\\theta)$ is equivalent to maximizing its logarithm, given by\n",
    "$$I(\\theta) = \\log L(\\theta) = \\sum_{i=1}^n \\log f_\\theta(X_i).$$\n",
    "Usually, $I(\\theta)$ reaches the minimum when $\\frac{\\partial}{\\partial \\theta}I = 0$, or\n",
    "$$0=s(\\theta) = \\frac{\\partial}{\\partial \\theta}I(\\theta) .$$\n",
    "And we call $s(\\theta)$ the score function.\n",
    "\n",
    "\n",
    "### Consistency\n",
    "\n",
    "MLE converges, i.e. $\\hat \\theta_{\\rm MLE} \\stackrel{\\mathbb P}{\\rightarrow} \\theta$ as $n\\rightarrow \\infty$.\n",
    "\n",
    "### Regression\n",
    "\n",
    "Suppose the parameters $\\Theta\\in \\mathbb R^{m\\times n}$ is unknown. Given $X\\in \\mathbb R^n$, there is a random variable $Y\\in \\mathbb R^m$  that $Y = \\Theta X+w$ where $w\\in \\mathbb R^m\\sim N(0,\\sigma^2\\mathbb I_m)$ is the noise and $\\sigma^2$ is unknown. Now that we have observed $(X_1,Y_1),\\dotsc,(X_t,Y_t)$, the likelihood function is \n",
    "$$L(\\Theta) = \\prod_{j=1}^t \\frac{1}{(2\\pi\\sigma^2)^\\frac m2  }\\exp\\left(-\\frac{1}{2\\sigma^{2}}(Y_j - \\Theta X_j)^T (Y_j - \\Theta X_j)\\right).$$\n",
    "\n",
    "It has log-likelihood function\n",
    "$$\\begin{aligned}I(\\theta)& = \\sum_{j=1}^t  \\left(-\\frac{1}{2\\sigma^2}(Y_j - \\Theta X_j)^T(Y_j - \\Theta X_j)-\\frac m2\\log(2\\pi) - m \\log|\\sigma| \\right)\\\\\n",
    "&=-\\frac{1}{2\\sigma^2}\\left({\\rm tr}(\\mathbf X^T\\Theta^T\\Theta \\mathbf X)-2{\\rm tr}(\\mathbf Y^T\\Theta\\mathbf X)+{\\rm tr}(\\mathbf Y^T\\mathbf Y)\\right)\n",
    "-\\frac m2t\\log(2\\pi) - mt \\log|\\sigma| \\\\\n",
    "&=-\\frac{1}{2\\sigma^2}\\Vert\\mathbf Y -\\Theta\\mathbf X\\Vert_F^2 -\\frac m2t\\log(2\\pi) - mt \\log|\\sigma|.\n",
    "\\end{aligned}$$\n",
    "\n",
    "The MLE $\\hat \\Theta$ is therefore given by $\\hat \\Theta = {\\rm argmin}\\Vert \\mathbf Y -\\Theta\\mathbf X\\Vert_F^2=\\mathbf Y\\mathbf X^\\dag$, solution to a least squares problem. Further, take the derivative with respect to $\\sigma$ and we obtain\n",
    "$$\\hat \\sigma^2=\\frac{\\Vert \\mathbf Y -\\mathbf Y\\mathbf X^\\dag \\mathbf X\\Vert_F^2}{mt} .$$\n",
    "\n",
    "\n",
    "### Univariable Gaussian\n",
    "\n",
    "Suppose $Y_1,\\dotsc,Y_t\\in \\mathbb R$ are samples from $N(\\mu,\\sigma^2)$ and $\\mu,\\sigma^2$ are unknown. MLE estimator to $(\\mu,\\sigma)$ is a special case of the regression. Introduce $X_1 = X_2 = \\dotsc  = X_n\\equiv 1$ and $\\Theta = \\mu$ in the regression above, we know that $\\mathbf X^\\dag = [\\frac 1n,\\dotsc,\\frac 1n]^T$ and \n",
    "$$\\hat \\mu = \\hat \\Theta = \\mathbf Y\\mathbf X^\\dag = \\sum_{i=1}^t\\frac 1t Y_i = \\overline Y$$\n",
    "and the MLE of the variance is given by\n",
    "$$\\hat \\sigma^2 = \\frac{1}{t}\\sum_{i=1}^t (Y_i - \\overline Y)^2.$$\n",
    "\n",
    "It is equivalent to MME in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Information\n",
    "\n",
    "Consider the score function $s(\\theta) = \\frac{\\partial}{\\partial \\theta}\\log f_{\\theta}(x)$ . Given $\\theta$ and $s(\\theta)$ is a random variable associated with $x$, then\n",
    "$$\\mathbb E(s(\\theta))\n",
    "=\\int s(\\theta)f_\\theta (x)dx=\\int \\frac{1}{ f_\\theta(x)}\\frac{\\partial f_\\theta  (x)}{\\partial \\theta}f_\\theta(x)dx\n",
    "=\\frac{\\partial}{\\partial\\theta}\\int f_{\\theta }(x)dx=\\frac{\\partial}{\\partial\\theta}{\\bf 1}=0\n",
    "$$\n",
    "\n",
    "And it has variance matrix\n",
    "$$\\mathcal I(\\theta) = {\\rm Var}(s(\\theta))=\\mathbb E(s(\\theta)s(\\theta)^T)$$\n",
    "\n",
    "Note that the $(i,j)$ entry can be represented by \n",
    "$$\\begin{aligned}&\\ \\mathbb E(\\frac{\\partial s}{\\partial \\theta_i}\\frac{\\partial s}{\\partial \\theta_j})\n",
    "=\\int \\frac{1}{f_\\theta (x)}\\frac{\\partial f_\\theta(x)}{\\partial \\theta_i}\n",
    "\\frac{1}{f_\\theta (x)}\\frac{\\partial f_\\theta (x)}{\\partial \\theta_j}\n",
    "f_\\theta (x)dx = \\int \\frac{1}{f_\\theta (x)}\\frac{\\partial f}{\\partial \\theta_i}\\frac{\\partial f}{\\partial \\theta_j}dx\\\\ &=  \\int \\frac{1}{f_\\theta (x)}\\frac{\\partial f}{\\partial \\theta_i}\\frac{\\partial f}{\\partial \\theta_j}\n",
    "dx-\\frac{\\partial ^2}{\\partial \\theta_i\\theta_j}{\\bf 1}\n",
    "\n",
    "=\\int \\left(\\frac{1}{f_\\theta (x)}\\frac{\\partial f}{\\partial \\theta_i}\\frac{\\partial f}{\\partial \\theta_j}\n",
    "-\\frac{\\partial ^2f}{\\partial \\theta_i\\theta_j}\\right)dx\\\\ &\n",
    "=\\mathbb E\\left(\\frac{1}{f^2_\\theta (x)}\\frac{\\partial f}{\\partial \\theta_i}\\frac{\\partial f}{\\partial \\theta_j}\n",
    "-\\frac{1}{f_\\theta (x)}\\frac{\\partial ^2f}{\\partial \\theta_i\\theta_j}\\right)\n",
    "=-\\mathbb E\\left(\\frac{\\partial }{\\partial \\theta_j}\\int \\frac{1}{f_\\theta (x)}\\frac{\\partial f}{\\partial \\theta_i}dx\n",
    "\\right) \\\\ &= \n",
    "-\\mathbb E\\left(\\frac{\\partial ^2}{\\partial \\theta_i\\partial \\theta_j}\\log f_\\theta (x)\n",
    "\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hence, $$\\mathcal I(\\theta) = {\\rm Var}(s(\\theta))=\\mathbb E(s(\\theta)s(\\theta)^T)\n",
    "=-\\nabla_{\\theta}^2\\mathbb E(\\log L(\\theta; x))$$\n",
    "The variance of $s(\\theta)$ is called the Fisher information matrix.\n",
    "\n",
    "In particular when $X=[X_1,\\dotsc,X_n]^T$ is a group of iids, we have \n",
    "$$\\begin{aligned}\\mathcal I_X(\\theta)& =-\\nabla_{\\theta}^2\\mathbb E_X(s_{X}(\\theta)) \n",
    "=-\\nabla_{\\theta}^2\\int \\dotsi\\int \\sum_{i=1}^n \\log f_\\theta (x_i) \\prod_{j=1}^n f_\\theta(x_i)dx_1\\dotsm dx_n\\\\\n",
    "&=-n\\nabla_{\\theta}^2\\int \\dotsi\\int \\prod_{j=2}^n f(x_j)dx_2\\dotsm dx_n\\int\\log f_\\theta (x_1) f_\\theta(x_1)dx_1 \n",
    "\\\\&=-n\\nabla_{\\theta}^2\\int\\log f_\\theta (x_1) f_\\theta(x_1)dx_1 \\\\&=n\\mathcal I_{X_1}(\\theta).\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "### Cramér-Rao Inequality\n",
    "\n",
    "Suppose $g(\\theta)$ is differentiable and a statistics $T$ satisfies that $\\mathbb E(T) = g(\\theta)$ at parameters $\\theta$ so that $T$ is an unbiased estimator for $g(\\theta)$ everywhere. Then, the variance of the statistics is bounded below by [[1](https://stats.stackexchange.com/questions/500529/proof-of-the-multivariate-cramer-rao-inequality)]\n",
    "$${\\rm Var}(T)\\succeq  \\left(\\frac{\\partial g}{\\partial \\theta}\\right)^T\\mathcal I(\\theta)^{-1}\\left(\\frac{\\partial g}{\\partial \\theta}\\right).$$\n",
    "\n",
    "Call $T$ the **minimum variance unbiased estimator (MVUE)** of $g$ when the equality holds. \n",
    "\n",
    "Particularly, if $g(\\theta)\\equiv \\theta$ and $T$ is unbiased, we learn that \n",
    "$${\\rm Var}(\\hat \\theta_j) \\succeq \\left(\\mathcal I(\\theta)_j\\right)^{-1}\n",
    "=-\\left(\\frac{\\partial^2}{\\partial \\theta_j^2}\\log f_\\theta (x)\\right)^{-1}.$$\n",
    "\n",
    "\n",
    "Example: For a 1D normal distribution $N(\\mu,\\sigma^2)$ where $\\sigma^2$ is known but $\\mu$ is unknown. Since we know $\\log L(\\mu; x) = -\\frac{(x-\\mu)^2}{2\\sigma^2}-\\log \\sqrt{2\\pi \\sigma^2}$,  the Fisher information matrix for $\\mu$ is given by\n",
    "$$\\mathcal I(\\mu) = -\\frac{\\partial^2}{\\partial \\mu^2}\\mathbb E(\\log L(\\mu; x))\n",
    "=\\sigma^{-2}.\n",
    "$$\n",
    "\n",
    "Now assume we have $X = [X_1,\\dotsc,X_n]^T$ as a sample, $\\mathcal I_X(\\mu) = n\\sigma^{-2}$. Let $T$ be the plugin estimator $T = \\overline X$ and $\\mathbb E(T)=\\mu$. Its variance is given by \n",
    "$${\\rm Var}(T) = {\\rm Var}\\left(N(\\mu,\\frac1n \\sigma^2)\\right) = \\frac 1n \\sigma^2=\n",
    "\\frac{\\left(\\mathbb E\\left(T\\right)\\right)^2}{\\mathcal I_X(\\mu)}.$$\n",
    "\n",
    "Hence we conclude that $T = \\overline X$ is the MVUE.\n",
    "\n",
    "\n",
    "### Asymptotic Normality\n",
    "\n",
    "As $n\\rightarrow \\infty$, the MLE estimator on iids holds the property that \n",
    "$$\\sqrt n(\\hat \\theta_{\\rm MLE} - \\theta)\\stackrel{d}{\\rightarrow}N(0,\\mathcal I_{X_1}(\\theta)^{-1}).$$\n",
    "\n",
    "This implies that MLE estimator attains the Cramér-Rao lower bound when $n\\rightarrow \\infty$. Any (consistent) estimator that has the property is called efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
