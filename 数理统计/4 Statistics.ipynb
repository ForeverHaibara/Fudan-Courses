{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Statistics\n",
    "\n",
    "Suppose there is a distribution with parameters $\\theta$. For example, a typical normal distribution has parameters \n",
    "$\\theta = (\\mu,\\sigma^2)$, the mean and the variance, while a Gammma distribution might have parameters such as $\\theta = (\\alpha, \\beta)$. \n",
    "\n",
    "Now we suppose that these parameters are **unknown**. The only information we have is a random sample from the distribution, $X_1,\\dotsc,X_n$. What we wish to do, is to guess (or estimate) the unknown paramaters $\\theta$ from the samples.\n",
    "\n",
    "\n",
    "### Sample \n",
    "\n",
    "Sample, or a random sample, is $n$ **independent** data sampled from the same distribution, which we often denote \n",
    "by $$X = \\left(X_1,X_2,\\dotsc,X_n\\right).$$\n",
    "\n",
    "### Statistic\n",
    "\n",
    "A function of the sample is called a statistic. For example, \n",
    "$$\\hat \\mu(X) = \\frac 1n\\left(X_1+X_2+\\dotsc +X_n\\right)$$\n",
    "is a statistic because one can compute $\\hat \\mu(X)$ from $X$ without other knowledge.\n",
    "\n",
    "\n",
    "## Point Estimation\n",
    "\n",
    "Point estimation, a fundamental task in the subject of statistics, requires a single 'best' guess for the unknown parameters $\\theta$ by a statistic,\n",
    "$$\\hat \\theta = g(X_1,X_2,\\dotsc,X_n).$$\n",
    "\n",
    "And our estimator $\\hat \\theta$, \n",
    "relying on samples $X_1,X_2,\\dotsc,X_n$, is also a random variable. In spite of the randomness of $\\hat \\theta$, we shall construct an estimator $\\hat \\theta$ that has a high probability to approach $\\theta$.\n",
    "\n",
    "\n",
    "\n",
    "### Mean Square Error\n",
    "\n",
    "To evaluate the performance of the estimator $\\theta$, we introduce the mean square error\n",
    "$$\\mathcal L_{MSE} = \\mathbb E\\left((\\hat \\theta -\\theta)^2\\right)$$\n",
    "\n",
    "This characterizes the average (expected) distance from $\\hat \\theta $ to $\\theta$. \n",
    "\n",
    "### Bias\n",
    "\n",
    "One can simplify the mean square error by \n",
    "$$\\begin{aligned}\n",
    "\\mathcal L_{MSE} = \\mathbb E\\left((\\hat \\theta -\\theta)^2\\right)\n",
    "= \\left( \\mathbb E(\\hat \\theta -\\theta)\\right)^2\n",
    "+\\left({\\rm Var}(\\hat \\theta-\\theta)\\right)^2\n",
    "=\\left( {\\rm Bias}(\\hat \\theta)\\right)^2\n",
    "+\\left({\\rm Var}(\\hat \\theta)\\right)^2\n",
    "\\end{aligned}$$\n",
    "where $ {\\rm Bias}(\\hat \\theta)= \\mathbb E(\\hat \\theta -\\theta)= \\mathbb E(\\hat \\theta )-\\theta$ is called the bias of the estimator. \n",
    "\n",
    "In particular, when a estimator has zero bias for whatever $\\theta$, we call the estimator unbiased.\n",
    "\n",
    "### Standard Deviation\n",
    "\n",
    "The standard deviation of the estimator is \n",
    "$${\\rm SE}(\\hat \\theta) = \\sqrt{{\\rm Var}(\\hat \\theta)}.$$\n",
    "\n",
    "Therefore we can rewrite the mean square error by bias and the standard deviation,\n",
    "$$\\mathcal L_{MSE} = \\left( {\\rm Bias}(\\hat \\theta)\\right)^2+\\left({\\rm SE}(\\hat \\theta)\\right)^2$$\n",
    "\n",
    "### Estimated Standard Error\n",
    "\n",
    "The estimated standard error is an estimation to ${\\rm SE}(\\hat \\theta)$ using estimated $\\hat \\theta$,\n",
    "$$\\widehat{\\rm SE}(\\hat \\theta) = \\sqrt{{\\rm Var_{\\hat \\theta}}(\\hat \\theta)}.$$\n",
    "\n",
    "### Consistency\n",
    "\n",
    "The estimator $\\hat \\theta$ is consistent if $\\hat \\theta_n\\stackrel{\\mathbb P}{\\rightarrow}\\theta$, converges as \n",
    "the sample grows large. \n",
    "\n",
    "If ${\\rm MSE}(\\hat \\theta_n)\\rightarrow 0$, we can conclude that \n",
    "$\\hat \\theta_n \\stackrel{m.s.}{\\rightarrow}\\theta$ and therefore $\\hat \\theta\\stackrel{\\mathbb P}{\\rightarrow}\\theta$.\n",
    "\n",
    "Proof: By the Markov inequality, for any $\\epsilon>0$ we have\n",
    "$$\\mathbb P((\\hat \\theta - \\theta)^2\\geqslant \\epsilon) \\leqslant \n",
    "\\frac{\\mathbb E\\left((\\hat \\theta - \\theta)^2\\right)}{\\epsilon}\\rightarrow 0$$\n",
    "\n",
    "### Asymptotic Normality\n",
    "\n",
    "An estimator $\\hat \\theta$ is asymptotically normal if\n",
    "$$\\frac{\\hat \\theta - \\theta}{{\\rm SE}(\\hat \\theta)}\\stackrel{d}{\\rightarrow}N(0,1).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example \n",
    "\n",
    "In a Bernoulli distribution $B(p)$ where $p$ is an unknown parameter. Suppose now we have sampled $X_1,X_2,\\dotsc,X_n$, we estimate $p$ by $\\hat p = \\frac{1}{n}\\left(X_1+X_2+\\dotsc +X_n\\right)$. Then its bias is \n",
    "$${\\rm Bias}(\\hat p) = \\mathbb (\\hat p ) - p = 0.$$\n",
    "Since $\\hat p\\sim B(n,p)$, its standard deviation is\n",
    "$${\\rm SE}(\\hat p) = \\frac {1}{n^2}np(1-p) = \\frac{p(1-p)}{n}$$\n",
    "and the estimated standard error is substituting $p$ by $\\hat p$, that is, \n",
    "$$\\widehat {\\rm SE}(\\hat p) = \\frac{\\hat p(1 - \\hat p )}{n}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Sets\n",
    "\n",
    "For a distribution withunknown parameters $\\theta$. We estimate $\\theta$ by random samples $X_1,X_2,\\dotsc,X_n$. If $L,U$ are statistics and for **fixed** $\\forall \\theta$,\n",
    "$$\\mathbb P(L(X_1,X_2,\\dotsc,X_n)\\leqslant \\theta \\leqslant U(X_1,X_2,\\dotsc,X_n)) \\geqslant \\alpha$$ \n",
    "with respect to $X$. Then we call $(L,U)$ is (at least) a $(100\\alpha) \\%$ confidence set for $\\theta$.\n",
    "\n",
    "<font color = red>The randomness is on samples $X$. </font>\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "Suppose $X_1,X_2,\\dotsc,X_n$ are $n$ samples from a normal distribution $N(\\mu,1)$ where $\\mu$ is an unknown constant. Then the outcome we observe has the probability that \n",
    "$$\\mathbb P(-1.96\\leqslant \\sqrt{n}(\\overline X - \\mu) \\leqslant 1.96) \\approx 0.95,$$\n",
    "which can be equivalently intepretted as \n",
    "$$\\mathbb P(\\overline X -\\frac{1.96}{\\sqrt n}\\leqslant   \\mu \\leqslant \\overline X +\\frac{1.96}{\\sqrt n}) \\approx 0.95.$$\n",
    "\n",
    "This means: if we estimate $\\mu$ by lower bound $\\overline X -\\frac{1.96}{\\sqrt n}$  and upper bound\n",
    "$\\overline X +\\frac{1.96}{\\sqrt n}$, then we have $95\\%$ chance to be correct (regardless of what $\\mu$ is!). And \n",
    "$\\left(\\overline X -\\frac{1.96}{\\sqrt n},\\overline X +\\frac{1.96}{\\sqrt n}\\right)$ is a $95\\%$ confidence set (or confidence interval) for estimating $\\mu$.\n",
    "\n",
    "### Asymptotical Normality\n",
    "\n",
    "If we assume $\\hat \\theta_n$ is asymptotically normal, i.e. $\\frac{\\hat \\theta - \\theta}{{\\rm  SE}(\\hat \\theta)}\\stackrel{d}{\\rightarrow} N(0,1)$. Then if we assume $n$ is sufficiently large, we can regard it as a normal distribution and for arbitrary $0\\leqslant \\alpha \\leqslant 1$, we have, asymptotically, that \n",
    "$$\\mathbb P\\left(-\\Phi^{-1}\\left(\\frac {1+\\alpha}{2}\\right)\\leqslant \\frac{\\hat \\theta - \\theta}{{\\rm  SE}(\\hat \\theta)}\\leqslant \\Phi^{-1}\\left(\\frac {1+\\alpha}{2}\\right)\\right) = \\alpha,$$\n",
    "or, \n",
    "$$\\mathbb P\\left(\\hat \\theta-{\\rm  SE}(\\hat \\theta)\\Phi^{-1}\\left(\\frac {1+\\alpha}{2}\\right)\\leqslant  \\theta \\leqslant \\hat \\theta+{\\rm  SE}(\\hat \\theta) \\Phi^{-1}\\left(\\frac {1+\\alpha}{2}\\right)\\right) = \\alpha$$\n",
    "\n",
    "In particular, $\\alpha = 2\\Phi(2) - 1$ (more precisely, $\\alpha = 2\\Phi(1.96) - 1$) leads to a practial $95\\%$ confidence interval, \n",
    "$$\\mathbb P\\left(\\hat \\theta-2{\\rm  SE}\\leqslant  \\theta \\leqslant \\hat \\theta+2{\\rm  SE}(\\hat \\theta) \\right) \\approx 0.95.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.954499736103642"
      ],
      "text/latex": [
       "0.954499736103642"
      ],
      "text/markdown": [
       "0.954499736103642"
      ],
      "text/plain": [
       "[1] 0.9544997"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "2 * pnorm(2) - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing \n",
    "\n",
    "There are three fundamental problems in statistics, point estimation, confidence sets and hypothesis testing.\n",
    "\n",
    "Assume we have a hypothesis $H_0$. We call it a null hypothesis. We check that whether or not the data \n",
    "reject the hypothesis. This is the hypothesis testing. The topic will be placed in the following courses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Distribution Function \n",
    "\n",
    "Let $X_1,\\dotsc,X_n$ be a sample from CDF $F$. An non-parametric estimator for $F$ is given by \n",
    "$$\\hat F(x) = \\frac1n \\sum_{i=1}^n \\mathbb I(X_i\\leqslant x) = \\frac{{\\rm Number \\ of\\ } X_i\\leqslant x}{n}.$$\n",
    "\n",
    "For fixed $x$ it is clear that $\\hat F(x)\\sim \\frac1n  B(n,F(x))$, so\n",
    "$$\\mathbb E(\\hat F(x)) = F(x)\\quad \\quad {\\rm Var}(\\hat F(x)) = \\frac1nF(x)(1 - F(x)).$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
