# 计算机视觉

张力老师的计算机视觉专业选修课。总共有1次个人Project和两次小组Project，不过两次小组Project每次有三个任务所以任务量和单人差不多。没有其他作业或考试。

前几节课会讲神经网络的基本知识（没记笔记），之后的课程内容大致记录如下。

## Convolutional Neural Networks

### Convolution Layers

* 1D, 2D, 3D Convolution
* Dilated Convolution

### Pooling

* Max Pooling

### Normalization

* [Batch Normalization](https://arxiv.org/abs/1502.03167)
* Layer Normalization
* Instance Normalization
* [Group Normalization](https://arxiv.org/abs/1803.08494)

### Nets

* [LeNet-5](https://ieeexplore.ieee.org/document/726791)
* [AlexNet](https://dl.acm.org/doi/10.1145/3065386)
* [ZFNet](https://arxiv.org/abs/1311.2901) (a bigger AlexNet)
* [VGG](https://arxiv.org/abs/1409.1556)
* [GoogLeNet](https://arxiv.org/abs/1409.4842) (aggressive stem, inception module, global average pooling, auxiliary classifiers)
* [ResNet](https://arxiv.org/abs/1512.03385) (bottleneck block)
* Model Ensembles
* [Squeeze-and-Excitation Network](https://arxiv.org/abs/1709.01507) (SeNet)
* [Densely Connected Network](https://ieeexplore.ieee.org/document/8099726) (DenseNet)
* [MobileNet](https://arxiv.org/abs/1704.04861)
* [ShuffleNet](https://arxiv.org/abs/1707.01083)
* [Neural Architecture Search](https://arxiv.org/abs/1611.01578)

## Training

### Activation Functions

* Sigmoid
* TanH
* ReLU
* [LeakyReLU](https://www.semanticscholar.org/paper/Rectifier-Nonlinearities-Improve-Neural-Network-Maas/367f2c63a6f6a10b3b64b8729d601e69337ee3cc)
* Maxout
* [GELU](https://arxiv.org/abs/1606.08415v4) (common for NLP)

### Data Preprocessing

* Zero-centering (e.g. AlexNet)
* Normalization (e.g. ResNet)

### Regularization

* L1/L2 Regularzation Term (e.g. Elastic Net)
* [Dropout](https://dl.acm.org/doi/abs/10.5555/2627435.2670313)
* [Batch Normalization](https://arxiv.org/abs/1502.03167)
* [Fractional Pooling](https://arxiv.org/abs/1412.6071)
* [Stochastic Depth](https://arxiv.org/abs/1603.09382v1)

### Data Augmentation

* Random Crops and Scalings
* Random Translation / Rotation / Stretching / Shearing / Lens Distortions...
* [Cutout](https://arxiv.org/abs/1708.04552), [Mixup](https://arxiv.org/abs/1710.09412), [CutMix](https://arxiv.org/abs/1905.04899)

### Weight Initialization

* [Xavier Initialization](https://www.semanticscholar.org/paper/Understanding-the-difficulty-of-training-deep-Glorot-Bengio/b71ac1e9fb49420d13e084ac67254a0bbd40f83f)
* [Kaiming / MSRA Initialization](https://arxiv.org/abs/1502.01852)
* [Fixup Initialization](https://arxiv.org/abs/1901.09321)
* Pretraining

### Other Tricks

* Learning Rate: [Cosine](https://arxiv.org/abs/1608.03983) / Linear / Inverse Sqrt / Constant / [Warmup](https://arxiv.org/abs/1706.02677)...
* Early Stopping
* Hyperparameter [Random](https://dl.acm.org/doi/10.5555/2188385.2188395) / Grid Search
* [Model Ensembles](https://arxiv.org/abs/1704.00109)
* Transfer Learning
* [Large-Batch Training](https://arxiv.org/abs/1706.02677)
* [More Data](https://arxiv.org/abs/1811.08883v1)

## Object Detection and Segmentation

### Detection

* Sliding Window
* [Region Proposals](https://ieeexplore.ieee.org/document/6133291)
* [R-CNN](https://arxiv.org/abs/1311.2524)
* [Fast R-CNN](https://arxiv.org/abs/1504.08083)
* [Faster R-CNN](https://arxiv.org/abs/1506.01497)
* [Feature Pyramid Network](https://arxiv.org/abs/1612.03144v2) (FPN)
* [FCOS](https://arxiv.org/abs/1904.01355)
* [CornerNet](https://arxiv.org/abs/1808.01244)
* [CenterNet](https://arxiv.org/abs/1904.08189v3)
* [ExtremeNet](https://arxiv.org/abs/1901.08043)
* [DETR](https://arxiv.org/abs/2005.12872)
* [Sparse R-CNN](https://arxiv.org/abs/2011.12450)
* [pix2seq](https://arxiv.org/abs/2109.10852)

### Segmentation

* [Fully Convolutional Network](https://arxiv.org/abs/1411.4038) (FCN)
* [U-Net](https://arxiv.org/abs/1505.04597)
* [Pyramid Scene Parsing Network](https://arxiv.org/abs/1612.01105) (PSPNet)
* [DeepLabV3](https://arxiv.org/abs/1706.05587v3)
* [BiSeNet](https://arxiv.org/abs/1808.00897)
* [SETR](https://arxiv.org/abs/2012.15840)
* [Mask R-CNN](https://arxiv.org/abs/1703.06870v3)
* [Dense Captioning](https://ieeexplore.ieee.org/document/7780863)
* [Mesh R-CNN](https://arxiv.org/abs/1906.02739)

### Miscellaneous

* IoU (Intersection over Union) / NMS (Non-Max Suppression)
* [Rol-Align](https://arxiv.org/abs/1703.06870v3) / [RoI-Pool](https://arxiv.org/abs/1504.08083)
* Unpooling / [Max Unpooling](https://arxiv.org/abs/1505.04366)
* Bilinear / Bicubic - Interpolation
* [Deformable Convolution](https://arxiv.org/abs/1703.06211)

## Self-supervised Learning

* [Relative Positioning](https://arxiv.org/abs/1505.05192)
* [Jigsaw Puzzles](https://arxiv.org/abs/1603.09246)
* [Colorization](https://arxiv.org/abs/1603.08511)
* [Exemplar Network](https://arxiv.org/abs/1406.6909)
* [Image Transformation](https://arxiv.org/abs/1803.07728)
* [Image Reconstruction](https://arxiv.org/abs/1604.07379) (inpainting)
* [BYOL](https://arxiv.org/abs/2006.07733)
* [Sequence Order](https://arxiv.org/abs/1603.08561)
* [Spatialtemporal Jigsaw](https://www.semanticscholar.org/paper/Self-Supervised-Video-Representation-Learning-with-Huo-Ding/02993d7d231deb300b72ddcc3106b93d6531329c)
* [Audio-Visual Co-supervision](https://arxiv.org/abs/1712.06651)

### Contrastive Learning

* [Contrastive Multiview Coding](https://arxiv.org/abs/1906.05849)
* [Memory Bank](https://arxiv.org/abs/1805.01978)
* [MoCo](https://arxiv.org/abs/1911.05722)
* [CoCLR](https://arxiv.org/abs/2010.09709v2) (hard positive, optical flow)
* [SimCLR](https://arxiv.org/abs/2002.05709)
* [SwAV](https://arxiv.org/abs/2006.09882)
* [SimSiam](https://arxiv.org/abs/2011.10566)

## Attention

* Recurrent Neural Network (for NLP)
* [seq2seq](https://arxiv.org/abs/1409.3215v3) (for NLP)
* [Image Captioning](https://arxiv.org/abs/1502.03044)
* [Vision Transformer](https://arxiv.org/abs/2010.11929)
* [CNN with Attention](https://arxiv.org/abs/1805.08318)
* [BEIT](https://arxiv.org/abs/2106.08254)
* [DETR](https://arxiv.org/abs/2005.12872)
* [Pyramid Vision Transformer](https://arxiv.org/abs/2102.12122)
* [Swin Transformer](https://arxiv.org/abs/2103.14030)
