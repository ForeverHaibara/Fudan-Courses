{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Ridge Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Ridge regression with regularization factor $\\lambda\\ (\\lambda \\geqslant 0)$ estimates $\\hat\\beta$ by\n",
    "$$\\hat\\beta_\\lambda = (X^TX+\\lambda I)^{-1}X^Ty.$$\n",
    "\n",
    "### Regularization\n",
    "\n",
    "**Thoerem** The ridge regression estaimtor $\\hat\\beta = (X^TX+\\lambda I)^{-1}X^Ty$ is the minimizer of the following:\n",
    "\n",
    "$$\\hat\\beta_\\lambda = \\inf_{\\beta}\\left\\{\\Vert y - X\\beta\\Vert^2 +\\lambda \\Vert \\beta\\Vert^2\\right\\}.$$\n",
    "\n",
    "**Proof**\n",
    "$$\\begin{aligned}\n",
    "\\Vert y - X\\beta\\Vert^2 +\\lambda \\Vert \\beta\\Vert^2\n",
    "=\\beta^T(X^TX+\\lambda I)\\beta - 2y^TX\\beta + y^Ty.\n",
    "\\end{aligned}$$\n",
    "\n",
    "This is a quadratic form with minimum reached at \n",
    "$$-\\frac{b}{2a} = -\\frac 12\\left(X^TX+\\lambda I\\right)^{-1}\\left(2X^Ty\\right) = \\hat\\beta_\\lambda $$\n",
    "\n",
    "### Singular Value Shrinkage\n",
    "\n",
    "Apply singular value decomposition on $X$, say $X = UDV^T$ where $U,V\\in\\mathbb R^{n\\times (k+1)}$ are orthogonal and $D\\in\\mathbb R^{(k+1)\\times (k+1)}$ is diagonal and positive.\n",
    "\n",
    "Observe that $(VD^2V^T+\\lambda I)V(D^2+\\lambda I)^{-1}V^T = 0$, we can see that \n",
    "\n",
    "$$\\hat\\beta = (VD^2V^T+\\lambda I)^{-1}VDU^Ty = V(D^2+\\lambda I)^{-1}D U^Ty$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Statistics\n",
    "\n",
    "Here lists some of the statistics properties of the ridge regression estimator $\\hat\\beta_\\lambda$:\n",
    "$$\\left\\{\\begin{aligned}& \\Vert \\mathbb E \\hat\\beta_\\lambda - \\beta\\Vert^2 = \\lambda^2\\beta^T(X^TX+\\lambda I)^{-2}\\beta\n",
    "=\\beta^T(\\frac 1\\lambda X^TX+\\lambda I)^{-2}\\beta\n",
    "\\\\ & {\\rm Cov}(\\hat\\beta_\\lambda) =\\sigma^2 (X^TX+\\lambda I)^{-1}X^TX(X^TX+\\lambda I)^{-1}\n",
    "\\preceq \\sigma^2(X^TX+\\lambda I)^{-1}\n",
    "\\end{aligned}\\right.$$\n",
    "\n",
    "**Proof** Note that \n",
    "$$(X^TX+\\lambda I)^{-1}(X^TX+\\lambda I) = I\\quad \\Rightarrow\\quad (X^TX+\\lambda I)^{-1}X^TX =I -\\lambda (X^TX+\\lambda I)^{-1}.$$\n",
    "\n",
    "The bias $ \\Vert \\mathbb E\\hat\\beta_\\lambda - \\beta\\Vert^2$ is given by\n",
    "$$\\begin{aligned}\\left\\Vert \\mathbb E((X^TX+\\lambda I)^{-1}X^Ty - \\beta) \\right\\Vert^2 & = \n",
    "\\left\\Vert (X^TX+\\lambda I)^{-1}X^TX\\beta - \\beta\\right\\Vert^2\n",
    "\\\\ &= \\left\\Vert\\left( (X^TX+\\lambda I)^{-1}X^TX  - I\\right)\\beta\\right\\Vert^2\n",
    "\\\\ &= \\left\\Vert -\\lambda (X^TX+\\lambda I)^{-1}\\beta \\right \\Vert^2\n",
    "\\\\ &= \\lambda^2\\beta^T(X^TX+\\lambda I)^{-2}\\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The derivation of the covariance is rather trivial.\n",
    "\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "The mean squared error for $\\hat\\beta_\\lambda$ to estimate $\\beta$ is given by\n",
    "$$\\begin{aligned}{\\rm MSE}(\\hat\\beta_\\lambda) &=\\Vert \\mathbb E(\\hat \\beta_\\lambda) - \\beta\\Vert^2 + \\mathbb E(\\Vert \\hat\\beta_\\lambda - \\mathbb E\\hat\\beta_\\lambda \\Vert^2)\n",
    "\\\\ &=\\Vert \\mathbb E(\\hat \\beta_\\lambda) - \\beta\\Vert^2 + \\mathbb E{\\rm tr}\\left[\\left(\\hat\\beta_\\lambda - \\mathbb E\\hat\\beta_\\lambda\\right) \\left(\\hat\\beta_\\lambda - \\mathbb E\\hat\\beta_\\lambda\\right) ^T\\right]\n",
    "\\\\ &= \\lambda^2\\beta^T (X^TX+\\lambda I)^{-2}\\beta + {\\rm tr}\\left[\\sigma^2 (X^TX+\\lambda I)^{-1}X^TX(X^TX+\\lambda I)^{-1}\\right]\n",
    "\\\\ &= \\lambda^2\\beta^T (X^TX+\\lambda I)^{-2}\\beta +\\sigma^2 {\\rm tr}\\left[X^TX(X^TX+\\lambda I)^{-2}\\right]\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "\n",
    "#### Orthogonal Case\n",
    "\n",
    "We first study a special case, when $X^TX = I$. In this case, \n",
    "$${\\rm MSE} = \\frac{\\lambda^2\\beta^T\\beta}{(1+\\lambda)^2} + \\frac{\\sigma^2(k+1)}{(1+\\lambda)^2}\n",
    "\\quad{\\rm and}\\quad \\frac{\\partial MSE}{\\partial \\lambda}=\\frac{2\\beta^T\\beta \\lambda}{(1+\\lambda)^3}-\\frac{2\\sigma^2(k+1)}{(1+\\lambda)^3}.$$\n",
    "\n",
    "Therefore, $\\lambda_*= \\dfrac{\\sigma^2(k+1)}{\\beta^T\\beta}$ minimizes the MSE (and is better than the ordinary least squares). However, in general it seems that it is hard to obtain a closed form solution to the best $\\lambda_*$.\n",
    "\n",
    "\n",
    "#### General Case\n",
    "\n",
    "In general, we show that the OLS,$\\lambda = 0$, is not the minimizer of ${\\rm MSE}(\\hat\\beta_\\lambda)$ assuming $X^TX$ is nonsingular. To achieve this, we only need to show that $\\left.\\frac{\\partial}{\\partial \\lambda}{\\rm MSE}(\\hat\\beta_\\lambda)\\right|_{\\lambda = 0}< 0$.\n",
    "\n",
    "**Proof**\n",
    "\n",
    "Lemma. Let $W = (X^TX+\\lambda I)^{-1}$, then $\\frac{\\partial W}{\\partial \\lambda} = -W^2$.\n",
    "\n",
    "Proof to Lemma: Note that we have \n",
    "\n",
    "$$0 = \\frac{\\partial }{\\partial \\lambda}\\left[W(X^TX+\\lambda I)\\right]= \\frac{\\partial W}{\\partial \\lambda}(X^TX+\\lambda I) +   W\\quad\\Rightarrow\\quad \\frac{\\partial W}{\\partial \\lambda} = -W^2. $$\n",
    "\n",
    "Back to the original problem, \n",
    "$$\\frac{\\partial}{\\partial \\lambda}{\\rm MSE}(\\hat\\beta_\\lambda) = O(\\lambda) + \\sigma^2\\frac{\\partial}{\\partial\\lambda}\\Vert \\sqrt{X^TX}W\\Vert_F^2= O(\\lambda) +\\sigma^2{\\rm tr}(-W^2\\sqrt{X^TX}\\sqrt{X^TX}W).$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\left.\\frac{\\partial}{\\partial \\lambda}{\\rm MSE}(\\hat\\beta_\\lambda)\\right|_{\\lambda = 0}\n",
    "= -\\sigma^2{\\rm tr}((X^TX)^{-2})<0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out-Cross-Validation (LOOCV)\n",
    "\n",
    "Leave-one-out-cross-validation (LOOCV) can help select the best hyperparameter $\\lambda$ in ridge regression. It has the following process:\n",
    "\n",
    "For each piece of datum $x_i\\ (i=1,\\dotsc,n)$ from $X\\in\\mathbb R^{n\\times (k+1)}$, we can remove it from $X$ to obtain $X_{(i)}\\in\\mathbb R^{(n-1)\\times (k+1)}$. We can fit a ridge regression model $\\hat\\beta_{\\lambda, (i)}$ on $X_{(i)}$ and validate it on $x_i$: ${\\rm CV}_i = \\Vert y_i - x_i^T\\hat\\beta_{\\lambda,(i)}\\Vert^2$. When taking $i = 1,\\dotsc,n$ to obtain losses ${\\rm CV}_1,\\dotsc,{\\rm CV}_n$, we can compute the average loss ${\\rm CV} = \\frac 1n \\sum_{i=1}^n {\\rm CV}_i$. If denote $\\hat\\beta_\\lambda = (X^TX+\\lambda I)^{-1}X^Ty$ to be the ridge regression fitted on all data, then we can show that, the average of the loss has the following equation:\n",
    "$${\\rm CV} = \\frac 1n \\sum_{i=1}^n {\\rm CV}_i \\equiv \\frac 1n \\sum_{i=1}^n \\left(\\frac{y_i -\\hat y_i}{1 - h_{ii}} \\right)^2$$\n",
    "where $\\hat y = X\\hat\\beta_\\lambda$ is the prediction with the full model and $h_{ii}$ is the $(i,i)$-th entry of $X(X^TX+\\lambda I)^{-1}X^T$.\n",
    "\n",
    "We select $\\lambda$ with small loss ${\\rm CV}$.\n",
    "\n",
    "**Proof** Note that $X_{(i)}^TX_{(i)} = X^TX - x_ix_i^T$, we have by Sherman-Morrison-Windbury formula that\n",
    "$$\\begin{aligned}(X_{(i)}^TX_{(i)} + \\lambda I)^{-1}  & = (X^TX +\\lambda I - x_ix_i^T)^{-1} = \n",
    "(X^TX+\\lambda I)^{-1} +\\frac{(X^TX+\\lambda I)^{-1}x_ix_i^T(X^TX+\\lambda I)^{-1}}{1-x_i^T(X^TX+\\lambda I)^{-1}x_i}\n",
    "\\\\ &= (X^TX+\\lambda I)^{-1} +\\frac{(X^TX+\\lambda I)^{-1}x_ix_i^T(X^TX+\\lambda I)^{-1}}{1-h_{ii}}.\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus we can show that each $\\hat\\beta_{\\lambda,(i)}$ is given by \n",
    "$$\\begin{aligned}\\hat\\beta_{\\lambda,(i)} & = (X_{(i)}^TX_{(i)} + \\lambda I)^{-1}X_{(i)}^Ty_{(i)}\n",
    "= \\left[(X^TX+\\lambda I)^{-1} +\\frac{(X^TX+\\lambda I)^{-1}x_ix_i^T(X^TX+\\lambda I)^{-1}}{1-h_{ii}}\\right](X^Ty - x_iy_i)\n",
    "\\\\ &= \\hat\\beta_{\\lambda}-(X^TX+\\lambda I)^{-1}x_iy_i + \\frac{(X^TX+\\lambda I)^{-1}x_i\\left(x_i^T\\hat\\beta_\\lambda - h_{ii}y_i\\right)}{1 - h_{ii}}\n",
    "\\\\ &= \\hat\\beta_{\\lambda}+ \\frac{(X^TX+\\lambda I)^{-1}x_i\\left(\\hat y_i-y_i\\right)}{1 - h_{ii}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\\begin{aligned}{\\rm CV} &= \\frac 1n \\sum_{i=1}^n \\Vert y_i - x_i^T\\hat\\beta_{\\lambda,(i)}\\Vert^2=\\frac 1n\\sum_{i=1}^n \\left\\Vert y_i - x_i^T\\left(\\hat\\beta_\\lambda + \\frac{(X^TX+\\lambda I)^{-1}x_i(\\hat y_i - y_i)}{1 - h_{ii}}\\right)\\right\\Vert^2\n",
    "\\\\ &= \\frac 1n\\sum_{i=1}^n \\left\\Vert y_i - \\hat y_i -\\frac{h_{ii}(\\hat y_i - y_i)}{1 - h_{ii}}\\right\\Vert^2\n",
    "= \\frac 1n \\sum_{i=1}^n \\left(\\frac{y_i - \\hat y_i}{1 - h_{ii}}\\right)^2\\end{aligned}\n",
    "$$\n",
    "\n",
    "### K-Fold Cross Validation \n",
    "\n",
    "K-fold cross vaildation is a generalization of leave-one-out-cross-validation (LOOCV). It split the data matrix (in row) into $K$ parts. For each part $i$, we remove it from $X$ to obtain $X_{(i)}$ and fit a ridge regression $\\hat\\beta_{\\lambda,(i)}$ and validate it on part $i$. Compute the average of validation loss for $i = 1,\\dotsc,K$. \n",
    "\n",
    "It is clear that, when $K = n$, then K-fold is exactly the LOOCV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4 (tags/v3.8.4:dfa645a, Jul 13 2020, 16:46:45) [MSC v.1924 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
