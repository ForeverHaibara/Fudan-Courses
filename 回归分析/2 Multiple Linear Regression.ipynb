{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multiple Linear Regression\n",
    "\n",
    "Now we have multiple factors, and the linear regression has the form ($y_i,x_{ij}\\in\\mathbb R$)\n",
    "$$y_i = \\beta_0+\\beta_1x_{i1}+\\dotsc +\\beta_k x_{ik}+\\epsilon_i,$$\n",
    "or in the matrix form, (with $x_i = [1,x_{i1},\\dotsc,x_{ik}]^T\\in\\mathbb R^{ (k+1)}$)\n",
    "$$y_i = x_i^T\\beta+\\epsilon_i.$$\n",
    "\n",
    "Still we assume that the noise is independent with $\\mathbb E(\\epsilon_i )=0$ and ${\\rm Var}(\\epsilon_i)=\\sigma^2$.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can stack all $n$ observations by matrices,\n",
    "$y = [y_1,\\dotsc,y_n]\\in\\mathbb R^n$, $X = [x_1,\\dotsc,x_{n}]^T\\in\\mathbb R^{n\\times (k+1)}$ and $\\epsilon=[\\epsilon_1,\\dotsc,\\epsilon_n]\\in\\mathbb R^n$. As a consequence, $\\mathbb E(Y) =X\\beta$ and ${\\rm Cov}(Y) = \\sigma^2I_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Least Squares Estimator\n",
    "\n",
    "$${\\rm argmin}_{\\hat \\beta} \\Vert y - X\\hat \\beta\\Vert^2\\quad\\Leftrightarrow\\quad X^TX\\hat\\beta = X^Ty $$\n",
    "\n",
    "Proof: For arbitrary $b\\in\\mathbb R^{k+1}$,\n",
    "$$\\Vert y - Xb\\Vert^2-\\Vert y -X\\hat\\beta\\Vert^2\n",
    "=\\Vert (y - X\\hat\\beta)+X(\\hat\\beta -b)\\Vert^2 - \\Vert y - X\\hat \\beta\\Vert^2\n",
    "=2(y-X\\hat\\beta)^TX(\\hat \\beta - b)+\\Vert X(\\hat\\beta -b)\\Vert^2.$$\n",
    "\n",
    "In particular, if $X^TX\\hat\\beta = X^Ty$, we have $2(y-X\\hat\\beta)^TX=0$ and thus, \n",
    "$$\\Vert y - Xb\\Vert^2-\\Vert y -X\\hat\\beta\\Vert^2\\geqslant 0.$$\n",
    "\n",
    "Such $\\hat \\beta$ always exists, and one of the solutions is given by $\\hat \\beta = X^\\dag y$ where $X^\\dag$ is the pseudoinverse.\n",
    "\n",
    "However, we shall further assume $X^TX$ is nonsingular and $\\hat\\beta =(X^TX)^{-1}X^Ty$.\n",
    "\n",
    "In this case, \n",
    "\n",
    "$${\\rm Cov}(\\hat \\beta) = {\\rm Cov}((X^TX)^{-1}X^T(X\\beta + \\epsilon))\n",
    "= {\\rm Cov}((X^TX)^{-1}X^T\\epsilon)=\\sigma^2(X^TX)^{-1}.$$\n",
    "\n",
    "Here we have used the fact that ${\\rm Cov}(Au) = A{\\rm Cov}(u)A^T$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ham Matrix\n",
    "\n",
    "Note that $\\hat y = X\\hat \\beta= X(X^TX)^{-1}X^Ty$. We denote $H = X(X^TX)^{-1}X^T$ and call it the ham matrix. Properties:\n",
    "\n",
    "1. ${\\rm tr}(H) = {\\rm tr}((X^TX)^{-1}X^TX) = k+1$.\n",
    "2. $H$ is symmetric.\n",
    "3. $H$ is idempotent ($H^2=H$).\n",
    "4. $(I - H)X = 0$.\n",
    "\n",
    "\n",
    "### Maximum Likelihood Estimator \n",
    "\n",
    "Under the assumption that $\\epsilon_i\\in N(0,\\sigma^2)$ are independent samples from normal distribution. It is clear that the least squares estimator is exactly the maximum likelihood estimator.  To derive the MLE for $\\sigma^2$, we have\n",
    "\n",
    "$$\\hat\\sigma^2_{MLE} = {\\rm argmax}_\\sigma \\left\\{-\\frac{1}{2\\sigma^2}\\Vert y - X\\hat\\beta \\Vert^2 -\\frac{n}{2}\\log\\sigma^2\\right\\}=\\frac{1}{n} \\Vert y - X\\hat\\beta \\Vert^2=\\frac{1}{n} \\Vert y - Hy \\Vert^2$$\n",
    "\n",
    "Note that $(I-H)$ is symmetric and idempotent, we obtain\n",
    "$\\hat\\sigma^2_{MLE} =\\frac{1}{n} y^T(I - H)y$. \n",
    "\n",
    "The MLE for $\\hat\\sigma^2$ is biased. In fact, \n",
    "$$\\hat\\sigma^2_{MLE}=\\frac{1}{n} \\Vert (I - H)y\\Vert^2\n",
    "=\\frac{1}{n} \\Vert (I - H)(X\\beta+\\epsilon)\\Vert^2=\\frac{1}{n} \\Vert (I - H)\\epsilon \\Vert^2.$$\n",
    "\n",
    "Recall that $I - H$ being symmetric and idempotent implies that it has spectral decomposition $I - H=Q^T\\Lambda Q$ with $\\Lambda = \\left[\\begin{matrix}I_r & 0 \\\\ 0 & 0\\end{matrix}\\right]$ and $Q$ orthogonal. Here the rank $r$ is given by $r = {\\rm tr}(I - H) = n - k - 1$. Thus, $(I - H)\\epsilon $ is the sum of $n-k-1$ independent normal distribution $N(0,\\sigma^2)$. And we conclude that \n",
    "$$\\hat\\sigma^2_{MLE} \\sim \\frac{1}{n}\\chi_{n-k-1}^2\\sigma^2.$$\n",
    "\n",
    "Then, $\\mathbb E(\\hat\\sigma^2_{MLE}) = \\dfrac{n-k-1}{n}\\sigma^2$. To fix the biasedness, we can use the unbiased estimator \n",
    "$$s^2 = \\frac{n}{n-k-1}{\\sigma^2_{MLE}}=\\frac{y^T(I - H)y}{n-k-1} = \\frac{\\epsilon^T(I - H)\\epsilon}{n-k-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution\n",
    "\n",
    "From above, we know that \n",
    "$$\\hat\\beta = (X^TX)^{-1}X^Ty=  \\beta+ (X^TX)^{-1}X^T\\epsilon\\sim \\mathcal N(\\beta, (X^TX)^{-1}\\sigma^2)$$\n",
    "and \n",
    "$$s^2 = \\Vert (I - H)\\epsilon\\Vert^2\\sim \\frac{1}{n-k-1}\\chi_{n-k-1}^2\\sigma^2.$$\n",
    "\n",
    "Note that $(X^TX)^{-1}X^T\\epsilon$ and $(I - H)\\epsilon$ are uncorrelated multivariate normal distributions, which thus implies independence, $\\hat\\beta $ and $s^2$ are independent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
