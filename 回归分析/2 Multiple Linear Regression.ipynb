{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multiple Linear Regression\n",
    "\n",
    "Now we have multiple factors, and the linear regression has the form ($y_i,x_{ij}\\in\\mathbb R$)\n",
    "$$y_i = \\beta_0+\\beta_1x_{i1}+\\dotsc +\\beta_k x_{ik}+\\epsilon_i,$$\n",
    "or in the matrix form, (with $x_i = [1,x_{i1},\\dotsc,x_{ik}]^T\\in\\mathbb R^{ (k+1)}$)\n",
    "$$y_i = x_i^T\\beta+\\epsilon_i.$$\n",
    "\n",
    "Still we assume that the noise is independent with $\\mathbb E(\\epsilon_i )=0$ and ${\\rm Var}(\\epsilon_i)=\\sigma^2$.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can stack all $n$ observations by matrices,\n",
    "$y = [y_1,\\dotsc,y_n]\\in\\mathbb R^n$, $X = [x_1,\\dotsc,x_{n}]^T\\in\\mathbb R^{n\\times (k+1)}$ and $\\epsilon=[\\epsilon_1,\\dotsc,\\epsilon_n]\\in\\mathbb R^n$. As a consequence, $\\mathbb E(Y) =X\\beta$ and ${\\rm Cov}(Y) = \\sigma^2I_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Least Squares Estimator\n",
    "\n",
    "$${\\rm argmin}_{\\hat \\beta} \\Vert y - X\\hat \\beta\\Vert^2\\quad\\Leftrightarrow\\quad X^TX\\hat\\beta = X^Ty $$\n",
    "\n",
    "Proof: For arbitrary $b\\in\\mathbb R^{k+1}$,\n",
    "$$\\Vert y - Xb\\Vert^2-\\Vert y -X\\hat\\beta\\Vert^2\n",
    "=\\Vert (y - X\\hat\\beta)+X(\\hat\\beta -b)\\Vert^2 - \\Vert y - X\\hat \\beta\\Vert^2\n",
    "=2(y-X\\hat\\beta)^TX(\\hat \\beta - b)+\\Vert X(\\hat\\beta -b)\\Vert^2.$$\n",
    "\n",
    "In particular, if $X^TX\\hat\\beta = X^Ty$, we have $2(y-X\\hat\\beta)^TX=0$ and thus, \n",
    "$$\\Vert y - Xb\\Vert^2-\\Vert y -X\\hat\\beta\\Vert^2\\geqslant 0.$$\n",
    "\n",
    "Such $\\hat \\beta$ always exists, and one of the solutions is given by $\\hat \\beta = X^\\dag y$ where $X^\\dag$ is the pseudoinverse.\n",
    "\n",
    "However, we shall further assume $X^TX$ is nonsingular and $\\hat\\beta =(X^TX)^{-1}X^Ty$.\n",
    "\n",
    "In this case, \n",
    "\n",
    "$${\\rm Cov}(\\hat \\beta) = {\\rm Cov}((X^TX)^{-1}X^T(X\\beta + \\epsilon))\n",
    "= {\\rm Cov}((X^TX)^{-1}X^T\\epsilon)=\\sigma^2(X^TX)^{-1}.$$\n",
    "\n",
    "Here we have used the fact that ${\\rm Cov}(Au) = A{\\rm Cov}(u)A^T$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ham Matrix\n",
    "\n",
    "Note that $\\hat y = X\\hat \\beta= X(X^TX)^{-1}X^Ty$. We denote $H = X(X^TX)^{-1}X^T$ and call it the ham matrix. Properties:\n",
    "\n",
    "1. ${\\rm tr}(H) = {\\rm tr}((X^TX)^{-1}X^TX) = k+1$.\n",
    "2. $H$ is symmetric.\n",
    "3. $H$ is idempotent ($H^2=H$) and $I - H$ is also idempotent.\n",
    "4. $HX = X$.\n",
    "5. $(I - H)X = 0$.\n",
    "6. $\\hat y = Hy$.\n",
    "\n",
    "\n",
    "### Maximum Likelihood Estimator \n",
    "\n",
    "Under the assumption that $\\epsilon_i\\in N(0,\\sigma^2)$ are independent samples from normal distribution. It is clear that the least squares estimator is exactly the maximum likelihood estimator.  To derive the MLE for $\\sigma^2$, we have\n",
    "\n",
    "$$\\hat\\sigma^2_{MLE} = {\\rm argmax}_\\sigma \\left\\{-\\frac{1}{2\\sigma^2}\\Vert y - X\\hat\\beta \\Vert^2 -\\frac{n}{2}\\log\\sigma^2\\right\\}=\\frac{1}{n} \\Vert y - X\\hat\\beta \\Vert^2=\\frac{1}{n} \\Vert y - Hy \\Vert^2$$\n",
    "\n",
    "Note that $(I-H)$ is symmetric and idempotent, we obtain\n",
    "$\\hat\\sigma^2_{MLE} =\\frac{1}{n} y^T(I - H)y$. \n",
    "\n",
    "The MLE for $\\hat\\sigma^2$ is biased. In fact, \n",
    "$$\\hat\\sigma^2_{MLE}=\\frac{1}{n} \\Vert (I - H)y\\Vert^2\n",
    "=\\frac{1}{n} \\Vert (I - H)(X\\beta+\\epsilon)\\Vert^2=\\frac{1}{n} \\Vert (I - H)\\epsilon \\Vert^2.$$\n",
    "\n",
    "Recall that $I - H$ being symmetric and idempotent implies that it has spectral decomposition $I - H=Q^T\\Lambda Q$ with $\\Lambda = \\left[\\begin{matrix}I_r & 0 \\\\ 0 & 0\\end{matrix}\\right]$ and $Q$ orthogonal. Here the rank $r$ is given by $r = {\\rm tr}(I - H) = n - k - 1$. Thus, $(I - H)\\epsilon $ is the sum of $n-k-1$ independent normal distribution $N(0,\\sigma^2)$. And we conclude that \n",
    "$$\\hat\\sigma^2_{MLE} \\sim \\frac{1}{n}\\chi_{n-k-1}^2\\sigma^2.$$\n",
    "\n",
    "Then, $\\mathbb E(\\hat\\sigma^2_{MLE}) = \\dfrac{n-k-1}{n}\\sigma^2$. To fix the biasedness, we can use the unbiased estimator \n",
    "$$s^2 = \\frac{n}{n-k-1}{\\sigma^2_{MLE}}=\\frac{y^T(I - H)y}{n-k-1} = \\frac{\\epsilon^T(I - H)\\epsilon}{n-k-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution\n",
    "\n",
    "From above, we know that \n",
    "$$\\hat\\beta = (X^TX)^{-1}X^Ty=  \\beta+ (X^TX)^{-1}X^T\\epsilon\\sim \\mathcal N(\\beta, (X^TX)^{-1}\\sigma^2)$$\n",
    "and \n",
    "$$s^2 = \\Vert (I - H)\\epsilon\\Vert^2\\sim \\frac{1}{n-k-1}\\chi_{n-k-1}^2\\sigma^2.$$\n",
    "\n",
    "Note that $(X^TX)^{-1}X^T\\epsilon$ and $(I - H)\\epsilon$ are uncorrelated multivariate normal distributions, which thus implies independence, $\\hat\\beta $ and $s^2$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "Denote $e = [1,\\dotsc,1]^T\\in\\mathbb R^{n}$ and $n\\bar y =  e^Ty$. Recall $H = X(X^TX)^{-1}X^T$ is the Ham matrix and $\\hat y = Hy$.\n",
    "\n",
    "### Total Sum of Squares\n",
    "\n",
    "The total sum of square is unrelated with the model. ${\\rm SST} = \\sum_{i=1}^n (y_i - \\bar y )^2$.  Also, we can write it in quadratic form, \n",
    "$${\\rm SST} = \\Vert y - \\frac 1n ee^Ty\\Vert^2 = y^T(I - \\frac 1n ee^T)y.$$\n",
    "\n",
    "We shall note that $I - \\frac 1nee^T$ is idempotent and ${\\rm tr}(I - \\frac 1n ee^T) = n-1$.\n",
    "\n",
    "\n",
    "\n",
    "### Regression Sum of Squares\n",
    "\n",
    "For regression sum of squares, it is \n",
    "\n",
    "$${\\rm SSR} = \\Vert \\hat y - \\frac 1n ee^Ty\\Vert^2 = \\Vert Hy - \\frac 1n ee^Ty\\Vert^2 = y^T(H - \\frac 1n ee^T)^2y.$$\n",
    "\n",
    "Recall that $HX = X$ and thus $He = e$ as $e$ is the first column of $X$, we can show that $H - \\frac 1n ee^T$ is idempotent and therefore, \n",
    "$${\\rm SSR} = y^T(H - \\frac 1n ee^T)y.$$\n",
    "\n",
    "And ${\\rm tr}(H - \\frac 1n ee^T) = k$.\n",
    "\n",
    "### Residual Sum of Squares\n",
    "\n",
    "For residual sum of squares, it is \n",
    "$${\\rm SSE} = \\Vert y - \\hat y \\Vert^2 = \\Vert y - Hy\\Vert^2 = y^T(I - H)y=(n-k-1)\\sigma^2.$$\n",
    "And $I - H$ is also idempotent with  ${\\rm tr}(I - H) = n - k - 1$.\n",
    "\n",
    "### Relations \n",
    "\n",
    "Still we have $${\\rm SST} = {\\rm SSE}+{\\rm SSR}$$\n",
    "and the orthogonality between ${\\rm SSE}$ and ${\\rm SSR}$: \n",
    "$(I - H)^T(H - \\frac 1nee^T) = 0$.\n",
    "\n",
    "Therefore, ${\\rm SSR} = \\Vert (H - \\frac 1n ee^T)y\\Vert^2$ and ${\\rm SSE} = \\Vert (I - H)y\\Vert^2$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$\n",
    "\n",
    "The coefficient of determination is given by $R^2 = 1 - \\dfrac{\\rm SSE}{\\rm SST} = \\dfrac{\\rm SSR}{\\rm SST}$. And we call $\\rho = \\sqrt {R^2}$ the multiple correlation coefficient.\n",
    "\n",
    "Observe that \n",
    "$$\\frac{{\\rm Cov}(y,\\hat y)^2}{{\\rm Var}(y){\\rm Var}(\\hat y)}\n",
    "=\\frac{\\left((y  - \\frac 1nee^Ty)^T(Hy - \\frac 1n ee^THy)\\right)^2}{\\Vert y - \\frac 1n ee^Ty\\Vert^2\\Vert Hy - \\frac 1n ee^THy\\Vert^2}=\n",
    " \\frac{\\left(y^T(H - \\frac 1nee^T)y\\right)^2 }{\\Vert y - \\frac 1n ee^Ty\\Vert^2y^T(H - \\frac 1nee^T)y}=\\frac{\\rm SSR}{\\rm SST} = R^2,$$\n",
    "we conclude that $R^2$ characterizes the correlation between $y$ and $\\hat y$.\n",
    "\n",
    "### Adjusted $R^2$\n",
    "\n",
    "$R^2$ is not larger the better. Because counting in more factors, even rubbish factors, will reduce ${\\rm SSR}$, leading to the increase in $R^2$. We can fix the problem by introducing adjusted $R^2$, which is $R_a^2$ defined as below. \n",
    "\n",
    "$$R_a^2 = 1 - \\frac{{\\rm SSE}/(n - k - 1)}{{\\rm SST} / (n - 1)}.$$\n",
    "\n",
    "In this case, larger number of factors, $k$, will penalize the $R_a^2$.\n",
    "\n",
    "\n",
    "### Extra Sum of Squares\n",
    "\n",
    "As mentioned, introducing new factors, useful or not, can reduce ${\\rm SSE}$ (or equivalently, increast ${\\rm SSR}$). But useful ones often reduce it more. For current $X_1\\in\\mathbb R^{n\\times(k_1+1)}$ and a new set of factors $X_2\\in\\mathbb R^{k_2}$, we can merge them to $X = [X_1,X_2]\\in\\mathbb R^{n\\times (k_1+k_2+1)}$. \n",
    "\n",
    "If denote by ${\\rm SSE}(X_1)$ and ${\\rm SSE}([X_1,X_2])$ by the residual sum of squares of the model fitted on $X_1$ and $X=[X_1,X_2]$ respectively, we define the extra sum of squares by the decrease in ${\\rm SSE}$:\n",
    "\n",
    "$${\\rm SSR}(X_2|X_1) = {\\rm SSE}(X_1) - {\\rm SSE}([X_1,X_2]),$$\n",
    "or equivalently the increase in regression sum of squares, \n",
    "\n",
    "$${\\rm SSR}(X_2|X_1) =  {\\rm SSR}([X_1,X_2])-{\\rm SSR}(X_1).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "It is important in practice to test whether a factor is indeed influential to the response.\n",
    "\n",
    "\n",
    "### Test All Coefficients\n",
    "\n",
    "When we want to test $H_0:\\ \\beta_1=\\dotsc =\\beta_k = 0$, except that $\\beta_0$ can be nonzero, against its opposite, we can test \n",
    "$$F=\\frac{\\rm MSR}{\\rm MSE} =\\frac{{\\rm SSR}/k}{{\\rm SSE}/(n-k-1)}=\\frac{\\sigma^2\\chi_k^2}{\\sigma^2\\chi_{n-k-1}^2}\\sim F_{k,n-k-1}.$$\n",
    "\n",
    "We reject $H_0$ when $F$ is large enough, since this implies that the regression fits much and the factors are not nonsense.\n",
    "\n",
    "\n",
    "### Test Single Coefficient\n",
    "\n",
    "When we want to test whether one of the factors contributes, say, $H_0:\\beta_1 = 0$ against $H_1:\\beta_1\\neq 0$, we can recall that $\\hat\\beta_1 \\sim N(\\beta_1, \\sigma^2e_1^T(X^TX)^{-1}e_1)$ where $e_1 = [1,0,\\dotsc,0]^T\\in\\mathbb R^{k+1}$. And thus under the assumption $\\beta_1 = 0$, we have\n",
    "$$\\frac{\\hat\\beta_1 }{\\sqrt{e_1^T(X^TX)^{-1}e_1 s^2}}\\sim \\frac{\\sigma^2 N(0,1)}{\\sigma^2\\sqrt{\\frac{\\chi_{n-k-1}^2}{n-k-1}}}\n",
    "\\sim t_{n-k-1}.\n",
    "$$\n",
    "\n",
    "We reject $H_0$ when $t$ is far from origin, i.e. $|t|$ being \"abnormally\" large indicates $\\beta_1$ does not vanish.\n",
    "\n",
    "### Test Linearity of Coefficients\n",
    "\n",
    "More generally, if we want to test whether $C\\beta = 0$ where $C\\in\\mathbb  R^{m\\times (k+1)}$, we can use the generalized extra sum of squares: find the best, reduced, model under $H_0:C\\beta = 0$ and compare it with the full model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
