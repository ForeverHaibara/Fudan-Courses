{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multiple Linear Regression\n",
    "\n",
    "Now we have multiple factors, and the linear regression has the form ($y_i,x_{ij}\\in\\mathbb R$)\n",
    "$$y_i = \\beta_0+\\beta_1x_{i1}+\\dotsc +\\beta_k x_{ik}+\\epsilon_i,$$\n",
    "or in the matrix form, (with $x_i = [1,x_{i1},\\dotsc,x_{ik}]^T\\in\\mathbb R^{ (k+1)}$)\n",
    "$$y_i = x_i^T\\beta+\\epsilon_i.$$\n",
    "\n",
    "Still we assume that the noise is independent with $\\mathbb E(\\epsilon_i )=0$ and ${\\rm Var}(\\epsilon_i)=\\sigma^2$.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can stack all $n$ observations by matrices,\n",
    "$y = [y_1,\\dotsc,y_n]\\in\\mathbb R^n$, $X = [x_1,\\dotsc,x_{n}]^T\\in\\mathbb R^{n\\times (k+1)}$ and $\\epsilon=[\\epsilon_1,\\dotsc,\\epsilon_n]\\in\\mathbb R^n$. As a consequence, $\\mathbb E(Y) =X\\beta$ and ${\\rm Cov}(Y) = \\sigma^2I_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Least Squares Estimator\n",
    "\n",
    "$${\\rm argmin}_{\\hat \\beta} \\Vert y - X\\hat \\beta\\Vert^2\\quad\\Leftrightarrow\\quad X^TX\\hat\\beta = X^Ty $$\n",
    "\n",
    "Proof: For arbitrary $b\\in\\mathbb R^{k+1}$,\n",
    "$$\\Vert y - Xb\\Vert^2-\\Vert y -X\\hat\\beta\\Vert^2\n",
    "=\\Vert (y - X\\hat\\beta)+X(\\hat\\beta -b)\\Vert^2 - \\Vert y - X\\hat \\beta\\Vert^2\n",
    "=2(y-X\\hat\\beta)^TX(\\hat \\beta - b)+\\Vert X(\\hat\\beta -b)\\Vert^2.$$\n",
    "\n",
    "In particular, if $X^TX\\hat\\beta = X^Ty$, we have $2(y-X\\hat\\beta)^TX=0$ and thus, \n",
    "$$\\Vert y - Xb\\Vert^2-\\Vert y -X\\hat\\beta\\Vert^2\\geqslant 0.$$\n",
    "\n",
    "Such $\\hat \\beta$ always exists, and one of the solutions is given by $\\hat \\beta = X^\\dag y$ where $X^\\dag$ is the pseudoinverse.\n",
    "\n",
    "However, we shall further assume $X^TX$ is nonsingular and $\\hat\\beta =(X^TX)^{-1}X^Ty$.\n",
    "\n",
    "In this case, \n",
    "\n",
    "$${\\rm Cov}(\\hat \\beta) = {\\rm Cov}((X^TX)^{-1}X^T(X\\beta + \\epsilon))\n",
    "= {\\rm Cov}((X^TX)^{-1}X^T\\epsilon)=\\sigma^2(X^TX)^{-1}.$$\n",
    "\n",
    "Here we have used the fact that ${\\rm Cov}(Au) = A{\\rm Cov}(u)A^T$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hat Matrix\n",
    "\n",
    "Note that $\\hat y = X\\hat \\beta= X(X^TX)^{-1}X^Ty$. We denote $H = X(X^TX)^{-1}X^T$ and call it the hat matrix. Properties:\n",
    "\n",
    "1. ${\\rm tr}(H) = {\\rm tr}((X^TX)^{-1}X^TX) = k+1$.\n",
    "2. $H$ is symmetric.\n",
    "3. $H$ is idempotent ($H^2=H$) and $I - H$ is also idempotent.\n",
    "4. $HX = X$.\n",
    "5. $(I - H)X = 0$.\n",
    "6. $\\hat y = Hy$.\n",
    "\n",
    "\n",
    "### Maximum Likelihood Estimator \n",
    "\n",
    "Under the assumption that $\\epsilon_i\\in N(0,\\sigma^2)$ are independent samples from normal distribution. It is clear that the least squares estimator is exactly the maximum likelihood estimator.  To derive the MLE for $\\sigma^2$, we have\n",
    "\n",
    "$$\\hat\\sigma^2_{MLE} = {\\rm argmax}_\\sigma \\left\\{-\\frac{1}{2\\sigma^2}\\Vert y - X\\hat\\beta \\Vert^2 -\\frac{n}{2}\\log\\sigma^2\\right\\}=\\frac{1}{n} \\Vert y - X\\hat\\beta \\Vert^2=\\frac{1}{n} \\Vert y - Hy \\Vert^2$$\n",
    "\n",
    "Note that $(I-H)$ is symmetric and idempotent, we obtain\n",
    "$\\hat\\sigma^2_{MLE} =\\frac{1}{n} y^T(I - H)y$. \n",
    "\n",
    "The MLE for $\\hat\\sigma^2$ is biased. In fact, \n",
    "$$\\hat\\sigma^2_{MLE}=\\frac{1}{n} \\Vert (I - H)y\\Vert^2\n",
    "=\\frac{1}{n} \\Vert (I - H)(X\\beta+\\epsilon)\\Vert^2=\\frac{1}{n} \\Vert (I - H)\\epsilon \\Vert^2.$$\n",
    "\n",
    "Recall that $I - H$ being symmetric and idempotent implies that it has spectral decomposition $I - H=Q^T\\Lambda Q$ with $\\Lambda = \\left[\\begin{matrix}I_r & 0 \\\\ 0 & 0\\end{matrix}\\right]$ and $Q$ orthogonal. Here the rank $r$ is given by $r = {\\rm tr}(I - H) = n - k - 1$. Thus, $(I - H)\\epsilon $ is the sum of $n-k-1$ independent normal distribution $N(0,\\sigma^2)$. And we conclude that \n",
    "$$\\hat\\sigma^2_{MLE} \\sim \\frac{1}{n}\\chi_{n-k-1}^2\\sigma^2.$$\n",
    "\n",
    "Then, $\\mathbb E(\\hat\\sigma^2_{MLE}) = \\dfrac{n-k-1}{n}\\sigma^2$. To fix the biasedness, we can use the unbiased estimator \n",
    "$$s^2 = \\frac{n}{n-k-1}{\\sigma^2_{MLE}}=\\frac{y^T(I - H)y}{n-k-1} = \\frac{\\epsilon^T(I - H)\\epsilon}{n-k-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution\n",
    "\n",
    "From above, we know that \n",
    "$$\\hat\\beta = (X^TX)^{-1}X^Ty=  \\beta+ (X^TX)^{-1}X^T\\epsilon\\sim \\mathcal N(\\beta, (X^TX)^{-1}\\sigma^2)$$\n",
    "and \n",
    "$$s^2 = \\Vert (I - H)\\epsilon\\Vert^2\\sim \\frac{1}{n-k-1}\\chi_{n-k-1}^2\\sigma^2.$$\n",
    "\n",
    "Note that $(X^TX)^{-1}X^T\\epsilon$ and $(I - H)\\epsilon$ are uncorrelated multivariate normal distributions, which thus implies independence, $\\hat\\beta $ and $s^2$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "Denote $e = [1,\\dotsc,1]^T\\in\\mathbb R^{n}$ and $n\\bar y =  e^Ty$. Recall $H = X(X^TX)^{-1}X^T$ is the hat matrix and $\\hat y = Hy$.\n",
    "\n",
    "### Total Sum of Squares\n",
    "\n",
    "The total sum of square is unrelated with the model. ${\\rm SST} = \\sum_{i=1}^n (y_i - \\bar y )^2$.  Also, we can write it in quadratic form, \n",
    "$${\\rm SST} = \\Vert y - \\frac 1n ee^Ty\\Vert^2 = y^T(I - \\frac 1n ee^T)y.$$\n",
    "\n",
    "We shall note that $I - \\frac 1nee^T$ is idempotent and ${\\rm tr}(I - \\frac 1n ee^T) = n-1$.\n",
    "\n",
    "\n",
    "\n",
    "### Regression Sum of Squares\n",
    "\n",
    "For regression sum of squares, it is \n",
    "\n",
    "$${\\rm SSR} = \\Vert \\hat y - \\frac 1n ee^Ty\\Vert^2 = \\Vert Hy - \\frac 1n ee^Ty\\Vert^2 = y^T(H - \\frac 1n ee^T)^2y.$$\n",
    "\n",
    "Recall that $HX = X$ and thus $He = e$ as $e$ is the first column of $X$, we can show that $H - \\frac 1n ee^T$ is idempotent and therefore, \n",
    "$${\\rm SSR} = y^T(H - \\frac 1n ee^T)y.$$\n",
    "\n",
    "And ${\\rm tr}(H - \\frac 1n ee^T) = k$.\n",
    "\n",
    "### Residual Sum of Squares\n",
    "\n",
    "For residual sum of squares, it is \n",
    "$${\\rm SSE} = \\Vert y - \\hat y \\Vert^2 = \\Vert y - Hy\\Vert^2 = y^T(I - H)y=(n-k-1)\\sigma^2.$$\n",
    "And $I - H$ is also idempotent with  ${\\rm tr}(I - H) = n - k - 1$.\n",
    "\n",
    "### Relations \n",
    "\n",
    "Still we have $${\\rm SST} = {\\rm SSE}+{\\rm SSR}$$\n",
    "and the orthogonality between ${\\rm SSE}$ and ${\\rm SSR}$: \n",
    "$(I - H)^T(H - \\frac 1nee^T) = 0$.\n",
    "\n",
    "Therefore, ${\\rm SSR} = \\Vert (H - \\frac 1n ee^T)y\\Vert^2$ and ${\\rm SSE} = \\Vert (I - H)y\\Vert^2$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$\n",
    "\n",
    "The coefficient of determination is given by $R^2 = 1 - \\dfrac{\\rm SSE}{\\rm SST} = \\dfrac{\\rm SSR}{\\rm SST}$. And we call $\\rho = \\sqrt {R^2}$ the multiple correlation coefficient.\n",
    "\n",
    "Observe that \n",
    "$$\\frac{{\\rm Cov}(y,\\hat y)^2}{{\\rm Var}(y){\\rm Var}(\\hat y)}\n",
    "=\\frac{\\left((y  - \\frac 1nee^Ty)^T(Hy - \\frac 1n ee^THy)\\right)^2}{\\Vert y - \\frac 1n ee^Ty\\Vert^2\\Vert Hy - \\frac 1n ee^THy\\Vert^2}=\n",
    " \\frac{\\left(y^T(H - \\frac 1nee^T)y\\right)^2 }{\\Vert y - \\frac 1n ee^Ty\\Vert^2y^T(H - \\frac 1nee^T)y}=\\frac{\\rm SSR}{\\rm SST} = R^2,$$\n",
    "we conclude that $R^2$ characterizes the correlation between $y$ and $\\hat y$.\n",
    "\n",
    "### Adjusted $R^2$\n",
    "\n",
    "$R^2$ is not larger the better. Because counting in more factors, even rubbish factors, will reduce ${\\rm SSR}$, leading to the increase in $R^2$. We can fix the problem by introducing adjusted $R^2$, which is $R_a^2$ defined as below. \n",
    "\n",
    "$$R_a^2 = 1 - \\frac{{\\rm SSE}/(n - k - 1)}{{\\rm SST} / (n - 1)}.$$\n",
    "\n",
    "In this case, larger number of factors, $k$, will penalize the $R_a^2$.\n",
    "\n",
    "\n",
    "### Extra Sum of Squares\n",
    "\n",
    "As mentioned, introducing new factors, useful or not, can reduce ${\\rm SSE}$ (or equivalently, increast ${\\rm SSR}$). But useful ones often reduce it more. For current $X_1\\in\\mathbb R^{n\\times(k_1+1)}$ and a new set of factors $X_2\\in\\mathbb R^{k_2}$, we can merge them to $X = [X_1,X_2]\\in\\mathbb R^{n\\times (k_1+k_2+1)}$. \n",
    "\n",
    "If denote by ${\\rm SSE}(X_1)$ and ${\\rm SSE}([X_1,X_2])$ by the residual sum of squares of the model fitted on $X_1$ and $X=[X_1,X_2]$ respectively, we define the extra sum of squares by the decrease in ${\\rm SSE}$:\n",
    "\n",
    "$${\\rm SSR}(X_2|X_1) = {\\rm SSE}(X_1) - {\\rm SSE}([X_1,X_2]),$$\n",
    "or equivalently the increase in regression sum of squares, \n",
    "\n",
    "$${\\rm SSR}(X_2|X_1) =  {\\rm SSR}([X_1,X_2])-{\\rm SSR}(X_1).$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "It is important in practice to test whether a factor is indeed influential to the response.\n",
    "\n",
    "\n",
    "### Test All Coefficients\n",
    "\n",
    "When we want to test $H_0:\\ \\beta_1=\\dotsc =\\beta_k = 0$, except that $\\beta_0$ can be nonzero, against its opposite, we can test \n",
    "$$F=\\frac{\\rm MSR}{\\rm MSE} =\\frac{{\\rm SSR}/k}{{\\rm SSE}/(n-k-1)}=\\frac{\\sigma^2\\chi_k^2}{\\sigma^2\\chi_{n-k-1}^2}\\sim F_{k,n-k-1}.$$\n",
    "\n",
    "We reject $H_0$ when $F$ is large enough, since this implies that the regression fits much and the factors are not nonsense.\n",
    "\n",
    "\n",
    "### Test Single Coefficient\n",
    "\n",
    "When we want to test whether one of the factors contributes, say, $H_0:\\beta_1 = 0$ against $H_1:\\beta_1\\neq 0$, we can recall that $\\hat\\beta_1 \\sim N(\\beta_1, \\sigma^2e_1^T(X^TX)^{-1}e_1)$ where $e_1 = [1,0,\\dotsc,0]^T\\in\\mathbb R^{k+1}$. And thus under the assumption $\\beta_1 = 0$, we have\n",
    "$$\\frac{\\hat\\beta_1 }{\\sqrt{e_1^T(X^TX)^{-1}e_1 s^2}}\\sim \\frac{\\sigma^2 N(0,1)}{\\sigma^2\\sqrt{\\frac{\\chi_{n-k-1}^2}{n-k-1}}}\n",
    "\\sim t_{n-k-1}.\n",
    "$$\n",
    "\n",
    "We reject $H_0$ when $t$ is far from origin, i.e. $|t|$ being \"abnormally\" large indicates $\\beta_1$ does not vanish.\n",
    "\n",
    "### Test Linearity of Coefficients\n",
    "\n",
    "More generally, if we want to test whether $C\\beta = 0$ where $C\\in\\mathbb  R^{m\\times (k+1)}$, we can use the generalized extra sum of squares: find the best, reduced, model under constraint $H_0:C\\beta = 0$ and compare it with the full model. \n",
    "\n",
    "**Theorem** Let ${\\rm SSE}_R$ and ${\\rm SSE}_F$ be the SSE of the model with and without constraint $C\\beta = 0$, then under assumption $H_0$ we have \n",
    "$${\\rm SSE}_R - {\\rm SSE}_F \\sim \\sigma^2 \\chi_m^2.$$\n",
    "\n",
    "Sometimes we approximate $\\sigma^2$ by $\\frac{1}{n-k-1}{\\rm SSE}_F$ so that we test \n",
    "$$\\frac{({\\rm SSE}_R - {\\rm SSE}_F) / m}{{\\rm SSE}_F / (n - k - 1)} \\sim F_{m,n-k-1}.$$\n",
    "\n",
    "**Proof** First we derive the best reduced model ${\\rm argmin}_\\beta\\left\\{\\Vert y - X\\beta\\Vert^2:\\ C\\beta = 0\\right\\}$. Lagrange dual shows that it is equivalent to solving for\n",
    "$$\\begin{aligned}&\\sup_{\\hat \\mu} \\inf_{\\hat \\beta} \\left\\{\\Vert y - X\\hat\\beta\\Vert^2+2 \\hat\\mu^T C\\hat\\beta \\right\\}\n",
    "=\\sup_{\\hat \\mu} \\left\\{ y^Ty - (\\hat\\mu^T C - y^TX)(X^TX)^{-1}( C^T\\hat\\mu -X^Ty)\\right\\}.\n",
    "\\end{aligned}$$\n",
    "The extrema is reached when $\\hat\\mu = (C(X^TX)^{-1}C^T)^{-1}C(X^TX)^{-1}X^Ty$ and $\\hat \\beta_R = (X^TX)^{-1}(X^Ty-C^T\\mu)$.\n",
    "\n",
    "Thus, the SSE of the reduced model is given by (note that $(I - H)X = 0$)\n",
    "$$\\begin{aligned}{\\rm SSE}_R&=\\Vert y - X\\hat \\beta_R\\Vert^2\n",
    "=\\Vert \\left((I - X(X^TX)^{-1}X^T + X(X^TX)^{-1}C^T(C(X^TX)^{-1}C^T)^{-1}C(X^TX)^{-1}X^T\\right)y\\Vert^2\n",
    "\\\\ &= y^T (I - H + 2(I - H)X(X^TX)^{-1}C^T(C(X^TX)^{-1}C^T)^{-1}C(X^TX)^{-1}X^T\n",
    "\\\\ &\\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad + X(X^TX)^{-1}C^T(C(X^TX)^{-1}C^T)^{-1}C(X^TX)^{-1}X^T )y\n",
    "\\\\ &= y^T\\left(I - H + X(X^TX)^{-1}C^T(C(X^TX)^{-1}C^T)^{-1}C(X^TX)^{-1}X^T \\right)y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, the extra sum of squares is given by\n",
    "$$\\begin{aligned}{\\rm SSE}_R - {\\rm SSE}_F &= y^T  X(X^TX)^{-1}C^T(C(X^TX)^{-1}C^T)^{-1}C(X^TX)^{-1}X^T y\n",
    "\\\\ &= \\Vert (C(X^TX)^{-1}C^T)^{-\\frac 12}C(X^TX)^{-1}X^T (X\\beta +\\epsilon)\\Vert^2\n",
    "\\\\ &= \\Vert (C(X^TX)^{-1}C^T)^{-\\frac 12} (C\\beta +C(X^TX)^{-1}X^T\\epsilon)\\Vert^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When assuming $H_0:\\ C\\beta = 0$, we can see that \n",
    "$${\\rm SSE}_R - {\\rm SSE}_F=\\epsilon^T X(X^TX)^{-1}C^T(C(X^TX)^{-1}C^T)^{-1}C(X^TX)^{-1}X^T \\epsilon=\\epsilon^TW\\epsilon.\n",
    "$$\n",
    "\n",
    "Note that $W$ is idempotent and ${\\rm rank}(W) = {\\rm tr}(W) =m $, by Cochran's theorem we learn ${\\rm SSE}_R - {\\rm SSE}_F\\sim \\sigma^2\\chi_m^2$.\n",
    "\n",
    "Lastly, ${\\rm SSE}_R - {\\rm SSE}_F$ and ${\\rm SSE}_F$ are independent (which thus implies independence with $s^2$) since \n",
    "$$ (I - H)X(X^TX)^{-1}C^T(C(X^TX)^{-1}C^T)^{-1}C(X^TX)^{-1}X^T  = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "In the covariates $X\\in\\mathbb R^{n\\times (k+1)}$ where there are $k$ different factors, some of them might be redundant. If we $k$ is very large ($k=O(n)$) and have used all $k$ of them, then the model might be overfitting and have a large variance. On the other hand, if we $k$ is too small, then the model might have a large balance. To conclude, it is important to extract essential factors.\n",
    "\n",
    "### All-Possible-Regression Selection\n",
    "\n",
    "Enumerate all subset of the $k$ factors, and select the subset with best criterion. The criterion could be $R^2$, $C_p$\n",
    "\n",
    "### $C_p$ Criterion\n",
    "\n",
    "Let $p$ be the number of factors in the reduced model (the interception term is not counted). We define\n",
    "$$C_p = \\frac{{\\rm SSE}_R}{{\\rm SSE}_F / (n - k - 1)}- (n - 2(p+1))$$\n",
    "where ${\\rm SSE}_R$ and ${\\rm SSE}_F$ are the residuals of the reduced and the full model respectively. \n",
    "\n",
    "**Theorem** Let $\\hat y_R$ be the prediction with the reduced model and $H_p\\in\\mathbb R^{(p+1)\\times (p+1)}$ be the corresponding hat matrix, then \n",
    "$$\\mathbb E\\left\\{\\frac{{\\rm SSE}_R}{\\sigma^2}- (n - 2(p+1))\\right\\} =   \\frac{\\mathbb E\\Vert  \\hat y_R-\\mathbb E(y)\\Vert^2 }{\\sigma^2}$$\n",
    "\n",
    "**Proof** \n",
    "$$\\mathbb E\\Vert  \\hat y_R-\\mathbb E(y)\\Vert^2\n",
    "=\\mathbb E \\Vert H_p(X\\beta+\\epsilon) - X\\beta\\Vert^2=\\beta^TX^T(I -H_p)X\\beta+\\sigma^2(p+1)$$\n",
    "Combining $\\mathbb E({\\rm SSE}_R)=\\sigma^2(n-p-1)$ and $n-p-1-(n-2(p+1))=p+1$ yields the result.\n",
    "\n",
    "\n",
    "### AIC Criterion\n",
    "\n",
    "Let $L$ be the log-likelihood of the maximum likelihood estimator, then we define\n",
    "$${\\rm AIC}= -\\frac 2n\\log L + \\frac {2p}{n}.$$\n",
    "The smaller ${\\rm AIC}$  the better.\n",
    "\n",
    "\n",
    "**Theorem** When assuming $\\epsilon\\sim N(0,\\sigma^2I_n)$, minimizing ${\\rm AIC}$ is to maximize $n\\log {\\rm SSE}+2p$.\n",
    "\n",
    "**Proof** Recall that $\\hat\\sigma_{MLE}^2= \\frac 1n {\\rm SSE}$,\n",
    "$$\\log L = -\\frac {1}{2\\sigma^2}  \\Vert y - \\hat y \\Vert^2+n\\log\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n",
    "=-\\frac n2-\\frac n2\\log {\\rm SSE} +{\\rm Const}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4 (tags/v3.8.4:dfa645a, Jul 13 2020, 16:46:45) [MSC v.1924 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
