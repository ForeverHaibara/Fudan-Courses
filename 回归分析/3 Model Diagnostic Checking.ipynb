{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Diagnostic Checking\n",
    "## VIF\n",
    "\n",
    "Assume we are doing multivariate linear regression $y = X\\beta +\\epsilon$ where $X\\in\\mathbb R^{n\\times (k+1)}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "We have assumed that the data $X$ is of full column rank. However, when $X$ is not full rank, or has small singular values, it implies underlying relation between features. This is called the multicolinearity. We should detect this to avoid mistakes.\n",
    "\n",
    "\n",
    "To test multicolinearity, we can extract feature $j\\ (1\\leqslant j\\leqslant k)$, and fit a linear mdoel to column $x_{*j}$ with the rest of data $[x_{*0},x_{*1},\\dotsc,x_{*(j-1)},x_{*(j+1)},\\dotsc,x_{*n}]$. If the model has a high $R^2$, then we know that feature $x_{*j}$  might have colinearity with other features.\n",
    "\n",
    "Explicitly, we can define VIF (variance inflation factor) to be\n",
    "$${\\rm VIF}_j = \\frac{1}{1 - R_j^2} = \\frac{1}{1 - \\frac{{\\rm SSR}_j}{{\\rm SST}_j}}=\\frac{{\\rm SST}_j}{{\\rm SSE}_j}$$\n",
    "where $R_j^2$ is the $R^2$ of the regression on $x_{*j}$ with the remaining features.\n",
    "\n",
    "<br>\n",
    "\n",
    "When there is no multicolinearity, then $R_j = 0$ and ${\\rm VIF}_j = 1$. When there is prominent multicolinearity, then $R_j^2\\rightarrow 1$ and ${\\rm VIF}_j\\rightarrow +\\infty$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conditional Variance\n",
    "\n",
    "**Theorem** Denote $X_j\\in\\mathbb R^n$ to be the $(j+1)$-th column of $X$ (do not forget that the first column is full of ones)\n",
    "$${\\rm VIF}_j = \\frac{{\\rm Var}X_j}{{\\rm Var}(X_j|X_0,X_1,\\dotsc,X_{j-1},X_{j+1},\\dotsc,X_k)}$$\n",
    "\n",
    "\n",
    "**Proof** On the one hand, ${\\rm Var}  X_j = \\frac{1}{n-1}\\sum_{i=1}^n (x_{ij} - \\bar x_{*j})^2=\\frac{1}{n-1}{\\rm SST}_j$. On the other, the denominator refers to the MSE of fitting $X_j$ linearly with the remaining $k-1$ features, which is $\\frac{1}{n-1}{\\rm SSE}_j$.\n",
    "\n",
    "### Inverse Entry\n",
    "\n",
    "**Theorem** Denote $X_j\\in\\mathbb R^n$ to be the $(j+1)$-th column of $X$ (do not forget that the first column is full of ones), then $\\frac{1}{\\rm SSE_j}$ is the $(j+1,j+1)$ entry of the matrix $(X^TX)^{-1}$.\n",
    "\n",
    "\n",
    "**Proof** Without loss of generality we may assume $j+1$ is the last column of $X$. And we can parition $X\\in\\mathbb R^{n\\times (k+1)}$ by $[U,v]$ where $U\\in\\mathbb R^{n\\times k}$ and $v\\in\\mathbb R^n$. On the one hand, \n",
    "$${\\rm SSE}_j= \\Vert v - U(U^TU)^{-1}U^Tv\\Vert^2= v^T(I - U(U^TU)^{-1}U^T)v. $$\n",
    "\n",
    "On the other hand, $X^TX = \\left[\\begin{matrix} U^TU & U^Tv \\\\ v^TU & v^Tv\\end{matrix}\\right]$. Using the idea of Schur complement we learn \n",
    "\n",
    "$$X^TX = \\left[\\begin{matrix} I &  \\\\ v^TU(U^TU)^{-1} & 1  \\end{matrix}\\right]\n",
    "\\left[\\begin{matrix}  U^TU &   \\\\   & v^T(I - U(U^TU)^{-1}U^T)v\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix} I &   (U^TU)^{-1} U^Tv \\\\ & 1  \\end{matrix}\\right].$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$(X^TX)^{-1}= \\left[\\begin{matrix} I &   -(U^TU)^{-1} U^Tv \\\\ & 1  \\end{matrix}\\right] \n",
    "\\left[\\begin{matrix}  (U^TU)^{-1} &  \\\\   & \\left[v^T(I - U(U^TU)^{-1}U^T)v\\right]^{-1}\\end{matrix}\\right]\\left[\\begin{matrix} I &  \\\\ -v^TU(U^TU)^{-1} & 1  \\end{matrix}\\right]\n",
    ".$$\n",
    "\n",
    "And it is clear that the bottom-right entry of $(X^TX)^{-1}$ is $ \\left[v^T(I - U(U^TU)^{-1}U^T)v\\right]^{-1}$, which is $1/{\\rm SSE}_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06239734207048804 0.06239734207048801\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.randn(10,5)\n",
    "\n",
    "# compute the conditional variance of the last column\n",
    "SSE = ((X[:,-1] - X[:,:-1] @ np.linalg.solve(X[:,:-1].T @ X[:,:-1], X[:,:-1].T @ X[:,-1])) ** 2).sum()\n",
    "\n",
    "# the two should be equal\n",
    "print(1 / SSE, np.linalg.inv(X.T @ X)[-1,-1]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heteroskedasticity\n",
    "\n",
    "Sometimes the noise $\\epsilon$ has different variance, this is called heteroskedasticity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series \n",
    "\n",
    "Sometimes the noise $\\epsilon$ are correlated, e.g. time series.\n",
    "\n",
    "\n",
    "### Durbin-Watson Test \n",
    "\n",
    "Since $\\rho = \\frac{{\\rm Cov}(\\epsilon_t,\\epsilon_{t-1})}{\\sqrt{{\\rm Var}(\\epsilon_t) {\\rm Var}(\\epsilon_{t-1})}}= \\frac{\\mathbb E(\\epsilon_t\\epsilon_{t-1})}{\\sqrt{\\mathbb E(\\epsilon_t^2)\\mathbb E(\\epsilon_{t-1}^2)}}$, we can use the following to estimate the autocorrelation:\n",
    "$$1 - \\frac{\\sum_{t=2}^n (\\hat\\epsilon_t - \\hat\\epsilon_{t-1})^2}{2\\sum_{t=2}^n \\hat\\epsilon_t^2}\n",
    "=\\frac{\\sum_{t=2}^n \\hat\\epsilon_t \\hat\\epsilon_{t-1}}{\\sum_{t=2}^n\\hat\\epsilon_t^2}\\approx \\rho.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
