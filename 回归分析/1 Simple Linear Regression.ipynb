{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Simple Linear Regression\n",
    "\n",
    "**Example** As shown below, there are some data $(X_i,Y_i)$ where $X_i,Y_i\\in\\mathbb R$ and it has a linear pattern. We can fit it with simple linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs4AAAIMCAYAAADlzCqTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqA0lEQVR4nO3dfZTddX0v+veXELxTrzRWKSWxtbaVaCVKIKWt9kGwNfWsI6RIe2ypWnSJF25VRKNGoMflwwEMHB+uikt8wFMfjkLTiK0YPRrPXaddgIEgETwRebB1ApbWBAt3GiF+7x97D0yGmeQ3M3vPfnq91srKzG/23vOdLzt73vzmM793qbUGAAA4sEN6vQAAABgEgjMAADQgOAMAQAOCMwAANCA4AwBAA4IzAAA00Cg4l1JeV0q5pZTyrVLKZ0op/0cp5SmllOtKKd8tpXy2lHJYtxcLAAC9ctDgXEpZkeQ1SdbUWo9JsiTJi5NcnOTdtdZfSbI7ySu6uVAAAOilpqMahyYZK6UcmuSnktyd5KQkV7U//okk6zq+OgAA6BMHDc611vEklyT5x7QC831Jbkiyp9b6UPtm30+yoluLBACAXjv0YDcopTw+ySlJnpJkT5Irk/xB009QSjkzyZlJ8tjHPvb4pz3tafNaKAAANHXDDTf8S631iE4+5kGDc5LfS3JnrfXeJCmlbErynCTLSimHts86PynJ+Ex3rrV+OMmHk2TNmjV127ZtHVk4AADMppTyvU4/ZpMZ539M8hullJ8qpZQkz0tya5KtSU5r3+ZlST7f6cUBAEC/aDLjfF1avwR4Y5Id7ft8OMmbkpxbSvlukick+WgX1wkAAD3VZFQjtdb/nOQ/Tzt8R5ITOr4iAADoQ5oDAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoAHBGQAAGhCcAQCgAcEZAAAaEJwBAKABwRkAABoQnAEAoIFDD3aDUsrKJJ+dcuiXkvxlkmVJXpnk3vbxt9Rav9jpBQIAcGCbt49n45ad2bVnIsuXjWX92pVZt3pFr5c1dA4anGutO5McmySllCVJxpP8TZIzkry71npJNxcIAMDsNm8fz4ZNOzLx4L4kyfieiWzYtCNJhOcOm+uoxvOS3F5r/V43FgMAwNxs3LLz4dA8aeLBfdm4ZWePVjS85hqcX5zkM1Pe/4tSys2llI+VUh4/0x1KKWeWUraVUrbde++9M90EAIB52rVnYk7Hmb/GwbmUcliSk5Nc2T50WZJfTmuM4+4kl850v1rrh2uta2qta4444oiFrRYAgP0sXzY2p+PM31zOOL8gyY211h8kSa31B7XWfbXWnyS5PMkJ3VggAACzW792ZcaWLtnv2NjSJVm/dmWPVjS8DvrLgVP8SaaMaZRSjqq13t1+9w+TfKuTCwMA4OAmfwHQVTW6r1FwLqU8NsnvJ3nVlMPvKqUcm6QmuWvaxwAAWCTrVq8QlBdBo+Bca30gyROmHXtJV1YEAAB9SHMgAAA0IDgDAEADgjMAADQgOAMAQAOCMwAANCA4AwBAA4IzAAA0IDgDAEADgjMAADQgOAMAQAOCMwAAw2VioisPKzgDADAc9u1LPvSh5MlP7srDC84AAAy+r389Oe645Kyzkqc/vSufQnAGAGBw3XVX8kd/lJx4YrJnT/K5z7VCdBcc2pVHBQCAbnrggeSii5JLLklKSd72tuQNb0jGxrr2KQVnAAAGR63JZz6TvPGNyfh48qd/2grQP//zXf/URjUAABgM27Ylv/VbyemnJ0cemfyv/5V86lOLEpoTwRkAgH53zz3Jy1+enHBC8t3vJh/9aPKNbyTPec6iLsOoBgAA/Wnv3uR970ve/vbk3/89ef3rkwsuSA4/vCfLEZwBAOgvtSZ/+7fJuee2zjD/x/+YXHppcvTRPV2WUQ0AAPrHt7+dvOAFycknJ4cemlxzTfKFL/Q8NCeCMwAA/WD37uScc5JVq5Jrr03e857k5puTP/iDXq/sYUY1AADonX37kssvT84/P/nhD5Mzz2zNNB9xRK9X9ijOOAMA0BtTa7Kf8YzkxhuTD32oL0NzIjgDALDYZqvJPvbY3q7rIIxqAACwOHpQk91JgjMAAN3Vw5rsTjKqAQBA9/S4JruTBGcAADqvT2qyO8moBgAAndNnNdmdJDgDALBwfVqT3UlGNQAAWJhbb201/PVhTXYnCc4AAMzPZE32M5+ZXHddX9Zkd5JRDQAA5maAarI7yRlnAACaG7Ca7E4SnAEAOLipNdn33ZdceeVA1GR3klENAABmN1mTvXFjcsghA1eT3UmCMwAAjzYkNdmdZFQDAID9DVFNdicJzgAAtAxhTXYnGdUAABh102uy3/CG1qXmhqAmu5MEZwCAUTVTTfZ//a/JU5/a65X1JaMaAACjaLaabKF5VoIzAMAoGbGa7E4yqgEAMApGtCa7k5xxBgAYdlNrso85ZqRqsjtJcAYAGFYz1WRv3TpSNdmdZFQDAGDYqMnuCsEZAGBYqMnuKqMaAADDQE121wnOAACDTE32ojGqAQAwiNRkLzrBGQBgkKjJ7hmjGgAAg0JNdk8JzgAA/U5Ndl8wqgEA0K/UZPcVZ5wBAPqRmuy+IzgDAPQTNdl9y6gGAEA/UJPd9wRnAIBeUpM9MIxqAAD0iprsgSI4AwAsNjXZA8moBgDAYtm7N3nve5N3vENN9gASnAEAuk1N9lAwqgEA0E1qsoeG4AwA0A1qsoeOUQ0AgE6aWpO9e3fyyleqyR4SzjgDAHTK9JrsG25Qkz1EBGcAgIVSkz0SjGoAAMyXmuyRIjgDAMyVmuyRZFQDAGAuptZk/9zPqckeIYIzAEATM9VkX3+9muwRYlQDAOBA1GTTJjgDAMxETTbTGNUAAJhOTTYzEJwBACapyeYAjGoAAEyvyT7zzNY1mTX+MYUzzgDAaJupJvuyy4RmHkVwBgBGk5ps5sioBgAwWtRkM0+CMwAwGtRks0BGNQCA4acmmw4QnAGA4TVZk/1rv6YmmwUzqgEADJ/pNdnr16vJZsEEZwBgeKjJpouMagAAw0FNNl0mOAMAg01NNovEqAYAMJjUZLPIDnrGuZSyspRy05Q/PyqlnFNK+ZlSyldKKbe1/378YiwYACBf/3rue/qq5Kyzcu1PHZU//78vy+Yzzxea6aqDnnGute5McmySlFKWJBlP8jdJ3pzkq7XWi0opb26//6buLRUAGHl33dW6QsZVV+X+n/7ZvPmUN+ealc9JSsl1m3YkSdatXtHbNTK05jrj/Lwkt9dav5fklCSfaB//RJJ1HVwXAMAjHnggueCC5GlPS/7u73L57/15TnrFZbnmab+VlJIkmXhwXzZu2dnjhTLM5hqcX5zkM+23j6y13t1++54kR850h1LKmaWUbaWUbffee+88lwkAjKRak09/Olm5snVN5he9KNm5M//l+NOyd+ljHnXzXXsmerBIRkXj4FxKOSzJyUmunP6xWmtNUme6X631w7XWNbXWNUeYOwIAmjpATfbyZWMz3mW249AJcznj/IIkN9Zaf9B+/wellKOSpP33P3d6cQDACGpQk71+7cqMLV2y393Gli7J+rUrF3u1jJC5BOc/ySNjGklydZKXtd9+WZLPd2pRAMAI2rs3ede7kqOPTj75ydYvAd52WytEH7J/ZFm3ekUuPHVVViwbS0myYtlYLjx1lV8MpKtKa8riIDcq5bFJ/jHJL9Va72sfe0KSzyX5hSTfS/LHtdYfHuhx1qxZU7dt27bgRQMAQ0RNNl1QSrmh1rqmk4/ZqACl1vpAkidMO/avaV1lAwBgfm69NXnd65Ivf7l1xYxrrtH4R99SuQ0ALD412QwgldsAwOJRk80Ac8YZAFgcX/96ctxxyVlnJccck9xwQ3LZZUIzA0NwBgC66667kj/6o+TEE5P77kuuvDLZujU59therwzmxKgGANAdDzyQXHRRsnFjsmRJ8va3J69/fTKmpITBJDgDAJ01WZP9pjcl4+PJn/5pcvHFyZOe1OuVwYIY1QAAOmfbtlbD35/92f412UIzQ0BwBgAWbmpN9u23z1iTDYPOqAYAMH979ybvfW/yjnck//7vrZrs889PDj+81yuDjhOcAYC5m16T/cIXJpdeqiaboWZUAwCYm1tvbTX8nXxysnRp8qUvJVdfLTQz9ARnAKCZ3buT1752/5rsb34zWbu21yuDRWFUAwA4MDXZkMQZZwDgQNRkw8MEZwDg0dRkw6MY1QAAHqEmG2YlOAMAarKhAaMaADDq1GRDI4IzAIwqNdkwJ0Y1AGDUqMmGeRGcAWBUqMmGBTGqAQCjQE02LJjgDADDTE02dIxRDQAYRmqyoeOccQaAYaMmG7pCcAaAYaEmG7rKqAYADDo12bAoBGcAGFRqsmFRGdUAgEGkJhsWneAMAINETTb0jFENABgEarKh5wRnAOhnarKhbxjVAIB+pSYb+orgDAD9Rk029CWjGgDQL9RkQ19zxhkA+sH0muwbb1STDX1GcAaAXpqtJvtZz+r1yoBpjGoAQC+oyYaBIzgDwGJSkw0Dy6gGACwWNdkw0ARnAOi2qTXZd9yRfOxjarJhABnVAIBumV6T/cY3JuedpyYbBpTgDACdpiYbhpJRDQDoJDXZMLQEZwDoBDXZMPSMagDAQqjJhpHhjDMAzJeabBgpgjMAzNWddyannaYmG0aM4AwATT3wQGsk4+lPT665plWT/e1vt0J0Kb1eHdBlZpwB4GDUZANxxhkADkxNNtAmOAPATNRkA9MY1QCAqdRkA7MQnAEgac0xf+ELrZrs229Xkw08ilENAJisyT7llOSww9RkAzMSnAEYXWqygTkwqgHA6FGTDcyDM84AjJatW9VkA/MiOAMwGiZrsk86SU02MC+CMwDDTU020CFmnAEYTmqygQ5zxhmA4aMmG+gCwRmA4aEmG+gioxoADD412cAiEJwBGFxqsoFFZFQDgMGkJhtYZIIzAINlak329de3RjTUZAOLwKgGAINBTTbQY844A9D/1GQDfUBwBqB/qckG+ohRDQBmtXn7eDZu2ZldeyayfNlY1q9dmXWrV3T/Ez/wQHLhhckllyRLlrRqsl//+mRsrPufG2AWgjMAM9q8fTwbNu3IxIP7kiTjeyayYdOOJOleeJ5ek3366clFF2n8A/qCUQ0AZrRxy86HQ/OkiQf3ZeOWnd35hFNrso86Kvn7v08++UmhGegbgjMAM9q1Z2JOx+dtpprs665Lnv3szn4egAUSnAGY0fJlM88Tz3Z8zvbuTd71rlZhySc/2arJ/s53kjPOSA7x7QnoP16ZAJjR+rUrM7Z0yX7HxpYuyfq1Kxf2wLW2Gv6e8YzWLPOJJya33JJcfHFy+OELe2yALhKcAZjRutUrcuGpq7Ji2VhKkhXLxnLhqasW9ouBarKBAeaqGgDMat3qFZ25gsbu3clb35p84APJ4x7Xqsk+66xk6dKFPzbAIhGcAeiehx5q1WRfcIGabGDgGdUAoDu2bk2OPz45+2w12cBQEJwB6Cw12cCQEpwB6IwHHkjOPz95+tOTa65p1WR/+9utEF1Kr1cHsGBmnAFYGDXZwIhwxhmA+VOTDYwQwRmAuVOTDYwgoxoANLd3b+sazG9/e+vtN74xOe88jX/ASBCcATi4WpMvfCE599zk9tuTF74wufRSjX/ASDGqAcCBqckGSCI4AzCb3buT1742eeYzk+uvb41ofPObydq1vV4ZQE80GtUopSxL8pEkxySpSV6eZG2SVya5t32zt9Rav9iFNQIsqs3bx7Nxy87s2jOR5cvGsn7tyqxbvWJ01qEmG2BGTWec35vkS7XW00ophyX5qbSC87trrZd0bXUAi2zz9vFs2LQjEw/uS5KM75nIhk07kmRRw3PP1rF1a3LOOcnNNye/+7uts8wa/wCSNBjVKKX8dJLfSfLRJKm1/rjWuqfL6wLoiY1bdj4cVidNPLgvG7fsHO51qMkGOKgmM85PSWsc4+OllO2llI+UUh7b/thflFJuLqV8rJTy+JnuXEo5s5SyrZSy7d57753pJgB9Y9eeiTkdH/h1qMkGaKxJcD40yXFJLqu1rk7yQJI3J7ksyS8nOTbJ3UkunenOtdYP11rX1FrXHGE+Duhzy5eNzen4wK6j1uRTn0pWrkze+c5WUN65sxWixxb3awUYFE2C8/eTfL/Wel37/auSHFdr/UGtdV+t9SdJLk9yQrcWCbBY1q9dmbGlS/Y7NrZ0SdavXTk86/jGN9RkA8zDQYNzrfWeJP9USpl8tX5ekltLKUdNudkfJvlWF9YHsKjWrV6RC09dlRXLxlKSrFg2lgtPXbXoV9XoyjruuSc544zkhBPUZAPMQ6m1HvxGpRyb1uXoDktyR5IzkrwvrTGNmuSuJK+qtd59oMdZs2ZN3bZt24IWDMAcTa/Jft3r1GQDQ6+UckOtdU0nH7PR5ehqrTclmf6JX9LJhQDQYWqyATpKcyDAMFKTDdBxgjPAMFGTDdA1TZsDAehnarIBus4ZZ4BBt3VrcvzxydlnJ8cck9x4Y3LZZUIzQIcJzgCDanpN9lVXqckG6CLBGWDQzFaT/aIXqckG6CIzzgCDotbk059O3vSmZHw8Of305KKLNP4BLBJnnAEGgZpsgJ4TnAH6mZpsgL5hVAOgH02vyX7jG9VkA/SY4AzQT6bXZJ98cnLJJRr/APqAUQ2AfjFTTfbnPy80A/QJwRmg19RkAwwEoxoAvaImG2CgOOMM0AtTa7JXrVKTDTAABGeAxTRTTfbXvqYmG2AACM4Ai0FNNsDAM+MM0E1qsgGGhjPOAN2iJhtgqAjOAJ2mJhtgKBnVAOgUNdkAQ01wBlgoNdkAI8GoBsBCqMkGGBmCM8B8qMkGGDlGNQDmQk02wMhyxhmgKTXZACNNcAY4GDXZAERwBpjd/feryQbgYWacAaarNfnUp1o12bt2qckGIIkzzgD7m6zJfslLkuXL1WQD8DBnnAGSVk32hg3JFVckRx7Zqsl+2cuSQ0b7/MLm7ePZuGVndu2ZyPJlY1m/dmXWrV7R62X1hL0ABGdgtKnJntXm7ePZsGlHJh7clyQZ3zORDZt2JMnIBUZ7ASRGNYBRVWty9dXJM57RmmU+6aTklluSiy8Wmts2btn5cFCcNPHgvmzcsrNHK+odewEkgjMwitRkN7Jrz8Scjg8zewEkgjMwStRkz8nyZWNzOj7M7AWQCM7AKHjooVbD31Ofmrz//a2a7NtuS17zmmTp0l6vrm+tX7syY0uX7HdsbOmSrF+7skcr6h17ASR+ORAYdlu3Jueck9x8c/Lc57bOMj/zmb1e1UCY/KU3V5KwF0BLqbUu2idbs2ZN3bZt26J9PmCE3Xlnsn598td/nTz5ycmllyannqrxD2BElFJuqLWu6eRjGtUAhouabAC6xKgGMBzUZAPQZc44A4NPTTYAi0BwBgbXPfckZ5yRnHBCcscdycc/nlx3XfLsZ/d6ZQAMIaMawOBRkw1ADwjOwOCoNfnCF5Jzz01uvz05+eTkkks0/gGwKIxqAIPh1ltbDX9qsgHoEcEZ6G9Ta7K/8Q012QD0jFENoD899FBy+eXJBRe0wvOrXpW87W3JE5/Y65UBMKKccQb6z9atyfHHJ2efnaxalWzfnnzwg0IzAD0lOAP94847k9NOS046KbnvvuSqq5Kvfa01pgEAPSY4A72nJhuAAWDGGegdNdkADBBnnIHeUJMNwIARnIHFpSYbgAFlVANYHGqyARhwgjPQXWqyARgSRjWA7plek71li5psAAaW4Ax03mw12c9/fq9XBgDzZlQD6Bw12QAMMWecgc5Qkw3AkBOcgYVRkw3AiBCcgflRkw3AiDHjDMyNmmwARpQzzkBzarIBGGGCM3BwarIBwKgGcABqsgHgYYIz8GhqsgHgUYxqAPtTkw0AMxKcgRY12QBwQEY1YNSpyQaARgRnGGVbt7bOMu/YkTz3ua2zzD1u/Nu8fTwbt+zMrj0TWb5sLOvXrsy61St6uiYASIxqwGi6885Ww99JJyU/+lHf1GRv3j6eDZt2ZHzPRGqS8T0T2bBpRzZvH+/pugAgEZxhtEytyf7Sl5J3vKOvarI3btmZiQf37Xds4sF92bhlZ49WBACPMKoBo2CmmuyLL05W9NcIxK49E3M6DgCLyRlnGHaz1WT3WWhOkuXLxuZ0HAAWk+AMw+ruuweuJnv92pUZW7pkv2NjS5dk/dqVPVoRADzCqAYMm717k/e8pzW/PGA12ZNXz3BVDQD6keAMw2JIarLXrV4hKAPQl4xqwDBQkw0AXSc4wyBTkw0Ai8aoBgwiNdkAsOiccYZBs3VrctxxydlnJ6tWJdu3Jx/8oNAMAF0mOMOgmFqT/W//1jc12QAwKoxqQL+7//7kootaV8hYsqR1mblzz03GlIIAwGISnKFfDUhNNgCMCqMa0I8GqCYbAEaF4Az9ZABrsgFgVBjVgH4wtSb7xz9ujWe85S0DUZMNAKNCcIZemqkm+9JLk1/5lV6vDACYxqgG9MpsNdlCMwD0pUZnnEspy5J8JMkxSWqSlyfZmeSzSX4xyV1J/rjWursbi4Ru2bx9PBu37MyuPRNZvmws69euzLrVXf4FvN27k7e+NfnAB5LHPa5Vk33WWcnSpd39vADAgjQ94/zeJF+qtT4tybOSfDvJm5N8tdb61CRfbb8PA2Pz9vFs2LQj43smUpOM75nIhk07snn7eHc+4UMPJZddljz1qcn735+ceWZy223Ja14jNAPAADhocC6l/HSS30ny0SSptf641ronySlJPtG+2SeSrOvOEqE7Nm7ZmYkH9+13bOLBfdm4ZWfnP5mabAAYeE3OOD8lyb1JPl5K2V5K+Ugp5bFJjqy13t2+zT1JjpzpzqWUM0sp20op2+69997OrBo6YNeeiTkdnxc12QAwNJoE50OTHJfkslrr6iQPZNpYRq21pjX7/Ci11g/XWtfUWtccccQRC10vdMzyZTNXVs92fE7uvz85//zk6U9PvvSl1mXmbr21FaJLWfjjAwCLrklw/n6S79dar2u/f1VaQfoHpZSjkqT99z93Z4nQHevXrszY0iX7HRtbuiTr166c/4PW2mr4W7kyeec7k9NOS77zneS885KxDgRyAKBnDhqca633JPmnUspkmnhekluTXJ3kZe1jL0vy+a6sELpk3eoVufDUVVmxbCwlyYplY7nw1FXzv6qGmmwAGGpNC1BeneRTpZTDktyR5Iy0QvfnSimvSPK9JH/cnSVC96xbvWLhl5+7++5Wy98VVyRHHtmqyX7pS5NDXCYdAIZJo+Bca70pyZoZPvS8jq4GBomabAAYKSq3Ya7UZAPASPKzZJgLNdkAMLIEZ2hi9+7kta9tXX/5G99o1WR/85vJ85/f65UBAIvEqAYcyEMPJZdfnlxwQSs8v+pVydvepvEPAEaQM84wGzXZAMAUgjNMN70m+6//Wk02ACA4w8Nmq8k+9VQ12QCAGWdIrcmnPtW6DvOuXcnppycXX6zxDwDYjzPOjDY12QBAQ4Izo+nuu5MzzkhOOCG5445WTfZ11yXPfnavVwYA9CmjGowWNdkAwDwJzowGNdkAwAIZ1WD4qckGADpAcGZ4/fCHyWteoyYbAOgIoxoMHzXZAEAXOOPMcFGTDQB0ieDMcFCTDQB0meDMYFOTDQAsEjPODKaf/CT59KfVZAMAi8YZZwbP9deryQYAFp3gzOCYrMn+9V9vzTSryQYAFpFRDfqfmmwAoA8IzvQvNdkAQB8xqkF/UpMNAPQZwZn+Mr0m+33vU5MNAPQFoxr0BzXZAECfc8aZ3lOTDQAMAMGZ3lGTDQAMEMGZxacmGwAYQGacWTxqsgGAASY4j6DN28ezccvO7NozkeXLxrJ+7cqsW93l8Hr99clrX5tce22yZk1y5ZUa/wCAgWJUY8Rs3j6eDZt2ZHzPRGqS8T0T2bBpRzZvH+/OJ1STDQAMCcF5xGzcsjMTD+7b79jEg/uyccvOzn6ivXtbYxhHH/3IeMZ3vpP8+Z8nh3jaAQCDx6jGiNm1Z2JOx+dMTTYAMKSc+hsxy5eNzen4nNxyi5psAGBoCc4jZv3alRlbumS/Y2NLl2T92pXzf9DJmuxnPUtNNgAwtIxqjJjJq2d05KoaarIBgBEiOI+gdatXLPzyc1u3ti4vt2NH8tznJu99r8Y/AGCoGdVgbtRkAwAjSnCmGTXZAMCIM6rBgU2vyf6zP0suukhNNgAwcpxxZnbXX5885znJS17SCsr/8A/JX/2V0AwAjCTBmUebqSb72muT3/zNXq8MAKBnjGrwiL17k/e8pzW//OMft8Yz3vKW5PDDe70yAICeE5xp1WRffXXy+teryQYAmIVRjVE3WZO9bl3ymMeoyQYAmIXgPKpmqsm+6SY12QAAszCqMWrUZAMAzIszzqNk69bkuOOSs89OVq1Ktm9PPvhBoRkAoAHBeRSoyQYAWDDBeZipyQYA6BgzzsNITTYAQMc54zxs1GQDAHSF4Dws1GQDAHSVUY1BpyYbAGBRCM6DanpN9imnJJdcovEPAKBLjGoMouk12V/+crJ5s9AMANBFgvMgma0m+/d/v9crAwAYekY1BoGabACAnnPGud+pyQYA6AuCc79Skw0A0FcE535z//3JeeepyQYA6DNmnPuFmmwAgL7mjHM/UJMNAND3BOdeUpMNADAwjGr0gppsAICBIzgvJjXZAAADy6jGYlGTDQAw0ATnblOTDQAwFIxqdIuabACAoeKMczeoyQYAGDqCcyepyQYAGFqCcydMr8l+5zuTb39bTTYAwBAx47wQarIBAEaGM87zpSYbAGCkCM5zpSYbAGAkGdVoSk02AMBIE5wPRk02AAAxqnFgarIBAGgTnGeiJhsAgGmMakylJhsAgFk44zzpa197pCb7mc9Ukw0AwH4E58ma7Oc975Ga7K9+VU02AAD7Gd3grCYbAIA5GL0ZZzXZAADMw2idcVaTDQDAPI1GcJ5ak33XXckVV6jJBgBgToZ7VGOmmuzzzkse97herwwAgAHTKDiXUu5K8m9J9iV5qNa6ppTy1iSvTHJv+2ZvqbV+sRuLnDM12QAAdNhczjifWGv9l2nH3l1rvaSTC1qwW25JXve65CtfSX71V1s12Rr/AABYoOGZcVaTDQBAFzUNzjXJl0spN5RSzpxy/C9KKTeXUj5WSnl8F9Z3cA89lFx2WXL00ckHPpCceWZy223Jq1+dLF3akyUBADB8Sq314DcqZUWtdbyU8rNJvpLk1Ul2JvmXtEL125McVWt9+Qz3PTPJZNg+Jsm3OrR2kiem9d+AhbOXnWU/O8t+do697Cz72Vn2s7NW1lo7ekWIRsF5vzu0finw/qmzzaWUX0zyt7XWYw5y32211jXzWCczsJ+dYy87y352lv3sHHvZWfazs+xnZ3VjPw86qlFKeWwp5XGTbyd5fpJvlVKOmnKzP4wzyQAADLEmV9U4MsnflFImb//pWuuXSil/VUo5Nq1RjbuSvKpbiwQAgF47aHCutd6R5FkzHH/JPD7fh+dxH2ZnPzvHXnaW/ews+9k59rKz7Gdn2c/O6vh+znnGGQAARtHwXMcZAAC6aN7BuX3t5n8upXxr2vFXl1L+dynlllLKu9rHfrGUMlFKuan950OzPObPlFK+Ukq5rf13b64NvcjmuJenT9nHm0opP2nPmk9/zLeWUsan3O4/LNKX03Mz7Wcp5bNT9uKuUspNUz62oZTy3VLKzlLK2lke8ymllOvat/tsKeWwRfhS+sJc9rOU8vvt673vaP990iyP6fnZbD+9dh7AHPfSa+dBzLKfx5ZSrm3vxbZSygnt46WU8r72a+LNpZTjZnnM49uvB99t374s1tfTa3Pcz9Pb+7ijlPIPpZRHjci2b3dFKeXOKc/PYxfpy+mpOe7lc0sp903Zo7+c5THn93291jqvP0l+J8lxSb415diJSf5Hkse03//Z9t+/OPV2B3jMdyV5c/vtNye5eL7rG6Q/c9nLafdbleT2WR7zrUne0OuvrV/2c9rHL03yl+23fzXJN5M8JslTktyeZMkM9/lckhe33/5QkrN6/XX26X6uTrK8/fYxScZnuY/nZ7P99NrZob2cdtxrZ8P9TPLlJC9ov/0fknx9ytvXJClJfiPJdbM85vXtj5f27V/Q66+zT/fz2Uke3377BQfYzyuSnNbrr63P9/K5aV0i+WCPOa/v6/M+41xr/X+T/HDa4bOSXFRr3du+zT/P8WFPSfKJ9tufSLJuvusbJAvYyz9J8t+7vLyBM8t+JmmdJUnyx0k+0z50SpL/XmvdW2u9M8l3k5www31OSnJV+9DIPDeTue1nrXV7rXVX+8O3JBkrpTxmURY6IOb4/GzKa+c0B9lLr50zmGU/a5LD22//dJLJf9+nJPlvteXaJMvK/pepTfv9w2ut19ZWOvlvGZHnZjK3/ay1/kOtdXf7+LVJnrQoixwQc3xuHtRCvq93esb56CS/3T71/T9LKb825WNPKaVsbx//7Vnuf2St9e722/ekdSm8UXWgvZz0n3Lgb7C9r0TvP7+d5Ae11tva769I8k9TPv799rGpnpBkT631oQPcZlRN38+pXpTkxsn/+ZuB5+ejzbSfXjvn50DPTa+dzZ2TZGMp5Z+SXJJkQ/t4k9fOFe3jB7rNqDknM+/nVK9I6+z8bN7Zfn6+e8RPTJyT2ffyN0sp3yylXFNKecYM95339/VOB+dDk/xMWj+WWZ/kc+1Uf3eSX6i1rk5ybpJPl1IOn/1hkvb/nY7yJT9m28skSSnl15P8f7XW2YpnLkvyy0mOTWv/L+3qagfHn2TuZ/OY3Yz72X6hujizX9/d83Nm0/fTa+f8zfbc9No5N2cleV2t9eeTvC7JR3u8nkF3wP0spZyYVnB+0yz335DkaUl+La2MMNvtRsFse3ljkifXWp+V5P9JsrmTn7TTwfn7STa1f3RzfZKfJHli+8fg/5oktdYb0pojPXqG+/9g8kc97b/nOuoxTGbcyykff3EOEABrrT+ote6rtf4kyeWZNn4wikophyY5NclnpxweT/LzU95/UvvYVP+a1o8hDz3AbUbOLPuZUsqTkvxNkpfWWm+f6b6en48203567Zyf2Z6bbV475+ZlSTa1374yj+xHk9fO8ew/cuC1c/b9TCnlmUk+kuSUyX/309Va727ngr1JPp7Rfn7OuJe11h/VWu9vv/3FJEtLKU+cdt95f1/vdHDenNYvtaWUcnSSw5L8SynliFLKkvbxX0ry1CR3zHD/q9PaiLT//nyH1zdINmeGvWy/f0has3uzzugVlegz+b0k/7vWOvVHh1cneXEp5TGllKek9dy8fuqd2mfwtiY5rX1o1J+bkx61n6WUZUn+Lq1fVPv72e7o+TmjmfbTa+f8zPRv3Wvn/OxK8rvtt09KMjn6cnWSl5aW30hy35RxoSStkJfkR6WU32j/xPSl8dyccT9LKb+QVgh8Sa31O7Pdecr/IJe0ZnJH+fk5217+3ORP6NtX2jgkraD8sAV9X2/yG4Sz/DbiZ9L6MdaDaZ0dfUVa4e6Taf2HvDHJSe3bviitXxS6qX38hVMe5yNJ1rTffkKSr7a/+P+R5Gfmu75B+jOXvayP/MbotTM8ztS9/KskO5LcnNYL3FG9/jp7uZ/t41ck+b9muP15aZ3J25kpv/Gd5It55AoRv5RWoP5uWv9n+5hef539uJ9Jzk/yQPvf+uSfyavreH7OfT+9dnZoL9vHvXbOcT+T/FaSG9K6+tB1SY5v37Yk+UD7tXPH5P61P3bTlLfXpPV97PYk70+7eG0U/sxxPz+SZPeU181tUx5n6veir7X3+1tpZYT/s9dfZx/u5V+0Xze/mdYvWj57lr2c1/d1zYEAANCA5kAAAGhAcAYAgAYEZwAAaEBwBgCABgRnAABoQHAGAIAGBGcAAGhAcAYAgAb+f7rdLf+KqkbHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "height = np.array([170, 174, 175, 176, 173, 183, 178, 176, 168, 181])\n",
    "weight = np.array([60, 65, 65, 66, 61, 75, 63, 63, 58, 70])\n",
    "fit = np.polyfit(height, weight, deg = 1)\n",
    "fit_x = np.linspace(165, 185, 500)\n",
    "plt.figure(figsize = (12, 9))\n",
    "plt.scatter(height, weight)\n",
    "plt.plot(fit_x, fit[0]*fit_x + fit[1], c='r')\n",
    "plt.xlim(165, 185)\n",
    "plt.ylim(55, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "Define a linear model by $Y = \\beta X  + \\alpha + \\epsilon $ where \n",
    "\n",
    "\n",
    "$Y$ is the dependent variable (response) and $X$ is the independent variable (predictor) and $\\epsilon $ is the random error. The parameter $\\beta$ is called slope while $\\alpha$ is the intercept. \n",
    "\n",
    "Also, we require $\\mathbb E(\\epsilon) = 0$ or $\\mathbb E(\\epsilon |X_i) = 0$. And we denote $\\sigma^2 = {\\rm Var}(\\epsilon)$ or $\\sigma^2 = {\\rm Var}(\\epsilon_i | X_i)$. Therefore, for fixed $X_i$, \n",
    "$$\\mathbb E(Y_i|X_i) = \\beta X_i + \\alpha \\quad{\\rm and}\\quad {\\rm Var}(Y_i|X_i) = \\sigma^2$$\n",
    "\n",
    "We call $r(x) =\\beta x+ \\alpha$ the regression line. \n",
    "\n",
    "E.g. Linear models can fit correlated Gaussian variables. \n",
    "\n",
    "### Least Squares Estimator\n",
    "\n",
    "We can estimate the value of $\\alpha$ and $\\beta$ and even $\\epsilon$ with observations. The estimtated values determine a regression line, which we call the fitted regression line. \n",
    "\n",
    "Usually, we wish to minimize $\\mathcal L = \\sum |Y_i - \\hat{Y_i}|^2 = \\sum |Y_i - \\hat \\beta X_i - \\hat \\alpha|^2$, so that \n",
    "$$\\left\\{\\begin{array}{ll}0 = \\dfrac{\\partial \\mathcal L}{\\partial \\hat \\alpha} = - 2\\sum (Y_i - \\hat \\beta X_i - \\hat \\alpha )\n",
    "\\\\ 0 = \\dfrac{\\partial \\mathcal L}{\\partial \\hat \\beta} =-2\\sum X_i (Y_i - \\hat \\beta X_i - \\hat \\alpha ) \\end{array}\\right.$$\n",
    "\n",
    "From the first equation we yield $\\hat \\alpha = \\bar Y - \\hat \\beta \\bar X$. Plug it into the second we learn \n",
    "$$n\\bar X\\bar Y - n \\hat \\beta (\\bar X)^2=\\sum X_iY_i - \\hat \\beta\\sum X_i^2$$\n",
    "And thus \n",
    "$$\\hat \\beta = \\frac{\\overline{XY} - \\bar X \\bar Y}{(\\bar X)^2 - \\overline {X^2}} = \\frac{{\\rm Cov}(X,Y)}{{\\rm Var}(X)}.$$\n",
    "\n",
    "Here \n",
    "$$\\begin{aligned}\n",
    "   {\\rm Cov}(X,Y) &=\\sum (X_i-\\bar X)(Y_i - \\bar Y)= \\sum X_iY_i -\\frac1n \\left(\\sum X_i\\right)\\left(\\sum Y_i\\right)\n",
    "\\\\ {\\rm Var}(X) =  {\\rm Cov}(X,X)&=\\sum (X_i-\\bar X)^2= \\sum X_i^2 - \\frac 1n\\left(\\sum X_i\\right)^2.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties\n",
    "\n",
    "If we substitude $Y_i = \\beta X_i + \\alpha + \\epsilon_i$, then \n",
    "$$\\hat \\beta = \\frac{\\sum (X_i - \\bar X)(Y_i - \\bar Y)}{{\\rm Var}(X)}= \\frac{\\sum (X_i - \\bar X)(\\beta X_i  +\\epsilon_i + \\alpha- \\bar Y)}{{\\rm Var}(X)}= \\frac{\\sum (X_i - \\bar X)(\\beta X_i  +\\epsilon_i )}{{\\rm Var}(X)}=\\beta +\\sum \\frac{X_i - \\bar X}{{\\rm Var}(X)}\\epsilon_i $$\n",
    "and \n",
    "$$\\hat \\alpha = \\beta \\bar X + \\alpha + \\bar \\epsilon -\\hat  \\beta \\bar X\n",
    "=\\alpha +\\sum \\left(\\frac1n  - \\frac{X_i - \\bar X}{{\\rm Var}(X)}\\bar X\\right)\\epsilon_i.$$\n",
    "\n",
    "Recall that the randomness is on $\\epsilon_i$ and $\\epsilon_i$ are independent with mean zero and variance $\\sigma^2$. Therefore, $\\hat \\beta $ and $\\hat \\alpha$ are unbiased estimator. The variance is given by \n",
    "$$\\begin{aligned}{\\rm Var}(\\hat \\beta) & = \\sigma^2 \\sum \\left(\\frac{X_i - \\bar X}{{\\rm Var}(X)}\\right)^2 =\\frac{\\sigma^2}{{\\rm Var}(X)}\n",
    "\\\\ {\\rm Var}(\\hat \\alpha) &= \\sigma^2 \\sum \\left(\\frac{1}{n^2}  - \\frac 2n \\frac{X_i - \\bar X}{{\\rm Var}(X)} \\bar X+ \\left(\\frac{X_i - \\bar X}{{\\rm Var}(X)}\\bar X\\right)^2\\right)=\\left(\\frac{1}{n}+\\frac{(\\bar X)^2}{{\\rm Var}(X)}\\right)\\sigma^2\n",
    "\\\\ {\\rm Cov}(\\hat \\alpha, \\hat \\beta) &=\\sigma^2 \\sum   \\left(\\frac1n  - \\frac{X_i - \\bar X}{{\\rm Var}(X)}\\bar X\\right)\\frac{X_i - \\bar X}{{\\rm Var}(X)} = -\\frac{\\bar X}{{\\rm Var}(X)}\\sigma^2\n",
    "\\\\ {\\rm Var}(\\hat{Y_i}) &= {\\rm Var}\\left(\\hat \\alpha + \\hat \\beta X_i\\right)\n",
    "= \\left(\\frac{1}{n^2}+\\frac{(X_i - \\bar X)^2}{{\\rm Var}(X)}\\right)\\sigma^2 \\\\\n",
    "{\\rm Var}(\\hat{Y_i} - Y_i)&={\\rm Var}(\\hat{Y_i})+\\sigma^2 - 2{\\rm Cov}(\\hat{Y_i},\\epsilon_i)\n",
    "= \\left(\\frac{(n-1)^2}{n^2}-\\frac{(X_i - \\bar X)^2}{{\\rm Var}(X)}  \\right)\\sigma^2 \n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Markov Theorem\n",
    "\n",
    "**Theorem** The simple linear regression by least squares is the best linear unbiased estimator (BLUE). \n",
    "\n",
    "**Proof** Suppose $\\tilde \\beta$ is BLUE. Then it must be linear with respect to $Y$, say, $\\tilde \\beta = \\sum \\lambda_i Y_i$. Since it should be unbiased, we require $\\mathbb E\\tilde\\beta = \\beta$, which implies $\\sum \\left(\\lambda_i \\beta X_i + \\lambda_i \\alpha \\right) \\equiv \\beta$. Thus $\\sum \\lambda_i X_i = 1$ and $\\sum \\lambda_i = 0$. Lastly, to minimize the variance of $\\tilde \\beta$, it is equivalent to minimizing $\\sum \\lambda_i^2$.\n",
    "\n",
    "This is certainly convex with Slater condition and we use the Lagrangian dual to derive the result. \n",
    "$$\\begin{aligned}\\inf \\{\\sum \\lambda_i^2 :\\ \\sum \\lambda_i X_i = 1,\\ \\sum\\lambda_i = 0\\}\n",
    "&=\\sup_\\mu  \\inf_\\lambda  \\{\\sum \\lambda_i^2 + \\mu_1(\\sum \\lambda_i X_i - 1) + \\mu_2 \\sum\\lambda_i\\}\\\\\n",
    "&= \\sup_\\mu \\{-\\mu_1-\\sum \\frac{(\\mu_1 X_i+\\mu_2)^2}{4}\\}\\end{aligned}.$$\n",
    "\n",
    "It reaches supremum when $\\mu_2 = -\\mu_1 \\bar X$ and $\\mu_1 = -\\dfrac{2}{{\\rm Var}(X)}$. Hence $\\lambda_i = -\\dfrac{\\mu_1X_i+\\mu_2}{2}=\\dfrac{X_i - \\bar X}{{\\rm Var}(X)} $ and \n",
    "$$\\tilde \\beta = \\sum \\frac{X_i - \\bar X}{{\\rm Var}(X)}Y_i  =  \\sum \\frac{X_i - \\bar X}{{\\rm Var}(X)}(Y_i - \\bar Y) = \\hat \\beta,$$\n",
    "which is exactly the solution to least squares. \n",
    "\n",
    "Similarly one can prove $\\hat \\alpha$ is also BLUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Maximum Likelihood Estimator\n",
    "\n",
    "**Theorem** If $\\epsilon_i\\sim N(0,\\sigma^2)$ are independent, then the simple linear regression by least squares is the maximum likelihoood estimator (MLE). \n",
    "\n",
    "**Proof** As randomness falls on $\\epsilon_i$, we learn $Y$ has joint multivariate normal distribution, $Y\\sim N(\\beta X+\\alpha, \\sigma^2 I)$. The log likelihoood is given by \n",
    "$$\\log L = \\log \\frac{1}{\\sqrt {2\\pi }^n \\sigma^n} -\\frac{1}{2\\sigma^2} \\left\\Vert Y - \\beta X - \\alpha\\right\\Vert^2.$$\n",
    "\n",
    "Given fixed $\\sigma$, then $\\hat \\alpha,\\hat \\beta$ minimize the distance $\\left\\Vert Y - \\beta X - \\alpha\\right\\Vert^2$, which corresponds to the least squares presented in the linear regression. \n",
    "\n",
    "Moreover, one can estimate $\\sigma^2$ by MLE. Note that \n",
    "$$0 = \\frac{\\partial \\log L}{\\partial \\sigma} =\\frac{1}{\\sigma^3} \\left\\Vert Y - \\hat \\beta X - \\hat \\alpha\\right\\Vert^2-\\frac {n}{\\sigma}\\quad \\Rightarrow \\quad \\hat\\sigma_{MLE}^2 = \\frac 1n\\left\\Vert Y -\\hat  \\beta X - \\hat \\alpha\\right\\Vert^2.  $$\n",
    "\n",
    "And \n",
    "$$\\mathbb E\\hat \\sigma_{MLE}^2=\\frac 1n \\sum \\mathbb E\\left(Y_i - \\hat \\beta X_i -\\hat \\alpha \\right)^2\n",
    "=\\frac 1n \\sum  \\left(\\frac{(n-1)^2}{n^2}-\\frac{(X_i - \\bar X)^2}{{\\rm Var}(X)}  \\right)\\sigma^2 =\\frac{n-2}{n}\\sigma^2\n",
    ".$$\n",
    "\n",
    "One can fix the unbiasedness by selecting \n",
    "$$s^2=\\hat \\sigma^2 = \\frac{n}{n-2}\\hat \\sigma_{MLE}^2=\\frac{1}{n-2}\\sum \\left(Y_i - \\hat \\beta X_i -\\hat \\alpha \\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution\n",
    "\n",
    "Next we see the distributions of the estimators $\\hat \\beta,\\hat \\alpha,s^2$. If we assume $\\epsilon\\sim N(0,\\sigma^2)$ are independent normal distribution. Since $\\hat \\beta$ and $\\hat \\alpha$ are linear combinations of $\\epsilon$, they are jointly normal. Therefore the joint distribution is determined by the mean and the covariance, \n",
    "$$\\left[\\begin{matrix}\\hat \\alpha\\\\ \\hat \\beta \\end{matrix}\\right] \\sim N\\left( \\left[\\begin{matrix}\\alpha \\\\ \\beta\\end{matrix}\\right], \\frac{\\sigma^2}{{\\rm Var}(X)}\\left[\\begin{matrix} \\frac{{\\rm Var}(X)}{n}+{(\\bar X)^2}{} & - {\\bar X}{ }\n",
    "\\\\ -{\\bar X}{} &  1\n",
    "\\end{matrix}\\right] \\right)$$\n",
    "\n",
    "$s^2$ has the distribution that $\\dfrac{(n-2)s^2}{\\sigma^2}\\sim \\chi_{n-2}^2$. Also, we can prove that $[\\hat \\alpha,\\hat \\beta]$ is independent with $s^2$.\n",
    "\n",
    "Proof: Let $a = \\left[\\frac1n - \\frac{X_i - \\overline X}{{\\rm Var}(X)}\\overline X\\right]^T$ be a column vector and $b =\\left[\\frac{X_i - \\overline X}{{\\rm Var}(X)}\\right]^T $ be another column vector. Then we have already known that $ \\hat \\alpha - \\alpha = a^T\\epsilon$ while $\\hat \\beta -  \\beta = b^T\\epsilon$ where $\\epsilon = \\left[\\epsilon_i\\right]^T$. We by Gram Schmidt theorem can find the orthogonal complement of $[a,b]$, say $Q^T[a,b]=0$ where $Q\\in\\mathbb R^{n\\times(n-2)}$. Then we know that \n",
    "$$[a,b,Q]^T\\epsilon =\\left[ \\begin{matrix}\\hat \\alpha - \\alpha \\\\ \\hat \\beta - \\beta \\\\ Q^T\\epsilon\\end{matrix}\\right] \\sim N\\left(0, [a,b,Q]^T[a,b,Q]\\sigma^2\\right)\n",
    "=N\\left(0,  \\left[\\begin{matrix} a^Ta  & a^Tb & 0 \\\\ b^Ta & b^Tb & 0 \\\\ 0 & 0 & I_{n-2}\\end{matrix}\\right]\\sigma^2\\right)$$\n",
    "\n",
    "This implies that $\\hat \\alpha, \\hat \\beta$ are independent with $Q^T\\epsilon$, and thus independent with $\\epsilon^TQQ^T\\epsilon$. To compute $\\epsilon ^TQQ^T\\epsilon$, let $r = b/\\Vert b\\Vert$, then we can orthonormalize $[a,b]$ to $[\\frac{a -rr^Ta }{\\Vert a -rr^Ta \\Vert },r]$. Moreover, we can notice that\n",
    "$$a -rr^Ta =a - \\frac{bb^Ta}{b^Tb} =a +\\overline X b = \\frac 1ne\n",
    "$$\n",
    "where $e = [1,1,\\dotsc,1]^T$.\n",
    "\n",
    "So \n",
    "$$\\begin{aligned}\\epsilon^T QQ^T\\epsilon&=\\epsilon^T\\left(I - \\left[ \\frac{e}{\\sqrt n},r\\right]\\left[ \\frac{e}{\\sqrt n},r\\right]^T\\right)\\epsilon\n",
    " = \\epsilon^T\\epsilon -\\left  \\Vert \\left[\\begin{matrix} e^T\\epsilon /\\sqrt n \\\\ {r^T\\epsilon} \\end{matrix}\\right] \\right\\Vert^2\n",
    " =  \\epsilon^T\\epsilon -\\frac{\\Vert e^T\\epsilon \\Vert^2}{n} - \\frac{\\Vert b^T\\epsilon \\Vert^2}{b^Tb} \n",
    "\\end{aligned}\n",
    "$$\n",
    "with $(i,j)$ entry (of the quadratic form) being \n",
    "$$\\begin{aligned}\n",
    "\\sim _{ij} &= \\delta_{ij} - \\frac 1n - {\\rm Var}(X)b_ib_j \n",
    "\\\\ &=  \\delta_{ij} - \\frac 1n - {\\rm Var}(X)\\frac{X_i - \\bar X}{{\\rm Var}(X)} \\frac{X_j - \\bar X}{{\\rm Var}(X)} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "On the other hand,\n",
    "$$\\begin{aligned}(n-2)s^2 &= \\Vert Y - X\\hat \\beta - \\hat \\alpha\\Vert^2 \n",
    "= \\Vert Y - X\\hat \\beta - \\left(\\bar Y - e\\bar X\\hat\\beta\\right)\\Vert^2\n",
    "=\\Vert (X - e\\bar X)(\\beta - \\hat\\beta)+\\epsilon - e\\bar \\epsilon \\Vert^2\n",
    "\\\\ &=\\left\\Vert \\left((  e\\bar X - X)b^T + I - \\frac 1nee^T\\right) \\epsilon \\right\\Vert^2\n",
    "\\\\ &=\\left\\Vert \\left( e\\left(\\frac 1n e^T - a^T\\right) -Xb^T+ I - \\frac 1nee^T\\right) \\epsilon \\right\\Vert^2\n",
    "\\\\ &=\\left\\Vert \\left( I -Xb^T-ea^T\\right) \\epsilon \\right\\Vert^2 \n",
    "\\end{aligned}$$\n",
    "\n",
    "To expand, recall $a + \\frac 1n bX^Te = \\frac 1n e$, \n",
    "$$\\begin{aligned}  \\left( I -Xb^T-ea^T\\right)^T\\left( I -Xb^T-ea^T\\right) &= I +bX^TXb^T + ae^Tea^T-(Xb^T+bX^T) - (ea^T+a^Te) + bX^Tea^T + ae^TXb^T\n",
    "\\\\ &= I+bX^TXb^T + naa^T-(Xb^T+bX^T) - (ea^T+a^Te)-(na-e)a^T-a(na-e)^T\n",
    "\\\\ &= I-naa^T  -(Xb^T+bX^T) +b(X^TX)b^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "with $(i,j)$ entry being \n",
    "$$\\begin{aligned}\\sim_{ij} &= \\delta_{ij} - na_ia_j - X_i b_j -X_jb_i + (X^TX)b_i b_j\n",
    "\\\\ &= \\delta_{ij}- n\\left(\\frac 1n - \\frac{X_i - \\bar X}{{\\rm Var}(X)}\\bar X\\right)\\left(\\frac 1n - \\frac{X_j - \\bar X}{{\\rm Var}(X)}\\bar X \\right) - \\frac{X_i(X_j - \\bar X)}{{\\rm Var}(X)}- \\frac{X_j(X_i - \\bar X)}{{\\rm Var}(X)}+({\\rm Var}(X) + n(\\bar X)^2)\\frac{(X_i - \\bar X)(X_j - \\bar X)}{{\\rm Var}(X)^2}\n",
    "\\\\ &= \\delta_{ij} - \\frac 1n +\\frac{(X_i+X_j - 2\\bar X)\\bar X-2X_iX_j + (X_i +X_j)\\bar X+(X_i - \\bar X)(X_j - \\bar X)}{{\\rm Var}(X)}\n",
    "\\\\ &= \\delta_{ij} - \\frac 1n +\\frac{(X_i+X_j - \\bar X)\\bar X- X_iX_j  }{{\\rm Var}(X)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore we conclude $\\epsilon^TQQ^T\\epsilon \\equiv (n-2)s^2$. And $\\hat \\alpha, \\hat \\beta$ are thus independent with $s^2$。  Also, because $Q^T\\epsilon\\sim N(0,I_{n-2})$, we have $\\epsilon^T QQ^T\\epsilon=\\Vert Q^T\\epsilon\\Vert^2\\sim \\chi_{n-2}^2$.\n",
    "\n",
    "\n",
    "* Remark:\n",
    "$$\\begin{aligned} & (I - Xb^T-ea^T)a = a - Xb^Ta - ea^Ta = \\left[\\frac 1n - \\frac{X_i - \\bar X}{{\\rm Var}(X)}\\bar X + X_i \\frac{\\bar X}{{\\rm Var}(X)} - \\left(\\frac 1n +\\frac{(\\bar X)^2}{{\\rm Var}(X)}\\right) \\right]^T=0\n",
    "\\\\ & (I - Xb^T-ea^T)b = b - Xb^Tb - ea^Tb = \\left[\\frac{X_i - \\bar X}{{\\rm Var}(X)} - \\frac{X_i}{{\\rm Var}(X)}+\\frac{\\bar X}{{\\rm Var}(X)}\\right]^T=0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "\n",
    "### Total Sum of Squares \n",
    "\n",
    "Total sum of squares is simply a multiple of  the variance of data, ${\\rm SST} = \\sum_{i=1}^n (Y_i - \\bar Y)^2$. It does not depend on the model but observations.\n",
    "\n",
    "### Regression Sum of Squares \n",
    "\n",
    "Regression sum of squares is defined by ${\\rm SSR} = \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2$ where $\\hat Y_i$ is the prediction of the model. It is also called explained sum of squares. When using linear regression, one can show that $\\bar Y = \\bar{\\hat Y}$, and thus it is also a multiple of  variance of the predicition. Moreover, we have the equality that \n",
    "$${\\rm SSR}  = \\hat \\beta^2 {\\rm Var}(X).$$\n",
    "\n",
    "It implies the randomness of ${\\rm SSR}$ only lies in $\\hat\\beta$.\n",
    "\n",
    "Proof: Let $X,Y,\\hat Y\\in\\mathbb R^n$ be the column vectors collecting each $X_i,Y_i,\\hat Y_i$, and let $e = [1,1,\\dotsc,1]^T$,\n",
    "$$\\begin{aligned}{\\rm SSR} &= \\Vert \\hat Y - e\\bar Y\\Vert^2=\\Vert X\\hat \\beta + \\hat \\alpha  - e\\bar Y\\Vert^2\n",
    "=\\Vert X\\hat \\beta + e(\\bar Y - \\bar X\\hat \\beta)  - e\\bar Y\\Vert^2\\\\ &=\\hat\\beta^2 \\Vert X -e \\bar X\\Vert^2=\\hat\\beta^2{\\rm Var}(X)=\\frac{{\\rm Cov}^2(X,Y)}{{\\rm Var}(X)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Residual Sum of Squares \n",
    "\n",
    "Residual sum of squares is defined by ${\\rm SSE} = \\sum_{i=1}^n (\\hat Y_i - Y_i)^2$. Recall that $\\dfrac{{\\rm SSE}}{n-2}= s^2\\rightarrow \\sigma^2$.\n",
    "\n",
    "### Relation\n",
    "\n",
    "1. We have the equation, ${\\rm SST} = {\\rm SSR}+{\\rm SSE}$. Note that $\\hat\\beta$ and $s^2$ are independent, ${\\rm SSR}$ and ${\\rm SSE}$ are also independent.\n",
    "\n",
    "\n",
    "Proof: Let $X,Y,\\hat Y\\in\\mathbb R^n$ be the column vectors collecting each $X_i,Y_i,\\hat Y_i$, and let $e = [1,1,\\dotsc,1]^T$,\n",
    "$$\\begin{aligned}\n",
    "{\\rm SST} &= \\Vert Y -e\\bar Y\\Vert^2 = \\Vert \\hat X - e\\bar Y+Y -\\hat Y\\Vert^2\n",
    "\\\\ &={\\rm SSR}+{\\rm SSE}+2(\\hat Y - e\\bar Y)^T(Y - \\hat Y) \n",
    "\\\\ &={\\rm SSR}+{\\rm SSE}+2( X \\hat \\beta + \\hat \\alpha - e\\bar Y)^T(Y - X \\hat \\beta - \\hat \\alpha) \n",
    "\\\\ &={\\rm SSR}+{\\rm SSE}+2( X \\hat \\beta +e (\\bar Y -\\bar X\\hat\\beta) - e\\bar Y)^T(Y - X \\hat \\beta -e(\\bar Y -   \\bar X\\hat\\beta) ) \n",
    "\\\\ &={\\rm SSR}+{\\rm SSE}+2\\hat \\beta (X - e\\bar X)^T((Y - e\\bar Y) - (X - e\\bar X)\\hat \\beta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that $(X - e\\bar X)^T(Y - e\\bar Y) = {\\rm Cov}(X,Y)$, while $(X - e\\bar X)^T(X - e\\bar X)\\hat \\beta={\\rm Var}(X)\\hat\\beta = {\\rm Cov}(X,Y)$, the last term thus equals to zero.\n",
    "\n",
    "2. ${\\rm SST,\\ SSR,\\ SSE}$ has degree of freedom $n-1,1,n-2$, respectively.\n",
    "\n",
    "### Mean Squares\n",
    "\n",
    "Mean squares is the sum of squares divided by degree of freedom.\n",
    "\n",
    "$${\\rm MST}=\\frac{\\rm SST}{n-1}\\quad {\\rm MSR}={\\rm SSR}\\quad {\\rm MSE}=\\frac{\\rm MSE}{n-2}$$\n",
    "\n",
    "### Coefficient of Determination\n",
    "\n",
    "$r^2={\\rm SSR}/{\\rm SST}\\in [0,1]$ is the propotion of the regression (explained) sum of squares. When $r^2=0$ there is no linear relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $H_0:\\ \\beta =0$\n",
    "\n",
    "### SSR\n",
    "When using null hypothesis $H_0:\\ \\beta = 0$, as we know that ${\\rm SSR}=\\hat\\beta^2{\\rm Var}(X)$, we know that ${\\rm SSR}$ is likely to be small. And thus we can reject $H_0$ when ${\\rm SSR}>c$. To be explicit,\n",
    "$$\\begin{aligned}{\\rm SSR} & = \\frac{{\\rm Cov}^2(X,Y)}{{\\rm Var}(X)}=\\frac{1}{{\\rm Var}(X)}\\left(\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y\n",
    ")\\right)^2=  \\frac{1}{{\\rm Var}(X)}\\left(\\sum_{i=1}^n(X_i - \\overline X)Y_i\\right)^2\\\\ &\n",
    "=\\frac{1}{{\\rm Var}(X)}\\left(\\sum_{i=1}^n (X_i - \\overline X)(\\alpha X_i +\\epsilon_i) \\right)^2=\\frac{1}{{\\rm Var}(X)}\\left(\\sum_{i=1}^n (X_i - \\overline X)\\epsilon_i \\right)^2.\\end{aligned}$$\n",
    "\n",
    "Note that $\\sum_{i=1}^n (X_i - \\overline X)\\epsilon_i \\sim N\\left(0, {\\rm Var}(X)\\sigma^2\\right)$, so \n",
    "$$\\frac{\\rm SSR}{\\sigma^2}\\sim \\chi_1^2\n",
    "$$\n",
    "\n",
    "### SST\n",
    "\n",
    "$${\\rm SST} = \\sum_{i=1}^n (\\alpha X_i +\\epsilon_i - \\alpha \\overline X - \\overline\\epsilon)^2\n",
    "$$\n",
    "\n",
    "### SSE\n",
    "\n",
    "### Hypothesis Testing \n",
    "\n",
    "1. Since we do not know $\\sigma^2$, we can not directly use $ {\\rm SSR}/{\\sigma^2}\\sim \\chi_1^2$. But $\\dfrac{\\rm SSR}{\\sigma^2}\\rightarrow \\dfrac{\\rm SSR}{s^2}$, we can use\n",
    "$$\\frac{\\rm SSR}{s^2}=\\frac{\\rm MSR}{\\rm MSE}\\rightarrow F_{1,n-2}.$$\n",
    "\n",
    "2. An alternative is  using $\\hat \\beta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Prediction\n",
    "\n",
    "1. Mean Response\n",
    "\n",
    "Given a new observation $X_{n+1}$, we wish to provide a confidence interval for the expectance of the new response, $\\mathbb E Y_{n+1}$.\n",
    "\n",
    "**Solution** First we find a point estimation for $\\mathbb EY_{n+1}$, which is simply $\\hat \\alpha +\\hat \\beta X_{n+1}$. We expect to find some $\\lambda$ such that \n",
    "$$\\mathbb P\\left(\\hat \\alpha +\\hat \\beta X_{n+1} - \\lambda \\hat s\\leqslant \\mathbb E\n",
    "Y_{n+1}\\leqslant \\hat \\alpha +\\hat \\beta X_{n+1} + \\lambda \\hat s\\right) = 1-k,$$\n",
    "or, equivalently,\n",
    "$$\\mathbb P\\left(-\\lambda \\leqslant \\frac{\\hat \\alpha +\\hat \\beta X_{n+1} -\\mathbb EY_{n+1}}{\\hat s} \\leqslant \\lambda\\right) = 1 - k.\n",
    "$$\n",
    "\n",
    "Note that \n",
    "$$\\left\\{\\begin{aligned}& \\hat \\alpha +\\hat \\beta X_{n+1} -\\mathbb EY_{n+1}\\sim N\\left(0,  {\\sigma^2} \\left(\\frac{1}{n}+\\frac{(\\overline X - X_{n+1})^2}{{\\rm Var}(X)}\\right)\\right)\\\\ \n",
    "& \\hat s^2\\sim \\frac{\\sigma^2}{n-2}\\chi_{n-2}^2\n",
    "\\end{aligned}\\right.$$\n",
    "and they are independent since $\\hat \\epsilon $ is independent with $(\\hat\\alpha,\\hat \\beta)$. Hence we conclude that \n",
    "$$\\frac{\\hat \\alpha +\\hat \\beta X_{n+1} -\\mathbb EY_{n+1}}{\\hat s} \n",
    "\\sim \\frac{N\\left(0,  {\\sigma^2} \\left(\\frac{1}{n}+\\frac{(\\overline X - X_{n+1})^2}{{\\rm Var}(X)}\\right)\\right)}{\\sqrt{\\frac{\\sigma^2}{n-2}\\chi_{n-2}^2}}\\sim \\sqrt{\\frac{1}{n}+\\frac{(\\overline X - X_{n+1})^2}{{\\rm Var}(X)}}t_{n-2}.$$\n",
    "<br>\n",
    "\n",
    "2. Individual Response\n",
    "   \n",
    "Given a new observation $X_{n+1}$, we wish to provide a confidence interval for $Y_{n+1}$.\n",
    "\n",
    "**Solution** First we find a point estimation for $Y_{n+1}$, which is simply $\\hat \\alpha +\\hat \\beta X_{n+1}$. We expect to find some $\\lambda$ such that \n",
    "$$\\mathbb P\\left(\\hat \\alpha +\\hat \\beta X_{n+1} - \\lambda \\hat s\\leqslant \n",
    "Y_{n+1}\\leqslant \\hat \\alpha +\\hat \\beta X_{n+1} + \\lambda \\hat s\\right) = 1-k,$$\n",
    "or, equivalently,\n",
    "$$\\mathbb P\\left(-\\lambda \\leqslant \\frac{\\hat \\alpha +\\hat \\beta X_{n+1} -Y_{n+1}}{\\hat s} \\leqslant \\lambda\\right) = 1 - k.\n",
    "$$\n",
    "\n",
    "Note that \n",
    "$$\\left\\{\\begin{aligned}& \\hat \\alpha +\\hat \\beta X_{n+1} -Y_{n+1}\\sim N\\left(0,  {\\sigma^2} \\left(\\frac{1}{n}+1+\\frac{(\\overline X - X_{n+1})^2}{{\\rm Var}(X)}\\right)\\right)\\\\ \n",
    "& \\hat s^2\\sim \\frac{\\sigma^2}{n-2}\\chi_{n-2}^2\n",
    "\\end{aligned}\\right.$$\n",
    "and they are independent since $\\hat \\epsilon $ is independent with $(\\hat\\alpha,\\hat \\beta)$. Hence we conclude that \n",
    "$$\\frac{\\hat \\alpha +\\hat \\beta X_{n+1} - Y_{n+1}}{\\hat s} \n",
    "\\sim \\frac{N\\left(0,  {\\sigma^2} \\left(\\frac{1}{n}+1+\\frac{(\\overline X - X_{n+1})^2}{{\\rm Var}(X)}\\right)\\right)}{\\sqrt{\\frac{\\sigma^2}{n-2}\\chi_{n-2}^2}}\\sim \\sqrt{\\frac{1}{n}+1+\\frac{(\\overline X - X_{n+1})^2}{{\\rm Var}(X)}}t_{n-2}.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1120dc956da57eca5c948a0118f4cdcd4d1b3be98c72752ed298d16085a61d24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
